{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spektral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout,Input\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from spektral.layers import GINConv,GCNConv #, GCSConv, GlobalAvgPool\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "from spektral.data import DisjointLoader, BatchLoader, Dataset, Graph\n",
    "#from spektral.transforms.normalize_adj import NormalizeAdj\n",
    "import gc\n",
    "import spektral.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignorar las siguientes 2 celdas\n",
    "\n",
    "Primeras pruebas, para ver si los métodos dentro de la clase tenían chance de funcionar ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captures = [\"10\",\"11\",\"12\",\"15\",\"15-2\",\"16\",\"16-2\",\"16-3\",\"17\",\"18\",\"18-2\",\"19\",\"15-3\"]\n",
    "\n",
    "for i in captures:\n",
    "    x_tmp = pd.read_csv(f'/mnt/features-prefix/capture201108{i}_features_prefix.csv', sep=\",\", header=0)\n",
    "    class_idx = {name: idx for idx, name in enumerate(sorted(x_tmp[\"label\"].unique()))}\n",
    "    node_idx = {name: idx for idx, name in enumerate(sorted(x_tmp[\"node\"].unique()))}\n",
    "    # Cambiamos los nodos y clases por su correspondiente número entero, en las features y en los grafos\n",
    "    x_tmp[\"node\"] = x_tmp[\"node\"].apply(lambda name: node_idx[name])\n",
    "    x_tmp[\"label\"] = x_tmp[\"label\"].apply(lambda value: class_idx[value])\n",
    "    x = tf.cast(x_tmp.sort_values(\"node\")[x_tmp.columns.difference([\"node\",\"label\"], sort=False)].to_numpy(), dtype=tf.dtypes.float32)                \n",
    "   \n",
    "    a_tmp = pd.read_csv(f'/mnt/ncol-prefix/capture201108{i}_ncol_prefix.ncol', sep=\" \", header=None, names=[\"source\", \"target\", \"weight\"])\n",
    "    a_tmp[\"source\"] = a_tmp[\"source\"].apply(lambda name: node_idx[name])\n",
    "    a_tmp[\"target\"] = a_tmp[\"target\"].apply(lambda name: node_idx[name])\n",
    "    a_source_tmp = a_tmp[[\"source\"]].to_numpy().T\n",
    "    a_source = np.reshape(a_source_tmp, a_source_tmp.shape[-1])\n",
    "    a_target_tmp = a_tmp[[\"target\"]].to_numpy().T\n",
    "    a_target = np.reshape(a_target_tmp, a_target_tmp.shape[-1])\n",
    "    a_weight_tmp = a_tmp[[\"weight\"]].to_numpy().T\n",
    "    a_weight = np.reshape(a_weight_tmp, a_weight_tmp.shape[-1])\n",
    "    a = sparse.coo_matrix((a_weight, (a_source, a_target)), shape=(x.shape[0], x.shape[0]))\n",
    "\n",
    "    y = tf.cast(x_tmp.sort_values(\"node\")[\"label\"].to_numpy(), dtype=tf.dtypes.int64)\n",
    "\n",
    "    filename = f'/mnt/grafos_npz2/graph_201108{i}.npz'\n",
    "    np.savez(filename, x=x, a=a, y=y)\n",
    "    \n",
    "    del x_tmp, feature_names, a_tmp, a_source_tmp, a_source, a_target_tmp, a_target, a_weight_tmp, a_weight\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "captures = [\"10\",\"11\",\"12\",\"15\",\"15-2\",\"16\",\"16-2\",\"18\",\"18-2\",\"15-3\"] # for training\n",
    "for i in captures:\n",
    "    data = np.load(f'/mnt/grafos_npz2/graph_201108{i}.npz', allow_pickle=True)\n",
    "    output.append(Graph(x=data['x'], a=data['a'][()], y=data['y']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos clase CTU13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según https://graphneural.network/creating-dataset/ para crear un dataset propio hay que crear una clase e incluir un método `download` y uno `read`: si al querer leer los datos, no los encuentra en el path correspondiente, entonces los descarga.\n",
    "En nuestro caso, además, lo que hace la \"descarga\" es \"acomodar\" los datos a un formato propio de Spektral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTU13(Dataset):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def download(self):\n",
    "        os.mkdir(self.path)\n",
    "        captures = [\"10\",\"11\",\"12\",\"15\",\"15-2\",\"16\",\"16-2\",\"16-3\",\"17\",\"18\",\"18-2\",\"19\",\"15-3\"]\n",
    "\n",
    "        for i in captures:\n",
    "            # x = nodes features (ID, OD, IDW, ODW)\n",
    "            # a = adjacency matrix\n",
    "            # y =labels\n",
    "            \n",
    "            # Read files with nodes features (csv file) and connections between nodes (ncol file)\n",
    "            x_tmp = pd.read_csv(f'/mnt/features-prefix/capture201108{i}_features_prefix.csv', sep=\",\", header=0)\n",
    "            a_tmp = pd.read_csv(f'/mnt/ncol-prefix/capture201108{i}_ncol_prefix.ncol', sep=\" \", header=None, names=[\"source\", \"target\", \"weight\"])\n",
    "            \n",
    "            # Create dictionaries that identify each node and label with an integer\n",
    "            class_idx = {name: idx for idx, name in enumerate(sorted(x_tmp[\"label\"].unique()))}\n",
    "            node_idx = {name: idx for idx, name in enumerate(sorted(x_tmp[\"node\"].unique()))}\n",
    "            \n",
    "            # Change node names and label for their corresponding integer\n",
    "            x_tmp[\"node\"] = x_tmp[\"node\"].apply(lambda name: node_idx[name])\n",
    "            x_tmp[\"label\"] = x_tmp[\"label\"].apply(lambda value: class_idx[value])\n",
    "            a_tmp[\"source\"] = a_tmp[\"source\"].apply(lambda name: node_idx[name])\n",
    "            a_tmp[\"target\"] = a_tmp[\"target\"].apply(lambda name: node_idx[name])\n",
    "            \n",
    "            # Node features:\n",
    "            x = x_tmp.sort_values(\"node\")[x_tmp.columns.difference([\"node\",\"label\"], sort=False)].to_numpy()       \n",
    "            x.astype(np.float32)                \n",
    "            \n",
    "            # Separate source, target and weight to create a sparce matrix\n",
    "            a_source = a_tmp[[\"source\"]].to_numpy().T\n",
    "            a_source = np.reshape(a_source, a_source.shape[-1])\n",
    "            a_target = a_tmp[[\"target\"]].to_numpy().T\n",
    "            a_target = np.reshape(a_target, a_target.shape[-1])\n",
    "            a_weight = a_tmp[[\"weight\"]].to_numpy().T\n",
    "            a_weight = np.reshape(a_weight, a_weight.shape[-1])\n",
    "            # Adjacency matrix:\n",
    "            a = sparse.coo_matrix((a_weight, (a_source, a_target)), shape=(x.shape[0], x.shape[0]))\n",
    "\n",
    "            # Label:\n",
    "            y = x_tmp.sort_values(\"node\")[\"label\"].to_numpy()\n",
    "            y.astype(np.int64)\n",
    "\n",
    "            # Save in format npz\n",
    "            filename = os.path.join(self.path, f'graph_201108{i}.npz')\n",
    "            np.savez(filename, x=x, a=a, y=y)\n",
    "\n",
    "            # Free memory\n",
    "            del x_tmp, x, a_tmp, a_source, a_target, a_weight, a, y\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "    def read(self):\n",
    "        # We must return a list of Graph objects\n",
    "        output = []\n",
    "        \n",
    "        captures = [\"10\",\"11\",\"12\",\"15\",\"15-2\",\"16\",\"16-2\",\"16-3\",\"17\",\"18\",\"18-2\",\"19\",\"15-3\"]\n",
    "\n",
    "        for i in captures:\n",
    "            data = np.load(os.path.join(self.path, f'graph_201108{i}.npz'), allow_pickle=True)\n",
    "            output.append(\n",
    "                Graph(x=data['x'], a=data['a'][()], y=data['y']) # también puede ser a=data['a'].item()\n",
    "            )\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargamos el dataset\n",
    "\n",
    "Separamos en train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CTU13()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture number 9 (capture20110817) is for testing\n",
    "dataset_test = dataset[8]\n",
    "\n",
    "# other captures are for training\n",
    "np.random.seed(123)\n",
    "dataset_tosplit = dataset[np.random.choice([0,1,2,3,4,5,6,7,9,10,11,12], 12, replace=False)]\n",
    "\n",
    "# split in training and validation\n",
    "split = int(0.8 * len(dataset_tosplit))\n",
    "dataset_train, dataset_val = dataset_tosplit[:split], dataset_tosplit[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  0,  4, 10,  9,  7, 11,  3,  1,  6, 12,  2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "np.random.choice([0,1,2,3,4,5,6,7,9,10,11,12], 12, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CTU13(n_graphs=9)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CTU13(n_graphs=3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val # 20110816-2, 20110815-3, 20110812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "loader_train = DisjointLoader(dataset_train, node_level=True, batch_size=batch_size, epochs=epochs, shuffle=True)\n",
    "loader_val = DisjointLoader(dataset_val, node_level=True, batch_size=batch_size)\n",
    "loader_test = DisjointLoader(dataset_test, node_level=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copia del código de Harpo\n",
    "Lo que sigue es copia directa (o sea, sin pensar) de lo que subió Harpo en https://github.com/harpomaxx/graph-representation-learning/blob/harpo-branch-pkts/code/python/notebooks/spektral-example.ipynb\n",
    "\n",
    "Da error, lo próximo que voy a analizar es ver si ese error tiene que ver con que sean problemas diferentes (en el ejemplo de Harpo el objetivo es clasificar grafos, en el problema de CTU13 el objetivo es clasificar nodos).\n",
    "\n",
    "El error dice \"ValueError: Shapes (None, 106580) and (None, 605195) are incompatible\". \n",
    "\n",
    "A tener en cuenta: capture20110816 tiene 106580 nodos, mientras que capture20110810 tiene 604195 nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gcn_model():\n",
    "    # Define input placeholders for node features, adjacency matrix, and segment indices\n",
    "    X_in = Input(shape=(dataset.n_node_features,))\n",
    "    A_in = Input((None,), sparse=True)\n",
    "    I_in = Input(shape=(), dtype=tf.int32)\n",
    "\n",
    "    # Apply the first GINConv layer with 32 units and ReLU activation\n",
    "    X_1 = GINConv(32, activation=\"relu\")([X_in, A_in])\n",
    "    # Apply dropout with a rate of 0.5\n",
    "    X_1 = Dropout(0.5)(X_1)\n",
    "\n",
    "    # Apply the second GINConv layer with 32 units and ReLU activation\n",
    "    X_2 = GINConv(32, activation=\"relu\")([X_1, A_in])\n",
    "    # Apply dropout with a rate of 0.5\n",
    "    X_2 = Dropout(0.5)(X_2)\n",
    "\n",
    "    # Aggregate the node features using the segment_mean function and the segment indices\n",
    "    X_3 = tf.math.segment_mean(X_2, I_in)\n",
    "    # Apply a dense output layer with the number of labels and softmax activation\n",
    "    out = Dense(dataset.n_labels, activation=\"softmax\")(X_3)\n",
    "\n",
    "    # Create and return the model with the defined inputs and outputs\n",
    "    model = Model(inputs=[X_in, A_in, I_in], outputs=out)\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = create_gcn_model()\n",
    "optimizer = Adam(lr=0.01)\n",
    "loss_fn = CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorate the function with @tf.function to compile as a TensorFlow graph\n",
    "# Use the input_signature from loader_train and relax shapes for varying graph sizes\n",
    "@tf.function(input_signature=loader_train.tf_signature(), experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    # Create a GradientTape context to record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute model predictions with the inputs, set training=True for training-specific behaviors\n",
    "        predictions = model(inputs, training=True)\n",
    "        # Calculate the loss using the provided loss_fn and add the model's regularization losses\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "\n",
    "    # Compute gradients of the loss with respect to the model's trainable variables\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Apply the gradients to the model's variables using the optimizer's apply_gradients method\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Compute the accuracy using the categorical_accuracy function from TensorFlow\n",
    "    # Calculate the mean accuracy using tf.reduce_mean\n",
    "    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "\n",
    "    # Return the loss and accuracy as output\n",
    "    return loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            tf.reduce_mean(categorical_accuracy(target, pred)),\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/spektral/data/utils.py:221: UserWarning: you are shuffling a 'CTU132' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"<ipython-input-32-5cb04d2d4fb9>\", line 10, in train_step  *\n        loss = loss_fn(target, predictions) + sum(model.losses)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 139, in __call__  **\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1787, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5119, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 106580) and (None, 605195) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Execute the train_step function with the current batch\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Obtain the loss and accuracy\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m loss, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Append the loss and accuracy to the results list\u001b[39;00m\n\u001b[1;32m     16\u001b[0m results\u001b[38;5;241m.\u001b[39mappend((loss, acc))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileohujbv4i.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(inputs, target)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     11\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model), (ag__\u001b[38;5;241m.\u001b[39mld(inputs),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[0;32m---> 12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(loss_fn), (ag__\u001b[38;5;241m.\u001b[39mld(target), ag__\u001b[38;5;241m.\u001b[39mld(predictions)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28msum\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mlosses,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope))\n\u001b[1;32m     13\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     14\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(optimizer)\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(gradients), ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/losses.py:139\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m   call_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx())\n\u001b[0;32m--> 139\u001b[0m losses \u001b[38;5;241m=\u001b[39m call_fn(y_true, y_pred)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m losses_utils\u001b[38;5;241m.\u001b[39mcompute_weighted_loss(\n\u001b[1;32m    141\u001b[0m     losses, sample_weight, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_reduction())\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/losses.py:243\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    240\u001b[0m   y_pred, y_true \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39msqueeze_or_expand_dimensions(y_pred, y_true)\n\u001b[1;32m    242\u001b[0m ag_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx())\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mag_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/losses.py:1787\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, label_smoothing, axis)\u001b[0m\n\u001b[1;32m   1782\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m y_true \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m label_smoothing) \u001b[38;5;241m+\u001b[39m (label_smoothing \u001b[38;5;241m/\u001b[39m num_classes)\n\u001b[1;32m   1784\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39msmart_cond\u001b[38;5;241m.\u001b[39msmart_cond(label_smoothing, _smooth_labels,\n\u001b[1;32m   1785\u001b[0m                                \u001b[38;5;28;01mlambda\u001b[39;00m: y_true)\n\u001b[0;32m-> 1787\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_crossentropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/backend.py:5119\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m   5117\u001b[0m target \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(target)\n\u001b[1;32m   5118\u001b[0m output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(output)\n\u001b[0;32m-> 5119\u001b[0m \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_is_compatible_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5121\u001b[0m \u001b[38;5;66;03m# Use logits whenever they are available. `softmax` and `sigmoid`\u001b[39;00m\n\u001b[1;32m   5122\u001b[0m \u001b[38;5;66;03m# activations cache logits on the `output` Tensor.\u001b[39;00m\n\u001b[1;32m   5123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(output, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_keras_logits\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-32-5cb04d2d4fb9>\", line 10, in train_step  *\n        loss = loss_fn(target, predictions) + sum(model.losses)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 139, in __call__  **\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1787, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5119, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 106580) and (None, 605195) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Initialize the epoch and step counters to -1\n",
    "# Create an empty list for storing training results\n",
    "epoch = step = -1\n",
    "results = []\n",
    "\n",
    "# Iterate through the batches in the loader_train data loader\n",
    "for batch in loader_train:\n",
    "    # Increment the step counter\n",
    "    step += 1\n",
    "\n",
    "    # Execute the train_step function with the current batch\n",
    "    # Obtain the loss and accuracy\n",
    "    loss, acc = train_step(*batch)\n",
    "\n",
    "    # Append the loss and accuracy to the results list\n",
    "    results.append((loss, acc))\n",
    "\n",
    "    # Check if the current step is equal to the number of steps per epoch (loader_train.steps_per_epoch)\n",
    "    if step == loader_train.steps_per_epoch:\n",
    "        # Reset the step counter to 0\n",
    "        # Increment the epoch counter\n",
    "        step = 0\n",
    "        epoch += 1\n",
    "\n",
    "        # Evaluate the model on the test set using the evaluate function (which should be defined beforehand)\n",
    "        # Store the test results in results_te\n",
    "        results_te = evaluate(loader_test)\n",
    "\n",
    "        # Print the epoch number, mean training loss and accuracy, and test loss and accuracy\n",
    "        print(\n",
    "            \"Ep. {} - Loss: {:.3f} - Acc: {:.3f} - Test loss: {:.3f} - Test acc: {:.3f}\".format(\n",
    "                epoch, *np.mean(results, 0), *results_te\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Reset the results list to start collecting results for the next epoch\n",
    "        results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
