{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spektral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout,Input\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from spektral.layers import GINConv,GCNConv #, GCSConv, GlobalAvgPool\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "from spektral.data import DisjointLoader, BatchLoader, Dataset, Graph\n",
    "#from spektral.transforms.normalize_adj import NormalizeAdj\n",
    "import gc\n",
    "import spektral.datasets\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos clase CTU13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según https://graphneural.network/creating-dataset/ para crear un dataset propio hay que crear una clase e incluir un método `download` y uno `read`: si al querer leer los datos, no los encuentra en el path correspondiente, entonces los descarga.\n",
    "En nuestro caso, además, lo que hace la \"descarga\" es \"acomodar\" los datos a un formato propio de Spektral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(x, norm=None):\n",
    "        \"\"\"\n",
    "        Apply one-hot encoding or z-score to a list of node features\n",
    "        \"\"\"\n",
    "        if norm == \"ohe\":\n",
    "            fnorm = OneHotEncoder(sparse=False, categories=\"auto\")\n",
    "        elif norm == \"zscore\":\n",
    "            fnorm = StandardScaler()\n",
    "        else:\n",
    "            return x\n",
    "        return fnorm.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTU13(Dataset):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def download(self):\n",
    "        os.mkdir(self.path)\n",
    "        captures = [\"10\",\"11\",\"12\",\"15\",\"15-2\",\"16\",\"16-2\",\"16-3\",\"17\",\"18\",\"18-2\",\"19\",\"15-3\"]\n",
    "\n",
    "        for i in captures:\n",
    "            # x = nodes features (ID, OD, IDW, ODW)\n",
    "            # a = adjacency matrix\n",
    "            # y =labels\n",
    "            \n",
    "            # Read files with nodes features (csv file) and connections between nodes (ncol file)\n",
    "            x_tmp = pd.read_csv(f'/mnt/features-prefix/capture201108{i}_features_prefix.csv', sep=\",\", header=0)\n",
    "            a_tmp = pd.read_csv(f'/mnt/ncol-prefix/capture201108{i}_ncol_prefix.ncol', sep=\" \", header=None, names=[\"source\", \"target\", \"weight\"])\n",
    "            \n",
    "            # Create dictionaries that identify each node and label with an integer\n",
    "            class_idx = {name: idx for idx, name in enumerate(sorted(x_tmp[\"label\"].unique()))}\n",
    "            node_idx = {name: idx for idx, name in enumerate(sorted(x_tmp[\"node\"].unique()))}\n",
    "            \n",
    "            # Change node names and label for their corresponding integer\n",
    "            x_tmp[\"node\"] = x_tmp[\"node\"].apply(lambda name: node_idx[name])\n",
    "            x_tmp[\"label\"] = x_tmp[\"label\"].apply(lambda value: class_idx[value])\n",
    "            a_tmp[\"source\"] = a_tmp[\"source\"].apply(lambda name: node_idx[name])\n",
    "            a_tmp[\"target\"] = a_tmp[\"target\"].apply(lambda name: node_idx[name])\n",
    "            \n",
    "            # Node features:\n",
    "            x = x_tmp.sort_values(\"node\")[x_tmp.columns.difference([\"node\",\"label\"], sort=False)].to_numpy()       \n",
    "            x = x.astype(np.float32)                \n",
    "            \n",
    "            # Separate source, target and weight to create a sparce matrix\n",
    "            a_source = a_tmp[[\"source\"]].to_numpy().T\n",
    "            a_source = np.reshape(a_source, a_source.shape[-1])\n",
    "            a_target = a_tmp[[\"target\"]].to_numpy().T\n",
    "            a_target = np.reshape(a_target, a_target.shape[-1])\n",
    "            a_weight = a_tmp[[\"weight\"]].to_numpy().T\n",
    "            a_weight = np.reshape(a_weight, a_weight.shape[-1])\n",
    "            # Adjacency matrix:\n",
    "            #a = sparse.coo_matrix((a_weight, (a_source, a_target)), shape=(x.shape[0], x.shape[0]))\n",
    "            a = sparse.csr_matrix((a_weight, (a_source, a_target)), shape=(x.shape[0], x.shape[0]))\n",
    "\n",
    "            # Label:\n",
    "            y = x_tmp.sort_values(\"node\")[\"label\"].to_numpy()\n",
    "            #y = y.astype(np.int64)\n",
    "            y = y.astype(np.float32)\n",
    "            y = _normalize(y[:, None], \"ohe\") #one-hot encoding\n",
    "            \n",
    "            # Save in format npz\n",
    "            filename = os.path.join(self.path, f'graph_201108{i}.npz')\n",
    "            np.savez(filename, x=x, a=a, y=y)\n",
    "\n",
    "            # Free memory\n",
    "            del x_tmp, x, a_tmp, a_source, a_target, a_weight, a, y\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "    def read(self):\n",
    "        # We must return a list of Graph objects\n",
    "        output = []\n",
    "        \n",
    "        captures = [\"10\",\"11\",\"12\",\"15\",\"15-2\",\"16\",\"16-2\",\"16-3\",\"17\",\"18\",\"18-2\",\"19\",\"15-3\"]\n",
    "\n",
    "        for i in captures:\n",
    "            data = np.load(os.path.join(self.path, f'graph_201108{i}.npz'), allow_pickle=True)\n",
    "            output.append(\n",
    "                Graph(x=data['x'], a=data['a'][()], y=data['y']) # también puede ser a=data['a'].item()\n",
    "            )\n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargamos el dataset\n",
    "\n",
    "Separamos en train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CTU13()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MEZCLAR LAS CAPTURAS CONDUCE A UN ERROR. QUEDA COMENTADO POR EL MOMENTO\n",
    "\n",
    "## capture number 9 (capture20110817) is for testing\n",
    "#dataset_test = dataset[8]\n",
    "\n",
    "## other captures are for training\n",
    "#np.random.seed(123)\n",
    "#dataset_tosplit = dataset[np.random.choice([0,1,2,3,4,5,6,7,9,10,11,12], 12, replace=False)]\n",
    "\n",
    "## split in training and validation\n",
    "#split = int(0.8 * len(dataset_tosplit))\n",
    "#dataset_train, dataset_val = dataset_tosplit[:split], dataset_tosplit[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CAPTURAS SIN MEZCLAR\n",
    "\n",
    "dataset_test = dataset[8]\n",
    "dataset_tosplit = dataset[0,1,2,3,4,5,6,7,9,10,11,12]\n",
    "split = int(0.8 * len(dataset_tosplit))\n",
    "dataset_train, dataset_val = dataset_tosplit[:split], dataset_tosplit[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(n_nodes=605195, n_node_features=4, n_edge_features=None, n_labels=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "batch_size = 1\n",
    "epochs = 20\n",
    "loader_train = DisjointLoader(dataset_train, node_level=True, batch_size=batch_size, epochs=epochs, shuffle=False)\n",
    "loader_val = DisjointLoader(dataset_val, node_level=True, batch_size=batch_size)\n",
    "loader_test = DisjointLoader(dataset_test, node_level=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copia del código de Harpo\n",
    "Lo que sigue es copia directa (o sea, sin pensar) de lo que subió Harpo en https://github.com/harpomaxx/graph-representation-learning/blob/harpo-branch-pkts/code/python/notebooks/spektral-example.ipynb\n",
    "\n",
    "Da error, lo próximo que voy a analizar es ver si ese error tiene que ver con que sean problemas diferentes (en el ejemplo de Harpo el objetivo es clasificar grafos, en el problema de CTU13 el objetivo es clasificar nodos).\n",
    "\n",
    "El error dice \"ValueError: Shapes (None, 106580) and (None, 605195) are incompatible\". \n",
    "\n",
    "A tener en cuenta: capture20110816 tiene 106580 nodos, mientras que capture20110810 tiene 604195 nodos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UPDATE:** el error sobre `shapes` al parecer se debía a que no le estaba pasando correctamente las etiquetas.\n",
    "Esperaba un array de forma `[n_nodes,n_labels]`, donde en este caso `n_labels=2` y yo se lo estaba pasando como `n_labels=1`\n",
    "\n",
    "Ahora se desprende otro error de una multiplicación de matrices cuando `batch_size=1`:\n",
    "```\n",
    "Node: 'gradient_tape/model/dense/MatMul/MatMul_1'\n",
    "Matrix size-incompatible: In[0]: [1,32], In[1]: [605195,2]\n",
    "\t [[{{node gradient_tape/model/dense/MatMul/MatMul_1}}]] [Op:__inference_train_step_1155]\n",
    "```\n",
    "\n",
    "Y otro error diferente cuando `batch_size=10`:\n",
    "```\n",
    "Node: 'categorical_crossentropy/softmax_cross_entropy_with_logits'\n",
    "logits and labels must be broadcastable: logits_size=[9,2] labels_size=[2424993,2]\n",
    "\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_step_1155]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gcn_model():\n",
    "    # Define input placeholders for node features, adjacency matrix, and segment indices\n",
    "    X_in = Input(shape=(dataset.n_node_features,))\n",
    "    A_in = Input((None,), sparse=True)\n",
    "    I_in = Input(shape=(), dtype=tf.int32)\n",
    "\n",
    "    # Apply the first GINConv layer with 32 units and ReLU activation\n",
    "    X_1 = GINConv(32, activation=\"relu\")([X_in, A_in])\n",
    "    # Apply dropout with a rate of 0.5\n",
    "    X_1 = Dropout(0.5)(X_1)\n",
    "    print(X_1.shape)\n",
    "\n",
    "    # Apply the second GINConv layer with 32 units and ReLU activation\n",
    "    X_2 = GINConv(32, activation=\"relu\")([X_1, A_in])\n",
    "    # Apply dropout with a rate of 0.5\n",
    "    X_2 = Dropout(0.5)(X_2)\n",
    "    print(X_2.shape)\n",
    "\n",
    "    # Aggregate the node features using the segment_mean function and the segment indices\n",
    "    X_3 = tf.math.segment_mean(X_2, I_in)\n",
    "    print(X_3.shape)\n",
    "    # Apply a dense output layer with the number of labels and softmax activation\n",
    "    out = Dense(dataset.n_labels, activation=\"softmax\")(X_3)\n",
    "    print(out.shape)\n",
    "\n",
    "    # Create and return the model with the defined inputs and outputs\n",
    "    model = Model(inputs=[X_in, A_in, I_in], outputs=out)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 32)\n",
      "(None, 32)\n",
      "(None, 32)\n",
      "(None, 2)\n"
     ]
    }
   ],
   "source": [
    "model = create_gcn_model()\n",
    "optimizer = Adam(lr=0.01)\n",
    "loss_fn = CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorate the function with @tf.function to compile as a TensorFlow graph\n",
    "# Use the input_signature from loader_train and relax shapes for varying graph sizes\n",
    "@tf.function(input_signature=loader_train.tf_signature(), experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    print(\"target:\",str(target))\n",
    "    # Create a GradientTape context to record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute model predictions with the inputs, set training=True for training-specific behaviors\n",
    "        predictions = model(inputs, training=True)\n",
    "        print(\"pred:\",str(predictions))\n",
    "        #predictions = tf.argmax(predictions1,axis=1)\n",
    "        # Calculate the loss using the provided loss_fn and add the model's regularization losses\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "\n",
    "    # Compute gradients of the loss with respect to the model's trainable variables\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Apply the gradients to the model's variables using the optimizer's apply_gradients method\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Compute the accuracy using the categorical_accuracy function from TensorFlow\n",
    "    # Calculate the mean accuracy using tf.reduce_mean\n",
    "    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "\n",
    "    # Return the loss and accuracy as output\n",
    "    return loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            tf.reduce_mean(categorical_accuracy(target, pred)),\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/model/dense/MatMul/MatMul_1' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 821, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 540, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-13-0082988e18f5>\", line 13, in <module>\n      loss, acc = train_step(*batch)\n    File \"<ipython-input-11-0822143c270f>\", line 16, in train_step\n      gradients = tape.gradient(loss, model.trainable_variables)\nNode: 'gradient_tape/model/dense/MatMul/MatMul_1'\nMatrix size-incompatible: In[0]: [1,32], In[1]: [605195,2]\n\t [[{{node gradient_tape/model/dense/MatMul/MatMul_1}}]] [Op:__inference_train_step_1155]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Execute the train_step function with the current batch\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Obtain the loss and accuracy\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m loss, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Append the loss and accuracy to the results list\u001b[39;00m\n\u001b[1;32m     16\u001b[0m results\u001b[38;5;241m.\u001b[39mappend((loss, acc))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/model/dense/MatMul/MatMul_1' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 821, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 540, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-13-0082988e18f5>\", line 13, in <module>\n      loss, acc = train_step(*batch)\n    File \"<ipython-input-11-0822143c270f>\", line 16, in train_step\n      gradients = tape.gradient(loss, model.trainable_variables)\nNode: 'gradient_tape/model/dense/MatMul/MatMul_1'\nMatrix size-incompatible: In[0]: [1,32], In[1]: [605195,2]\n\t [[{{node gradient_tape/model/dense/MatMul/MatMul_1}}]] [Op:__inference_train_step_1155]"
     ]
    }
   ],
   "source": [
    "# Initialize the epoch and step counters to -1\n",
    "# Create an empty list for storing training results\n",
    "epoch = step = -1\n",
    "results = []\n",
    "\n",
    "# Iterate through the batches in the loader_train data loader\n",
    "for batch in loader_train:\n",
    "    # Increment the step counter\n",
    "    step += 1\n",
    "\n",
    "    # Execute the train_step function with the current batch\n",
    "    # Obtain the loss and accuracy\n",
    "    loss, acc = train_step(*batch)\n",
    "\n",
    "    # Append the loss and accuracy to the results list\n",
    "    results.append((loss, acc))\n",
    "\n",
    "    # Check if the current step is equal to the number of steps per epoch (loader_train.steps_per_epoch)\n",
    "    if step == loader_train.steps_per_epoch:\n",
    "        # Reset the step counter to 0\n",
    "        # Increment the epoch counter\n",
    "        step = 0\n",
    "        epoch += 1\n",
    "\n",
    "        # Evaluate the model on the test set using the evaluate function (which should be defined beforehand)\n",
    "        # Store the test results in results_te\n",
    "        results_te = evaluate(loader_val) # CAMBIO A loader_val\n",
    "\n",
    "        # Print the epoch number, mean training loss and accuracy, and test loss and accuracy\n",
    "        print(\n",
    "            \"Ep. {} - Loss: {:.3f} - Acc: {:.3f} - Val loss: {:.3f} - Val acc: {:.3f}\".format(\n",
    "                epoch, *np.mean(results, 0), *results_te\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Reset the results list to start collecting results for the next epoch\n",
    "        results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo=loader_train.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((430265, 4), (430265, 2))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo[0][0].shape,algo[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_train.steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': {'spec': tensorflow.python.framework.tensor_spec.TensorSpec,\n",
       "  'shape': (None, 4),\n",
       "  'dtype': tf.float32},\n",
       " 'a': {'spec': tensorflow.python.framework.sparse_tensor.SparseTensorSpec,\n",
       "  'shape': (None, None),\n",
       "  'dtype': tf.int64},\n",
       " 'y': {'spec': tensorflow.python.framework.tensor_spec.TensorSpec,\n",
       "  'shape': (2,),\n",
       "  'dtype': tf.float64}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
