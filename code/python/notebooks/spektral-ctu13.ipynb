{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spektral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout,Input\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy #AUC \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from spektral.layers import GINConv,GCNConv #, GCSConv, GlobalAvgPool\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "from spektral.data import DisjointLoader, BatchLoader, Dataset, Graph\n",
    "from spektral.transforms.normalize_adj import NormalizeAdj\n",
    "import gc\n",
    "import spektral.datasets\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from spektral.models.gcn import GCN\n",
    "\n",
    "#from spektral.transforms import AdjToSpTensor, LayerPreprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos clase CTU13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según https://graphneural.network/creating-dataset/ para crear un dataset propio hay que crear una clase e incluir un método `download` y uno `read`: si al querer leer los datos, no los encuentra en el path correspondiente, entonces los descarga.\n",
    "En nuestro caso, además, lo que hace la \"descarga\" es \"acomodar\" los datos a un formato propio de Spektral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UPDATES\n",
    "\n",
    "**UPDATE 1:** Agrego función que hace un one-hot encoding en los labels\n",
    "\n",
    "**UPDATE 2:** Agrego links a los archivos con los que genero la clase `CTU13` para que los puedan descargar.\n",
    "\n",
    "**UPDATE 3:** Aplico normalización en la matriz de adyacencia al cargar los datos. Con este cambio, la loss da valores \"normales\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(x, norm=None):\n",
    "        \"\"\"\n",
    "        Copied from: https://github.com/danielegrattarola/spektral/blob/master/spektral/datasets/tudataset.py\n",
    "        \n",
    "        Apply one-hot encoding or z-score to a list of node features\n",
    "        \"\"\"\n",
    "        if norm == \"ohe\":\n",
    "            fnorm = OneHotEncoder(sparse_output=False, categories=\"auto\")\n",
    "        elif norm == \"zscore\":\n",
    "            fnorm = StandardScaler()\n",
    "        else:\n",
    "            return x\n",
    "        return fnorm.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class CTU13random(Dataset):\n",
    "class CTU13(Dataset):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def download(self):\n",
    "        os.mkdir(self.path)\n",
    "        \n",
    "        captures = [\"10\",\"11\",\"12\",\"15\",\"15-2\",\"16\",\"16-2\",\"16-3\",\"17\",\"18\",\"18-2\",\"19\",\"15-3\"]\n",
    "        \n",
    "        # para poder descargar los archivos con las features y los ncol de cada captura\n",
    "        features_links = [\"ktAgJS22kBXcMsT\", \n",
    "                          \"qfM8pjZgDi9EFfd\",\n",
    "                          \"34PWCn9JmdDA9HE\",\n",
    "                          \"YtcCmiNEyPiYB4Y\",\n",
    "                          \"D2HdaEZYcFz5ryt\",\n",
    "                          \"cTY262moFLGjtmn\",\n",
    "                          \"TbfPFbSKWqYqR7n\",\n",
    "                          \"s2GTjFz8rNxbs4z\",\n",
    "                          \"9dZPAENNACreDEK\",\n",
    "                          \"bwR2Zrky49JjtgA\",\n",
    "                          \"CmYc9JyBsHwzaYD\",\n",
    "                          \"TNSkGJcq2CPoFtM\",\n",
    "                          \"XwZFrQYzMLNJxAY\"\n",
    "        ]\n",
    "\n",
    "        ncol_links = [\"B5EBDnAr4z55cc9\",\n",
    "                      \"Pz4ba4jn3nCNgAp\",\n",
    "                      \"EbkwSBHyAkHmdHE\",\n",
    "                      \"ttyoxLc36s7ABCB\",\n",
    "                      \"R3b9fe25x6ncoaT\",\n",
    "                      \"wFZ72f9kL3XFki6\",\n",
    "                      \"7EcYp9ACPqkQqDs\",\n",
    "                      \"YcTZCARwKCY2jiB\",\n",
    "                      \"3cc8mcTZaEC9LGM\",\n",
    "                      \"NDgw4PwXAwQKgb2\",\n",
    "                      \"wY38ypkj7QSJYib\",\n",
    "                      \"dEZYJ84z53ozZZo\",\n",
    "                      \"NKdZfBX6DG9nB8o\"            \n",
    "        ]\n",
    "        \n",
    "        for i in range(len(captures)):\n",
    "            # x = nodes features (ID, OD, IDW, ODW)\n",
    "            # a = adjacency matrix\n",
    "            # y =labels\n",
    "            \n",
    "            # Read files with nodes features (csv file) and connections between nodes (ncol file)\n",
    "            x_tmp = pd.read_csv(f'https://nube.ingenieria.uncuyo.edu.ar/s/{features_links[i]}/download', sep=\",\", header=0)\n",
    "            a_tmp = pd.read_csv(f'https://nube.ingenieria.uncuyo.edu.ar/s/{ncol_links[i]}/download', sep=\" \", header=None, names=[\"source\", \"target\", \"weight\"])\n",
    "            \n",
    "            # Create dictionaries that identify each node and label with an integer\n",
    "            class_idx = {name: idx for idx, name in enumerate(sorted(x_tmp[\"label\"].unique()))}\n",
    "            node_idx = {name: idx for idx, name in enumerate(sorted(x_tmp[\"node\"].unique()))}\n",
    "            \n",
    "            # Change node names and label for their corresponding integer\n",
    "            x_tmp[\"node\"] = x_tmp[\"node\"].apply(lambda name: node_idx[name])\n",
    "            x_tmp[\"label\"] = x_tmp[\"label\"].apply(lambda value: class_idx[value])\n",
    "            a_tmp[\"source\"] = a_tmp[\"source\"].apply(lambda name: node_idx[name])\n",
    "            a_tmp[\"target\"] = a_tmp[\"target\"].apply(lambda name: node_idx[name])\n",
    "            \n",
    "            # Node features:\n",
    "            x = x_tmp.sort_values(\"node\")[x_tmp.columns.difference([\"node\",\"label\"], sort=False)].to_numpy()       \n",
    "            x = x.astype(np.float32)                \n",
    "            \n",
    "            # Separate source, target and weight to create a sparce matrix\n",
    "            a_source = a_tmp[[\"source\"]].to_numpy().T\n",
    "            a_source = np.reshape(a_source, a_source.shape[-1])\n",
    "            a_target = a_tmp[[\"target\"]].to_numpy().T\n",
    "            a_target = np.reshape(a_target, a_target.shape[-1])\n",
    "            a_weight = a_tmp[[\"weight\"]].to_numpy().T\n",
    "            a_weight = np.reshape(a_weight, a_weight.shape[-1])\n",
    "            # Adjacency matrix:\n",
    "            #a = sparse.coo_matrix((a_weight, (a_source, a_target)), shape=(x.shape[0], x.shape[0]))\n",
    "            a = sparse.csr_matrix((a_weight, (a_source, a_target)), shape=(x.shape[0], x.shape[0]), dtype=np.float32)\n",
    "\n",
    "            # Label (CTU13):\n",
    "            y = x_tmp.sort_values(\"node\")[\"label\"].to_numpy()\n",
    "            y = y.astype(np.float32)\n",
    "            y = _normalize(y[:, None], \"ohe\") #one-hot encoding\n",
    "            \n",
    "            # Label (CTU13random):\n",
    "#             y = []\n",
    "#             for j in range(x_tmp.shape[0]):\n",
    "#                 np.random.seed(i+j)\n",
    "#                 y_tmp = np.random.randint(0,2)\n",
    "#                 if y_tmp:\n",
    "#                     y.append(np.array([1., 0.]))\n",
    "#                 else:\n",
    "#                     y.append(np.array([0., 1.]))\n",
    "#             y = np.array(y)\n",
    "#             y.astype(np.float32)    \n",
    "            \n",
    "            # Save in format npz\n",
    "            filename = os.path.join(self.path, f'graph_201108{captures[i]}.npz')\n",
    "            np.savez(filename, x=x, a=a, y=y)\n",
    "\n",
    "            # Free memory\n",
    "            del x_tmp, x, a_tmp, a_source, a_target, a_weight, a, y\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "    def read(self):\n",
    "        # We must return a list of Graph objects\n",
    "        output = []\n",
    "        \n",
    "        captures = [\"10\",\"11\",\"12\",\"15\",\"15-2\",\"16\",\"16-2\",\"16-3\",\"17\",\"18\",\"18-2\",\"19\",\"15-3\"]\n",
    "\n",
    "        for i in captures:\n",
    "            data = np.load(os.path.join(self.path, f'graph_201108{i}.npz'), allow_pickle=True)\n",
    "            output.append(\n",
    "                Graph(x=data['x'], a=data['a'][()], y=data['y']) # también puede ser a=data['a'].item()\n",
    "            )\n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargamos el dataset\n",
    "\n",
    "Separamos en train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = CTU13random(transforms=[NormalizeAdj()])\n",
    "\n",
    "dataset = CTU13(transforms=[NormalizeAdj()]) #normalize_x=True, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MEZCLAR LAS CAPTURAS CONDUCE A UN ERROR. QUEDA COMENTADO POR EL MOMENTO\n",
    "\n",
    "## capture number 9 (capture20110817) is for testing\n",
    "#dataset_test = dataset[8]\n",
    "\n",
    "## other captures are for training\n",
    "#np.random.seed(123)\n",
    "#dataset_tosplit = dataset[np.random.choice([0,1,2,3,4,5,6,7,9,10,11,12], 12, replace=False)]\n",
    "\n",
    "## split in training and validation\n",
    "#split = int(0.8 * len(dataset_tosplit))\n",
    "#dataset_train, dataset_val = dataset_tosplit[:split], dataset_tosplit[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CAPTURAS SIN MEZCLAR\n",
    "\n",
    "dataset_test = dataset[8:9] # capture number 9 for testing (20110817)\n",
    "dataset_test_Tati = dataset[10:11] # capture number 11 for external validation (20110818-2)\n",
    "\n",
    "dataset_tosplit = dataset[0,1,2,3,4,5,6,7,9,11,12]\n",
    "split = int(0.8 * len(dataset_tosplit))\n",
    "dataset_train, dataset_val = dataset_tosplit[:split], dataset_tosplit[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "batch_size = 1\n",
    "epochs = 200\n",
    "loader_train = DisjointLoader(dataset_train, node_level=True, batch_size=batch_size, epochs=epochs, shuffle=False)\n",
    "loader_val = DisjointLoader(dataset_val, node_level=True, batch_size=batch_size, shuffle=False)\n",
    "loader_test = DisjointLoader(dataset_test, node_level=True, batch_size=batch_size, shuffle=False)\n",
    "loader_test_Tati = DisjointLoader(dataset_test_Tati, node_level=True, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copia del código de Harpo\n",
    "Lo que sigue es copia directa (o sea, sin pensar) de lo que subió Harpo en https://github.com/harpomaxx/graph-representation-learning/blob/harpo-branch-pkts/code/python/notebooks/spektral-example.ipynb\n",
    "\n",
    "Da error, lo próximo que voy a analizar es ver si ese error tiene que ver con que sean problemas diferentes (en el ejemplo de Harpo el objetivo es clasificar grafos, en el problema de CTU13 el objetivo es clasificar nodos).\n",
    "\n",
    "El error dice \"ValueError: Shapes (None, 106580) and (None, 605195) are incompatible\". \n",
    "\n",
    "A tener en cuenta: capture20110816 tiene 106580 nodos, mientras que capture20110810 tiene 604195 nodos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UPDATES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UPDATE:** el error sobre `shapes` al parecer se debía a que no le estaba pasando correctamente las etiquetas.\n",
    "Esperaba un array de forma `[n_nodes,n_labels]`, donde en este caso `n_labels=2` y yo le estaba pasando `n_labels` como si fuera `n_nodes`.\n",
    "\n",
    "Ahora se desprende otro error de una multiplicación de matrices cuando `batch_size=1`:\n",
    "```\n",
    "Node: 'gradient_tape/model/dense/MatMul/MatMul_1'\n",
    "Matrix size-incompatible: In[0]: [1,32], In[1]: [605195,2]\n",
    "\t [[{{node gradient_tape/model/dense/MatMul/MatMul_1}}]] [Op:__inference_train_step_1155]\n",
    "```\n",
    "\n",
    "Y otro error diferente cuando `batch_size=10`:\n",
    "```\n",
    "Node: 'categorical_crossentropy/softmax_cross_entropy_with_logits'\n",
    "logits and labels must be broadcastable: logits_size=[9,2] labels_size=[2424993,2]\n",
    "\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_step_1155]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UPDATE 2:**\n",
    "\n",
    "El modelo `create_gcn_model` no anda (problemas con las dimensiones... tiene sentido porque la capa densa tiene dos neuronas, y lo que nosotros queremos es que prediga la clase para cada nodo. No sé cómo podría modificarlo para que eso depdenda de cada grafo).\n",
    "\n",
    "Por el contrario, lo que hice fue ignorar la función `create_gcn_model` y utilizar directamente el modelo `GCN` de spektral, y con eso al menos empezó a entrenar!!\n",
    "\n",
    "También modifiqué la loss por una `BinaryCrossentropy`.\n",
    "\n",
    "Ahora hay que ver qué está haciendo el modelo y qué tan bien anda..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IGNORAR ESTA FUNCION\n",
    "\n",
    "def create_gcn_model():\n",
    "    # Define input placeholders for node features, adjacency matrix, and segment indices\n",
    "    X_in = Input(shape=(dataset.n_node_features,))\n",
    "    A_in = Input((None,), sparse=True)\n",
    "    I_in = Input(shape=(), dtype=tf.int32)\n",
    "\n",
    "    # Apply the first GINConv layer with 32 units and ReLU activation\n",
    "    X_1 = GINConv(32, activation=\"relu\")([X_in, A_in])\n",
    "    # Apply dropout with a rate of 0.5\n",
    "    X_1 = Dropout(0.5)(X_1)\n",
    "    print(X_1.shape)\n",
    "\n",
    "    # Apply the second GINConv layer with 32 units and ReLU activation\n",
    "    X_2 = GINConv(32, activation=\"relu\")([X_1, A_in])\n",
    "    # Apply dropout with a rate of 0.5\n",
    "    X_2 = Dropout(0.5)(X_2)\n",
    "    print(X_2.shape)\n",
    "\n",
    "    # Aggregate the node features using the segment_mean function and the segment indices\n",
    "    X_3 = tf.math.segment_mean(X_2, I_in)\n",
    "    print(X_3.shape)\n",
    "    # Apply a dense output layer with the number of labels and softmax activation\n",
    "    out = Dense(dataset.n_labels, activation=\"softmax\")(X_3)\n",
    "    print(out.shape)\n",
    "\n",
    "    # Create and return the model with the defined inputs and outputs\n",
    "    model = Model(inputs=[X_in, A_in, I_in], outputs=out)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model = create_gcn_model()\n",
    "\n",
    "model = GCN(n_labels=dataset.n_labels)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "#loss_fn = CategoricalCrossentropy()\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorate the function with @tf.function to compile as a TensorFlow graph\n",
    "# Use the input_signature from loader_train and relax shapes for varying graph sizes\n",
    "@tf.function(input_signature=loader_train.tf_signature(), experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    # Create a GradientTape context to record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute model predictions with the inputs, set training=True for training-specific behaviors\n",
    "        predictions = model(inputs, training=True)\n",
    "        # Calculate the loss using the provided loss_fn and add the model's regularization losses\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "\n",
    "    # Compute gradients of the loss with respect to the model's trainable variables\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Apply the gradients to the model's variables using the optimizer's apply_gradients method\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Compute the accuracy using the categorical_accuracy function from TensorFlow\n",
    "    # Calculate the mean accuracy using tf.reduce_mean\n",
    "    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "\n",
    "    # Return the loss and accuracy as output\n",
    "    return loss, acc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            tf.reduce_mean(categorical_accuracy(target, pred)),\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 0 - Loss: 72.597 - Acc: 0.69936 - Val loss: 1.109 - Val acc: 0.93471\n",
      "Ep. 1 - Loss: 16.511 - Acc: 0.89649 - Val loss: 1.772 - Val acc: 0.93471\n",
      "Ep. 2 - Loss: 8.007 - Acc: 0.91935 - Val loss: 2.296 - Val acc: 0.93471\n",
      "Ep. 3 - Loss: 2.246 - Acc: 0.92258 - Val loss: 2.763 - Val acc: 0.93471\n",
      "Ep. 4 - Loss: 4.459 - Acc: 0.82107 - Val loss: 3.043 - Val acc: 0.93471\n",
      "Ep. 5 - Loss: 0.511 - Acc: 0.92483 - Val loss: 3.248 - Val acc: 0.93471\n",
      "Ep. 6 - Loss: 1.737 - Acc: 0.92504 - Val loss: 3.413 - Val acc: 0.93471\n",
      "Ep. 7 - Loss: 2.715 - Acc: 0.92430 - Val loss: 3.554 - Val acc: 0.93471\n",
      "Ep. 8 - Loss: 3.110 - Acc: 0.91561 - Val loss: 3.689 - Val acc: 0.93471\n",
      "Ep. 9 - Loss: 1.063 - Acc: 0.92493 - Val loss: 3.714 - Val acc: 0.93471\n",
      "Ep. 10 - Loss: 1.591 - Acc: 0.81909 - Val loss: 3.700 - Val acc: 0.93471\n",
      "Ep. 11 - Loss: 1.538 - Acc: 0.92514 - Val loss: 3.675 - Val acc: 0.93471\n",
      "Ep. 12 - Loss: 0.516 - Acc: 0.92497 - Val loss: 3.661 - Val acc: 0.93471\n",
      "Ep. 13 - Loss: 1.100 - Acc: 0.92457 - Val loss: 3.637 - Val acc: 0.93471\n",
      "Ep. 14 - Loss: 1.155 - Acc: 0.92557 - Val loss: 3.659 - Val acc: 0.93471\n",
      "Ep. 15 - Loss: 1.268 - Acc: 0.92499 - Val loss: 3.756 - Val acc: 0.93471\n",
      "Ep. 16 - Loss: 0.895 - Acc: 0.92573 - Val loss: 3.810 - Val acc: 0.93471\n",
      "Ep. 17 - Loss: 0.646 - Acc: 0.92532 - Val loss: 3.833 - Val acc: 0.93471\n",
      "Ep. 18 - Loss: 0.496 - Acc: 0.92563 - Val loss: 3.829 - Val acc: 0.93471\n",
      "Ep. 19 - Loss: 0.737 - Acc: 0.92542 - Val loss: 3.848 - Val acc: 0.93471\n",
      "Ep. 20 - Loss: 0.436 - Acc: 0.92567 - Val loss: 3.840 - Val acc: 0.93471\n",
      "Ep. 21 - Loss: 1.099 - Acc: 0.92527 - Val loss: 3.824 - Val acc: 0.93471\n",
      "Ep. 22 - Loss: 0.857 - Acc: 0.92551 - Val loss: 3.792 - Val acc: 0.93471\n",
      "Ep. 23 - Loss: 0.461 - Acc: 0.92531 - Val loss: 3.767 - Val acc: 0.93471\n",
      "Ep. 24 - Loss: 0.594 - Acc: 0.92568 - Val loss: 3.730 - Val acc: 0.93471\n",
      "Ep. 25 - Loss: 1.217 - Acc: 0.92551 - Val loss: 3.791 - Val acc: 0.93471\n",
      "Ep. 26 - Loss: 0.734 - Acc: 0.92567 - Val loss: 3.826 - Val acc: 0.93471\n",
      "Ep. 27 - Loss: 1.078 - Acc: 0.92554 - Val loss: 3.812 - Val acc: 0.93471\n",
      "Ep. 28 - Loss: 0.892 - Acc: 0.92375 - Val loss: 3.783 - Val acc: 0.93471\n",
      "Ep. 29 - Loss: 0.675 - Acc: 0.92308 - Val loss: 3.754 - Val acc: 0.93471\n",
      "Ep. 30 - Loss: 0.792 - Acc: 0.92573 - Val loss: 3.641 - Val acc: 0.93471\n",
      "Ep. 31 - Loss: 0.609 - Acc: 0.92548 - Val loss: 3.667 - Val acc: 0.93471\n",
      "Ep. 32 - Loss: 0.419 - Acc: 0.92544 - Val loss: 3.669 - Val acc: 0.93471\n",
      "Ep. 33 - Loss: 0.474 - Acc: 0.92547 - Val loss: 3.646 - Val acc: 0.93471\n",
      "Ep. 34 - Loss: 0.504 - Acc: 0.92499 - Val loss: 3.621 - Val acc: 0.93471\n",
      "Ep. 35 - Loss: 0.532 - Acc: 0.92555 - Val loss: 3.590 - Val acc: 0.93471\n",
      "Ep. 36 - Loss: 0.560 - Acc: 0.92573 - Val loss: 3.560 - Val acc: 0.93471\n",
      "Ep. 37 - Loss: 0.626 - Acc: 0.92512 - Val loss: 3.526 - Val acc: 0.93471\n",
      "Ep. 38 - Loss: 0.652 - Acc: 0.92558 - Val loss: 3.490 - Val acc: 0.93471\n",
      "Ep. 39 - Loss: 0.649 - Acc: 0.91915 - Val loss: 3.458 - Val acc: 0.93471\n",
      "Ep. 40 - Loss: 0.661 - Acc: 0.92569 - Val loss: 3.416 - Val acc: 0.93471\n",
      "Ep. 41 - Loss: 0.649 - Acc: 0.92563 - Val loss: 3.383 - Val acc: 0.93471\n",
      "Ep. 42 - Loss: 0.706 - Acc: 0.92510 - Val loss: 3.363 - Val acc: 0.93471\n",
      "Ep. 43 - Loss: 0.494 - Acc: 0.92460 - Val loss: 3.329 - Val acc: 0.93471\n",
      "Ep. 44 - Loss: 0.438 - Acc: 0.92517 - Val loss: 3.296 - Val acc: 0.93471\n",
      "Ep. 45 - Loss: 0.564 - Acc: 0.92528 - Val loss: 3.277 - Val acc: 0.93471\n",
      "Ep. 46 - Loss: 0.935 - Acc: 0.92524 - Val loss: 3.381 - Val acc: 0.93471\n",
      "Ep. 47 - Loss: 1.400 - Acc: 0.92427 - Val loss: 3.448 - Val acc: 0.93471\n",
      "Ep. 48 - Loss: 1.364 - Acc: 0.92498 - Val loss: 3.500 - Val acc: 0.93471\n",
      "Ep. 49 - Loss: 0.625 - Acc: 0.92549 - Val loss: 3.526 - Val acc: 0.93471\n",
      "Ep. 50 - Loss: 0.698 - Acc: 0.92576 - Val loss: 3.492 - Val acc: 0.93471\n",
      "Ep. 51 - Loss: 0.685 - Acc: 0.92516 - Val loss: 3.434 - Val acc: 0.93471\n",
      "Ep. 52 - Loss: 0.370 - Acc: 0.92570 - Val loss: 3.389 - Val acc: 0.93471\n",
      "Ep. 53 - Loss: 0.341 - Acc: 0.92572 - Val loss: 3.358 - Val acc: 0.93471\n",
      "Ep. 54 - Loss: 0.579 - Acc: 0.92534 - Val loss: 3.307 - Val acc: 0.93471\n",
      "Ep. 55 - Loss: 0.532 - Acc: 0.92566 - Val loss: 3.270 - Val acc: 0.93471\n",
      "Ep. 56 - Loss: 0.632 - Acc: 0.92550 - Val loss: 3.220 - Val acc: 0.93471\n",
      "Ep. 57 - Loss: 0.405 - Acc: 0.92571 - Val loss: 3.176 - Val acc: 0.93471\n",
      "Ep. 58 - Loss: 0.552 - Acc: 0.92538 - Val loss: 3.116 - Val acc: 0.93471\n",
      "Ep. 59 - Loss: 0.635 - Acc: 0.92552 - Val loss: 3.059 - Val acc: 0.93471\n",
      "Ep. 60 - Loss: 0.402 - Acc: 0.92566 - Val loss: 3.014 - Val acc: 0.93471\n",
      "Ep. 61 - Loss: 0.465 - Acc: 0.92572 - Val loss: 2.969 - Val acc: 0.93471\n",
      "Ep. 62 - Loss: 0.332 - Acc: 0.92525 - Val loss: 2.927 - Val acc: 0.93471\n",
      "Ep. 63 - Loss: 1.104 - Acc: 0.92418 - Val loss: 2.878 - Val acc: 0.93471\n",
      "Ep. 64 - Loss: 0.525 - Acc: 0.92551 - Val loss: 2.821 - Val acc: 0.93471\n",
      "Ep. 65 - Loss: 0.515 - Acc: 0.92562 - Val loss: 2.805 - Val acc: 0.93471\n",
      "Ep. 66 - Loss: 0.500 - Acc: 0.92560 - Val loss: 2.784 - Val acc: 0.93471\n",
      "Ep. 67 - Loss: 0.534 - Acc: 0.92536 - Val loss: 2.740 - Val acc: 0.93471\n",
      "Ep. 68 - Loss: 0.490 - Acc: 0.92570 - Val loss: 2.692 - Val acc: 0.93471\n",
      "Ep. 69 - Loss: 0.706 - Acc: 0.92528 - Val loss: 2.652 - Val acc: 0.93471\n",
      "Ep. 70 - Loss: 0.569 - Acc: 0.92574 - Val loss: 2.609 - Val acc: 0.93471\n",
      "Ep. 71 - Loss: 0.489 - Acc: 0.92546 - Val loss: 2.568 - Val acc: 0.93471\n",
      "Ep. 72 - Loss: 0.655 - Acc: 0.92552 - Val loss: 2.513 - Val acc: 0.93471\n",
      "Ep. 73 - Loss: 0.526 - Acc: 0.92504 - Val loss: 2.469 - Val acc: 0.93471\n",
      "Ep. 74 - Loss: 0.540 - Acc: 0.82637 - Val loss: 2.421 - Val acc: 0.93471\n",
      "Ep. 75 - Loss: 0.417 - Acc: 0.92560 - Val loss: 2.380 - Val acc: 0.93471\n",
      "Ep. 76 - Loss: 0.380 - Acc: 0.92560 - Val loss: 2.338 - Val acc: 0.93471\n",
      "Ep. 77 - Loss: 0.360 - Acc: 0.92550 - Val loss: 2.310 - Val acc: 0.93471\n",
      "Ep. 78 - Loss: 0.449 - Acc: 0.92553 - Val loss: 2.272 - Val acc: 0.93471\n",
      "Ep. 79 - Loss: 0.256 - Acc: 0.92564 - Val loss: 2.239 - Val acc: 0.93471\n",
      "Ep. 80 - Loss: 0.352 - Acc: 0.92529 - Val loss: 2.212 - Val acc: 0.93471\n",
      "Ep. 81 - Loss: 0.476 - Acc: 0.92571 - Val loss: 2.166 - Val acc: 0.93471\n",
      "Ep. 82 - Loss: 0.347 - Acc: 0.92533 - Val loss: 2.137 - Val acc: 0.93471\n",
      "Ep. 83 - Loss: 0.416 - Acc: 0.92562 - Val loss: 2.098 - Val acc: 0.93471\n",
      "Ep. 84 - Loss: 0.542 - Acc: 0.83760 - Val loss: 2.048 - Val acc: 0.93471\n",
      "Ep. 85 - Loss: 0.355 - Acc: 0.92479 - Val loss: 1.997 - Val acc: 0.93471\n",
      "Ep. 86 - Loss: 0.263 - Acc: 0.92533 - Val loss: 1.953 - Val acc: 0.93471\n",
      "Ep. 87 - Loss: 0.317 - Acc: 0.92557 - Val loss: 1.923 - Val acc: 0.93471\n",
      "Ep. 88 - Loss: 0.271 - Acc: 0.92560 - Val loss: 1.927 - Val acc: 0.93471\n",
      "Ep. 89 - Loss: 0.706 - Acc: 0.91179 - Val loss: 1.906 - Val acc: 0.93471\n",
      "Ep. 90 - Loss: 0.383 - Acc: 0.92549 - Val loss: 1.872 - Val acc: 0.93471\n",
      "Ep. 91 - Loss: 0.451 - Acc: 0.92569 - Val loss: 1.830 - Val acc: 0.93471\n",
      "Ep. 92 - Loss: 0.499 - Acc: 0.92559 - Val loss: 1.781 - Val acc: 0.93471\n",
      "Ep. 93 - Loss: 0.264 - Acc: 0.92566 - Val loss: 1.745 - Val acc: 0.93471\n",
      "Ep. 94 - Loss: 0.383 - Acc: 0.92511 - Val loss: 1.714 - Val acc: 0.93471\n",
      "Ep. 95 - Loss: 0.494 - Acc: 0.92534 - Val loss: 1.668 - Val acc: 0.93471\n",
      "Ep. 96 - Loss: 0.320 - Acc: 0.92569 - Val loss: 1.625 - Val acc: 0.93471\n",
      "Ep. 97 - Loss: 0.323 - Acc: 0.92526 - Val loss: 1.598 - Val acc: 0.93471\n",
      "Ep. 98 - Loss: 0.294 - Acc: 0.92549 - Val loss: 1.578 - Val acc: 0.93471\n",
      "Ep. 99 - Loss: 0.301 - Acc: 0.92518 - Val loss: 1.617 - Val acc: 0.93471\n",
      "Ep. 100 - Loss: 0.345 - Acc: 0.92540 - Val loss: 1.604 - Val acc: 0.93471\n",
      "Ep. 101 - Loss: 0.370 - Acc: 0.92530 - Val loss: 1.661 - Val acc: 0.93471\n",
      "Ep. 102 - Loss: 0.379 - Acc: 0.92565 - Val loss: 1.672 - Val acc: 0.93471\n",
      "Ep. 103 - Loss: 0.214 - Acc: 0.92553 - Val loss: 1.657 - Val acc: 0.93471\n",
      "Ep. 104 - Loss: 0.327 - Acc: 0.92554 - Val loss: 1.640 - Val acc: 0.93471\n",
      "Ep. 105 - Loss: 0.337 - Acc: 0.92512 - Val loss: 1.650 - Val acc: 0.93471\n",
      "Ep. 106 - Loss: 0.353 - Acc: 0.91991 - Val loss: 1.657 - Val acc: 0.93471\n",
      "Ep. 107 - Loss: 0.349 - Acc: 0.92526 - Val loss: 1.637 - Val acc: 0.93471\n",
      "Ep. 108 - Loss: 0.290 - Acc: 0.92571 - Val loss: 1.609 - Val acc: 0.93471\n",
      "Ep. 109 - Loss: 0.303 - Acc: 0.92531 - Val loss: 1.580 - Val acc: 0.93471\n",
      "Ep. 110 - Loss: 0.337 - Acc: 0.92560 - Val loss: 1.554 - Val acc: 0.93471\n",
      "Ep. 111 - Loss: 0.389 - Acc: 0.92564 - Val loss: 1.518 - Val acc: 0.93471\n",
      "Ep. 112 - Loss: 0.197 - Acc: 0.92485 - Val loss: 1.494 - Val acc: 0.93471\n",
      "Ep. 113 - Loss: 0.398 - Acc: 0.91537 - Val loss: 1.497 - Val acc: 0.93471\n",
      "Ep. 114 - Loss: 0.363 - Acc: 0.92527 - Val loss: 1.485 - Val acc: 0.93471\n",
      "Ep. 115 - Loss: 0.278 - Acc: 0.92567 - Val loss: 1.461 - Val acc: 0.93471\n",
      "Ep. 116 - Loss: 0.258 - Acc: 0.92507 - Val loss: 1.438 - Val acc: 0.93471\n",
      "Ep. 117 - Loss: 0.314 - Acc: 0.92553 - Val loss: 1.409 - Val acc: 0.93471\n",
      "Ep. 118 - Loss: 0.288 - Acc: 0.92543 - Val loss: 1.382 - Val acc: 0.93471\n",
      "Ep. 119 - Loss: 0.257 - Acc: 0.92551 - Val loss: 1.351 - Val acc: 0.93471\n",
      "Ep. 120 - Loss: 0.299 - Acc: 0.92457 - Val loss: 1.316 - Val acc: 0.93471\n",
      "Ep. 121 - Loss: 0.354 - Acc: 0.92552 - Val loss: 1.269 - Val acc: 0.93471\n",
      "Ep. 122 - Loss: 0.249 - Acc: 0.92539 - Val loss: 1.236 - Val acc: 0.93471\n",
      "Ep. 123 - Loss: 0.270 - Acc: 0.92528 - Val loss: 1.215 - Val acc: 0.93471\n",
      "Ep. 124 - Loss: 0.265 - Acc: 0.92542 - Val loss: 1.182 - Val acc: 0.93471\n",
      "Ep. 125 - Loss: 0.262 - Acc: 0.92461 - Val loss: 1.151 - Val acc: 0.93471\n",
      "Ep. 126 - Loss: 0.199 - Acc: 0.92236 - Val loss: 1.124 - Val acc: 0.93471\n",
      "Ep. 127 - Loss: 0.205 - Acc: 0.92562 - Val loss: 1.104 - Val acc: 0.93471\n",
      "Ep. 128 - Loss: 0.197 - Acc: 0.92536 - Val loss: 1.088 - Val acc: 0.93471\n",
      "Ep. 129 - Loss: 0.255 - Acc: 0.92540 - Val loss: 1.068 - Val acc: 0.93471\n",
      "Ep. 130 - Loss: 0.232 - Acc: 0.92543 - Val loss: 1.053 - Val acc: 0.93471\n",
      "Ep. 131 - Loss: 0.172 - Acc: 0.92404 - Val loss: 1.053 - Val acc: 0.93471\n",
      "Ep. 132 - Loss: 0.242 - Acc: 0.92560 - Val loss: 1.050 - Val acc: 0.93471\n",
      "Ep. 133 - Loss: 0.266 - Acc: 0.92552 - Val loss: 1.023 - Val acc: 0.93471\n",
      "Ep. 134 - Loss: 0.321 - Acc: 0.92558 - Val loss: 0.984 - Val acc: 0.93471\n",
      "Ep. 135 - Loss: 0.221 - Acc: 0.92545 - Val loss: 0.953 - Val acc: 0.93471\n",
      "Ep. 136 - Loss: 0.203 - Acc: 0.92544 - Val loss: 0.938 - Val acc: 0.93471\n",
      "Ep. 137 - Loss: 0.258 - Acc: 0.92485 - Val loss: 0.919 - Val acc: 0.93471\n",
      "Ep. 138 - Loss: 0.244 - Acc: 0.92532 - Val loss: 0.899 - Val acc: 0.93471\n",
      "Ep. 139 - Loss: 0.286 - Acc: 0.92519 - Val loss: 0.880 - Val acc: 0.93471\n",
      "Ep. 140 - Loss: 0.296 - Acc: 0.81878 - Val loss: 0.854 - Val acc: 0.93471\n",
      "Ep. 141 - Loss: 0.253 - Acc: 0.92532 - Val loss: 0.839 - Val acc: 0.93471\n",
      "Ep. 142 - Loss: 0.150 - Acc: 0.92523 - Val loss: 0.828 - Val acc: 0.93471\n",
      "Ep. 143 - Loss: 0.155 - Acc: 0.92095 - Val loss: 0.823 - Val acc: 0.93471\n",
      "Ep. 144 - Loss: 0.142 - Acc: 0.92517 - Val loss: 0.830 - Val acc: 0.93471\n",
      "Ep. 145 - Loss: 0.223 - Acc: 0.92456 - Val loss: 0.835 - Val acc: 0.93471\n",
      "Ep. 146 - Loss: 0.176 - Acc: 0.92547 - Val loss: 0.845 - Val acc: 0.93471\n",
      "Ep. 147 - Loss: 0.287 - Acc: 0.92509 - Val loss: 0.838 - Val acc: 0.93471\n",
      "Ep. 148 - Loss: 0.245 - Acc: 0.92524 - Val loss: 0.833 - Val acc: 0.93471\n",
      "Ep. 149 - Loss: 0.167 - Acc: 0.92358 - Val loss: 0.823 - Val acc: 0.93471\n",
      "Ep. 150 - Loss: 0.178 - Acc: 0.92557 - Val loss: 0.812 - Val acc: 0.93471\n",
      "Ep. 151 - Loss: 0.133 - Acc: 0.92559 - Val loss: 0.798 - Val acc: 0.93471\n",
      "Ep. 152 - Loss: 0.241 - Acc: 0.92541 - Val loss: 0.773 - Val acc: 0.93471\n",
      "Ep. 153 - Loss: 0.184 - Acc: 0.92441 - Val loss: 0.755 - Val acc: 0.93471\n",
      "Ep. 154 - Loss: 0.241 - Acc: 0.85027 - Val loss: 0.730 - Val acc: 0.93471\n",
      "Ep. 155 - Loss: 0.106 - Acc: 0.92523 - Val loss: 0.722 - Val acc: 0.93471\n",
      "Ep. 156 - Loss: 0.253 - Acc: 0.91570 - Val loss: 0.711 - Val acc: 0.93471\n",
      "Ep. 157 - Loss: 0.158 - Acc: 0.92531 - Val loss: 0.697 - Val acc: 0.93471\n",
      "Ep. 158 - Loss: 0.259 - Acc: 0.92564 - Val loss: 0.679 - Val acc: 0.93471\n",
      "Ep. 159 - Loss: 0.197 - Acc: 0.92534 - Val loss: 0.670 - Val acc: 0.93471\n",
      "Ep. 160 - Loss: 0.221 - Acc: 0.83719 - Val loss: 0.661 - Val acc: 0.93471\n",
      "Ep. 161 - Loss: 0.258 - Acc: 0.91522 - Val loss: 0.655 - Val acc: 0.93471\n",
      "Ep. 162 - Loss: 0.217 - Acc: 0.92501 - Val loss: 0.653 - Val acc: 0.93471\n",
      "Ep. 163 - Loss: 0.200 - Acc: 0.82048 - Val loss: 0.653 - Val acc: 0.93471\n",
      "Ep. 164 - Loss: 0.218 - Acc: 0.92204 - Val loss: 0.645 - Val acc: 0.93471\n",
      "Ep. 165 - Loss: 0.240 - Acc: 0.92186 - Val loss: 0.649 - Val acc: 0.93471\n",
      "Ep. 166 - Loss: 0.282 - Acc: 0.92341 - Val loss: 0.659 - Val acc: 0.93471\n",
      "Ep. 167 - Loss: 0.149 - Acc: 0.92526 - Val loss: 0.659 - Val acc: 0.93471\n",
      "Ep. 168 - Loss: 0.193 - Acc: 0.92517 - Val loss: 0.652 - Val acc: 0.93471\n",
      "Ep. 169 - Loss: 0.206 - Acc: 0.92485 - Val loss: 0.642 - Val acc: 0.93471\n",
      "Ep. 170 - Loss: 0.194 - Acc: 0.92425 - Val loss: 0.638 - Val acc: 0.93471\n",
      "Ep. 171 - Loss: 0.138 - Acc: 0.92553 - Val loss: 0.646 - Val acc: 0.93471\n",
      "Ep. 172 - Loss: 0.175 - Acc: 0.92564 - Val loss: 0.670 - Val acc: 0.93471\n",
      "Ep. 173 - Loss: 0.169 - Acc: 0.92547 - Val loss: 0.692 - Val acc: 0.93471\n",
      "Ep. 174 - Loss: 0.182 - Acc: 0.92574 - Val loss: 0.700 - Val acc: 0.93471\n",
      "Ep. 175 - Loss: 0.184 - Acc: 0.92533 - Val loss: 0.691 - Val acc: 0.93471\n",
      "Ep. 176 - Loss: 0.172 - Acc: 0.92477 - Val loss: 0.679 - Val acc: 0.93471\n",
      "Ep. 177 - Loss: 0.143 - Acc: 0.92551 - Val loss: 0.673 - Val acc: 0.93471\n",
      "Ep. 178 - Loss: 0.134 - Acc: 0.92518 - Val loss: 0.669 - Val acc: 0.93471\n",
      "Ep. 179 - Loss: 0.173 - Acc: 0.92543 - Val loss: 0.662 - Val acc: 0.93471\n",
      "Ep. 180 - Loss: 0.214 - Acc: 0.92529 - Val loss: 0.646 - Val acc: 0.93471\n",
      "Ep. 181 - Loss: 0.176 - Acc: 0.92549 - Val loss: 0.619 - Val acc: 0.93471\n",
      "Ep. 182 - Loss: 0.185 - Acc: 0.92517 - Val loss: 0.601 - Val acc: 0.93471\n",
      "Ep. 183 - Loss: 0.146 - Acc: 0.92570 - Val loss: 0.593 - Val acc: 0.93471\n",
      "Ep. 184 - Loss: 0.166 - Acc: 0.92569 - Val loss: 0.583 - Val acc: 0.93471\n",
      "Ep. 185 - Loss: 0.140 - Acc: 0.91968 - Val loss: 0.576 - Val acc: 0.93471\n",
      "Ep. 186 - Loss: 0.148 - Acc: 0.92511 - Val loss: 0.573 - Val acc: 0.93471\n",
      "Ep. 187 - Loss: 0.180 - Acc: 0.92521 - Val loss: 0.571 - Val acc: 0.93471\n",
      "Ep. 188 - Loss: 0.202 - Acc: 0.92557 - Val loss: 0.571 - Val acc: 0.93471\n",
      "Ep. 189 - Loss: 0.184 - Acc: 0.92520 - Val loss: 0.572 - Val acc: 0.93471\n",
      "Ep. 190 - Loss: 0.179 - Acc: 0.92543 - Val loss: 0.576 - Val acc: 0.93471\n",
      "Ep. 191 - Loss: 0.168 - Acc: 0.92314 - Val loss: 0.584 - Val acc: 0.93471\n",
      "Ep. 192 - Loss: 0.148 - Acc: 0.92513 - Val loss: 0.587 - Val acc: 0.93471\n",
      "Ep. 193 - Loss: 0.189 - Acc: 0.92512 - Val loss: 0.576 - Val acc: 0.93471\n",
      "Ep. 194 - Loss: 0.173 - Acc: 0.92559 - Val loss: 0.561 - Val acc: 0.93471\n",
      "Ep. 195 - Loss: 0.226 - Acc: 0.92562 - Val loss: 0.546 - Val acc: 0.93471\n",
      "Ep. 196 - Loss: 0.188 - Acc: 0.92550 - Val loss: 0.528 - Val acc: 0.93471\n",
      "Ep. 197 - Loss: 0.139 - Acc: 0.92551 - Val loss: 0.515 - Val acc: 0.93471\n",
      "Ep. 198 - Loss: 0.183 - Acc: 0.92528 - Val loss: 0.512 - Val acc: 0.93471\n"
     ]
    }
   ],
   "source": [
    "# Initialize the epoch and step counters to -1\n",
    "# Create an empty list for storing training results\n",
    "epoch = step = -1\n",
    "results = []\n",
    "\n",
    "# Iterate through the batches in the loader_train data loader\n",
    "for batch in loader_train:\n",
    "    # Increment the step counter\n",
    "    step += 1\n",
    "\n",
    "    # Execute the train_step function with the current batch\n",
    "    # Obtain the loss and accuracy\n",
    "    loss, acc = train_step(*batch)\n",
    "\n",
    "    # Append the loss and accuracy to the results list\n",
    "    results.append((loss, acc))\n",
    "\n",
    "    # Check if the current step is equal to the number of steps per epoch (loader_train.steps_per_epoch)\n",
    "    if step == loader_train.steps_per_epoch:\n",
    "        # Reset the step counter to 0\n",
    "        # Increment the epoch counter\n",
    "        step = 0\n",
    "        epoch += 1\n",
    "\n",
    "        # Evaluate the model on the test set using the evaluate function (which should be defined beforehand)\n",
    "        # Store the test results in results_te\n",
    "        results_te = evaluate(loader_val) # CAMBIO A loader_val\n",
    "        \n",
    "\n",
    "        # Print the epoch number, mean training loss and accuracy, and test loss and accuracy\n",
    "        print(\n",
    "            \"Ep. {} - Loss: {:.3f} - Acc: {:.5f} - Val loss: {:.3f} - Val acc: {:.5f}\".format(\n",
    "                epoch, *np.mean(results, 0), *results_te\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Reset the results list to start collecting results for the next epoch\n",
    "        results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[    0     3]\n",
      " [ 2717 38992]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# PREDICCION\n",
    "\n",
    "for _ in range(loader_test_Tati.steps_per_epoch):\n",
    "    inputs,target = loader_test_Tati.__next__()\n",
    "    y_prediction = model(inputs, training=False)\n",
    "    y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "    y_test=np.argmax(target, axis=1)\n",
    "    #Create confusion matrix and normalizes it over predicted (columns)\n",
    "    result = tf.math.confusion_matrix(y_test, y_prediction, num_classes=2) \n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOBRE LA PREDICCIÓN\n",
    "\n",
    "Para predecir hay que ejecutar `model(inputs,training=False)`, donde en un paso previo se define `inputs` como `inputs,target = loader_test.__next__()`.\n",
    "\n",
    "Había intentado predecir con `model.predict(loader_test)`, pero esto NO REALIZA LA PREDICCIÓN. Probablemente se quedaba en un loop infinito o algo así, tratando de \"desempaquetar\" `loader_test`.\n",
    "\n",
    "En el caso de las proteínas, al hacer `model.predict` lo corté en \"653683/Unknown - 7336s 11ms/step\", mientras que para CTU13 se quedaba sin memoria mucho antes, no pudiendo calcular ninguna predicción.\n",
    "\n",
    "\n",
    "```\n",
    "#Predict\n",
    "y_prediction = model.predict(loader_test_Tati)\n",
    "y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "y_test=np.argmax(dataset_test[0].y, axis=1)\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
    "print(result)\n",
    "```\n",
    "\n",
    "Incluso, pensando que era un problema de memoria, guardé el modelo y después quise forzar la predicción en la CPU al reiniciar el kernel, pero tampoco llegaba a buen puerto:\n",
    "\n",
    "```\n",
    "# Después de entrenar:\n",
    "os.mkdir(\"/mnt/modelo\")\n",
    "model.save(\"/mnt/modelo/\", include_optimizer=False)\n",
    "\n",
    "# Reiniciar el kernel, cargar el modelo e intentar predecir (con la función equivocada...)\n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"/mnt/modelo/\")\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    y_prediction = model.predict(loader_test_Tati)\n",
    "    y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
