{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.layers import Dense, Dropout,Input\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy #AUC \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from spektral.layers import GINConv,GCNConv #, GCSConv, GlobalAvgPool\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "from spektral.data import DisjointLoader, BatchLoader, Dataset, Graph\n",
    "from spektral.transforms.normalize_adj import NormalizeAdj\n",
    "from spektral.transforms.normalize_one import NormalizeOne\n",
    "from spektral.transforms.normalize_sphere import NormalizeSphere\n",
    "import gc\n",
    "import spektral.datasets\n",
    "from spektral.data import DisjointLoader, BatchLoader\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from spektral.models.gcn import GCN \n",
    "from spektral.models.general_gnn import GeneralGNN\n",
    "import glob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import gc\n",
    "from spektral.data import Dataset, Graph\n",
    "\n",
    "# Custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, graph_feature_files, ncol_files, **kwargs):\n",
    "        # Store the graph feature and ncol file lists\n",
    "        self.graph_feature_files = graph_feature_files\n",
    "        self.ncol_files = ncol_files\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def read(self):\n",
    "        # We must return a list of Graph objects\n",
    "        output = []\n",
    "\n",
    "        # Iterate through graph_feature_files and ncol_files\n",
    "        for graph_feature_file, ncol_file in zip(self.graph_feature_files, self.ncol_files):\n",
    "            # Read graph features\n",
    "            x_tmp = pd.read_csv(graph_feature_file, sep=\",\", header=0)\n",
    "            # Replace background for normal\n",
    "            x_tmp['label'] = x_tmp['label'].replace('background','normal')\n",
    "\n",
    "           \n",
    "            # Read graph topology\n",
    "            a_tmp = pd.read_csv(ncol_file, sep=\" \", header=None, names=[\"source\", \"target\", \"weight\"])\n",
    "\n",
    "            # Replace all non-zero values in the 'weight' column with 1\n",
    "            a_tmp.loc[a_tmp['weight'] != 0, 'weight'] = 1\n",
    "            \n",
    "            # Create dictionaries that identify each node and label with an integer\n",
    "            class_idx = {name: idx for idx, name in enumerate(sorted(x_tmp[\"label\"].unique()))}\n",
    "            node_idx = {name: idx for idx, name in enumerate(sorted(x_tmp[\"node\"].unique()))}\n",
    "\n",
    "            # Change node names and label for their corresponding integer\n",
    "            x_tmp[\"node\"] = x_tmp[\"node\"].apply(lambda name: node_idx[name])\n",
    "            x_tmp[\"label\"] = x_tmp[\"label\"].apply(lambda value: class_idx[value])\n",
    "            a_tmp[\"source\"] = a_tmp[\"source\"].apply(lambda name: node_idx[name])\n",
    "            a_tmp[\"target\"] = a_tmp[\"target\"].apply(lambda name: node_idx[name])\n",
    "\n",
    "            # Node features: all but node and label\n",
    "            x = x_tmp.sort_values(\"node\")[x_tmp.columns.difference([\"node\",\"label\"], sort=False)]\n",
    "            # normalize Node features\n",
    "            x = (x - x.min()) / (x.max() - x.min())\n",
    "            # Convert to numpy\n",
    "            x = x.to_numpy()\n",
    "            x = x.astype(np.float32)\n",
    "\n",
    "            # Create adjacency matrix from source, target, and weight\n",
    "            a_source = a_tmp[[\"source\"]].to_numpy().T\n",
    "            a_source = np.reshape(a_source, a_source.shape[-1])\n",
    "            a_target = a_tmp[[\"target\"]].to_numpy().T\n",
    "            a_target = np.reshape(a_target, a_target.shape[-1])\n",
    "            a_weight = a_tmp[[\"weight\"]].to_numpy().T\n",
    "            a_weight = np.reshape(a_weight, a_weight.shape[-1])\n",
    "\n",
    "            # Adjacency matrix:\n",
    "            #a = sparse.coo_matrix((a_weight, (a_source, a_target)), shape=(x.shape[0], x.shape[0]))\n",
    "            a = sparse.csr_matrix((a_weight, (a_source, a_target)), shape=(x.shape[0], x.shape[0]), dtype=np.float32)\n",
    "\n",
    "            # Label (CTU13random):\n",
    "            #y = []\n",
    "            #for j in range(x_tmp.shape[0]):\n",
    "            #     np.random.seed(200+j)\n",
    "            #     y_tmp = np.random.randint(0,2)\n",
    "            #     if y_tmp:\n",
    "            #         y.append(np.array([1., 0.]))\n",
    "            #     else:\n",
    "            #         y.append(np.array([0., 1.]))\n",
    "            #y = np.array(y)\n",
    "            #y_one_hot = y.astype(np.int64)\n",
    "\n",
    "            #y = []\n",
    "            #for j in range(x_tmp.shape[0]):\n",
    "            #    #if((x[j,0] > 1) or (x[j,1] > 1)): # Condition for CTU13balancedIDOD: \n",
    "            #    if (x_tmp.iloc[j,4] > 2):  \n",
    "            #        y.append(np.array([0., 1.])) # clase 1 = \"infected\"\n",
    "            #    else:\n",
    "            #        y.append(np.array([1., 0.])) # clase 0 = \"normal\"\n",
    "            #y = np.array(y)\n",
    "            #y_one_hot = y.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "            # Label:\n",
    "            y = x_tmp.sort_values(\"node\")[\"label\"].to_numpy()\n",
    "            y.astype(np.int64)\n",
    "            y_one_hot = np.eye(2)[y]\n",
    "            \n",
    "            # Create a Graph object and add it to the output list\n",
    "            output.append(Graph(x=x, a=a, y=y_one_hot))\n",
    "\n",
    "            # Free memory\n",
    "            del x_tmp, x, a_tmp, a_source, a_target, a_weight, a, y\n",
    "            gc.collect()\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "tmp = pd.read_csv(\"/mnt/CEPH/ctu13/features/capture20110810.binetflow.labels-positive-weights.labeled.csv\", sep=\",\", header=0)\n",
    "tmp = tmp.drop(\"node\", axis=1)\n",
    "tmp = tmp.drop(\"label\",axis=1)\n",
    "tmp = (tmp - tmp.min()) / (tmp.max() - tmp.min())\n",
    "\n",
    "tmp\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance CTU13 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of graph feature files and ncol files\n",
    "graph_feature_files = sorted(glob.glob(\"/mnt/CEPH/ctu13/features/*\"))\n",
    "ncol_files = sorted(glob.glob(\"/mnt/CEPH/ctu13/ncol/*\"))\n",
    "# Instantiate the dataset\n",
    "dataset = MyDataset(graph_feature_files, ncol_files,transforms=[NormalizeAdj(symmetric=False)])\n",
    "#dataset = MyDataset(graph_feature_files, ncol_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(13):\n",
    "    suma = 0\n",
    "    for i in range(dataset[j].n_nodes):\n",
    "        if all(dataset[j].y[i] == [0.,1.]):\n",
    "            suma+=1\n",
    "    print(\"cap \",j+1,\": infected:\",suma, \" - normal:\",dataset[j].n_nodes-suma, \" -- prop:\",suma/dataset[j].n_nodes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove capture 9 from dataset\n",
    "dataset_test2 = dataset[9]\n",
    "dataset = dataset[:8] + dataset[9:]\n",
    "\n",
    "split = int(0.8 * len(dataset))\n",
    "dataset_train, dataset_test = dataset[:split], dataset[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train\n",
    "dataset_test[0].n_nodes\n",
    "dataset_train.n_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "loader_train = DisjointLoader(dataset_train, node_level= True, batch_size=batch_size, epochs=300, shuffle=False, )\n",
    "loader_test = DisjointLoader(dataset_test, node_level = True, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom GCN model (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gcn_model():\n",
    "    # Define input placeholders for node features, adjacency matrix, and segment indices\n",
    "    X_in = Input(shape=(dataset.n_node_features,))\n",
    "    A_in = Input((None,), sparse=True)\n",
    "    I_in = Input(shape=(), dtype=tf.int32)\n",
    "\n",
    "    # Apply the first GINConv layer with 32 units and ReLU activation\n",
    "    X_1 = GINConv(32, activation=\"relu\")([X_in, A_in])\n",
    "    # Apply dropout with a rate of 0.5\n",
    "    X_1 = Dropout(0.5)(X_1)\n",
    "\n",
    "    # Apply the second GINConv layer with 32 units and ReLU activation\n",
    "    X_2 = GINConv(32, activation=\"relu\")([X_1, A_in])\n",
    "    # Apply dropout with a rate of 0.5\n",
    "    X_2 = Dropout(0.5)(X_2)\n",
    "\n",
    "    # Aggregate the node features using the segment_mean function and the segment indices\n",
    "    X_3 = tf.math.segment_mean(X_2, I_in)\n",
    "    # Apply a dense output layer with the number of labels and softmax activation\n",
    "    out = Dense(dataset.n_labels, activation=\"softmax\")(X_3)\n",
    "\n",
    "    # Create and return the model with the defined inputs and outputs\n",
    "    model = Model(inputs=[X_in, A_in, I_in], outputs=out)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current implementation of GCN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import Loss\n",
    "\n",
    "class FocalLoss(Loss):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, name=\"focal_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-8, 1 - 1e-8)\n",
    "        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        loss = -self.alpha * (1 - pt) ** self.gamma * tf.math.log(pt)\n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "\n",
    "class BinaryFocalLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, from_logits=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Initialize the hyperparameters alpha and gamma\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        # If True, the input to the loss function is assumed to be logits\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # If from_logits is True, apply sigmoid activation to logits (y_pred)\n",
    "        if self.from_logits:\n",
    "            y_pred = tf.nn.sigmoid(y_pred)\n",
    "\n",
    "        # Cast y_true to float32\n",
    "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "\n",
    "        # Clip y_pred values between epsilon and (1 - epsilon) to avoid log(0)\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "\n",
    "        # Compute binary cross-entropy\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "        \n",
    "        # Compute focal loss\n",
    "        focal_loss = self.alpha * tf.pow(1 - y_pred, self.gamma) * cross_entropy\n",
    "\n",
    "        # Reduce focal_loss along the last axis and then calculate the mean over the batch\n",
    "        return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=-1))\n",
    "\n",
    "    def get_config(self):\n",
    "        # Get the configuration of the loss function, including alpha, gamma, and from_logits\n",
    "        config = super().get_config()\n",
    "        config.update({\"alpha\": self.alpha, \"gamma\": self.gamma, \"from_logits\": self.from_logits})\n",
    "        return config\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GCN model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = create_gcn_model()\n",
    "model = GCN(n_labels=dataset.n_labels)\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "#optimizer = SGD(learning_rate =0.0001)\n",
    "#loss_fn = BinaryCrossentropy(from_logits=False)\n",
    "#loss_fn = CategoricalCrossentropy(from_logits=False)\n",
    "loss_fn= BinaryFocalLoss(alpha=0.25, gamma=2.0)\n",
    "auc_metric = AUC()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorate the function with @tf.function to compile as a TensorFlow graph\n",
    "# Use the input_signature from loader_train and relax shapes for varying graph sizes\n",
    "\n",
    "#class_weights = np.array([0.0000001, 0.9999999])\n",
    "\n",
    "@tf.function(input_signature=loader_train.tf_signature(), experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    #print(inputs.shape)\n",
    "    # Create a GradientTape context to record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute model predictions with the inputs, set training=True for training-specific behaviors\n",
    "        predictions = model(inputs, training=True)\n",
    "\n",
    "        ### CLASS WEIGHT\n",
    "        ## Create a tensor with class weights\n",
    "        #sample_weights = tf.gather(class_weights, tf.argmax(target, axis=-1))\n",
    "        # Calculate the loss using the provided loss_fn and add the model's regularization losses and including sample_weight\n",
    "        #loss = loss_fn(target, predictions,sample_weight=sample_weights) + sum(model.losses)\n",
    "\n",
    "        \n",
    "        # Calculate the loss using the provided loss_fn and add the model's regularization losses\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "\n",
    "\n",
    "    # Compute gradients of the loss with respect to the model's trainable variables\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Apply the gradients to the model's variables using the optimizer's apply_gradients method\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Compute the accuracy using the categorical_accuracy function from TensorFlow\n",
    "    # Calculate the mean accuracy using tf.reduce_mean\n",
    "    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "     # Update the AUC metric\n",
    "    auc_metric.update_state(target, predictions)\n",
    "    # Get the current AUC value\n",
    "    auc = auc_metric.result()\n",
    "\n",
    "\n",
    "    # Return the loss and accuracy as output\n",
    "    return loss, acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug dataset loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loader input signature:\", loader_train.tf_signature())\n",
    "sample_batch = loader_train.__next__()\n",
    "inputs, targets = sample_batch\n",
    "for tensor in inputs:\n",
    "    print(tensor.shape, tensor.dtype)\n",
    "print(\"Targets shape:\", targets.shape, \"dtype:\", targets.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_auc(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    auc_metric_test = AUC()\n",
    "\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        \n",
    "        # Update the AUC metric with the true labels and predictions\n",
    "        auc_metric_test.update_state(target, pred)\n",
    "        \n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            auc_metric_test.result().numpy(),  # Get the current AUC value\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            tf.reduce_mean(categorical_accuracy(target, pred)),\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN CODE for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the epoch and step counters to -1\n",
    "# Create an empty list for storing training results\n",
    "epoch = step = -1\n",
    "results = []\n",
    "results_train = []\n",
    "# Iterate through the batches in the loader_train data loader\n",
    "for batch in loader_train:\n",
    "    inputs,targets = batch\n",
    "    targets = tf.convert_to_tensor(targets)\n",
    "    # Increment the step counter\n",
    "    step += 1\n",
    "\n",
    "    # Execute the train_step function with the current batch\n",
    "    # Obtain the loss and accuracy\n",
    "    loss, metric = train_step(inputs,targets)\n",
    "\n",
    "    # Append the loss and accuracy to the results list\n",
    "    results.append((loss, metric))\n",
    "    results_train.append( np.mean(results,0) )\n",
    "    # Check if the current step is equal to the number of steps per epoch (loader_train.steps_per_epoch)\n",
    "    if step == loader_train.steps_per_epoch:\n",
    "        # Reset the step counter to 0\n",
    "        # Increment the epoch counter\n",
    "        step = 0\n",
    "        epoch += 1\n",
    "\n",
    "        # Evaluate the model on the test set using the evaluate function (which should be defined beforehand)\n",
    "        # Store the test results in results_te\n",
    "        results_te = evaluate(loader_test)\n",
    "\n",
    "        # Print the epoch number, mean training loss and accuracy, and test loss and accuracy\n",
    "        print(\n",
    "            \"Ep. {} - Loss: {:.3f} - Metric: {:.3f} - Test loss: {:.3f} - Test Metric: {:.3f}\".format(\n",
    "                epoch, *np.mean(results, 0), *results_te\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Reset the results list to start collecting results for the next epoch\n",
    "        results = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "losses_train = [item[0] for item in results_train]\n",
    "auc_train = [item[1] for item in results_train]\n",
    "\n",
    "# Create a list of epoch numbers\n",
    "epochs = list(range(1, len(losses_train) + 1))\n",
    "\n",
    "# Create a new figure and axis\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot losses_train on the first y-axis\n",
    "ax1.plot(epochs, losses_train, 'b-', label='Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "\n",
    "# Create a second y-axis to plot auc_train\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, auc_train, 'r-', label='ACC')\n",
    "ax2.set_ylabel('ACC', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "# Add legends for both y-axes\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = loader_test.__next__()\n",
    "inputs, targets = sample_batch\n",
    "for tensor in inputs:\n",
    "    print(tensor.shape, tensor.dtype)\n",
    "print(\"Targets shape:\", targets.shape, \"dtype:\", targets.dtype)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "step = 0\n",
    "while step < loader_test.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader_test.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        predictions.append(pred)\n",
    "        targets.append(target)\n",
    "    \n",
    "predictions = np.vstack(predictions)\n",
    "    \n",
    "targets = np.vstack(targets)\n",
    "# Post-process the predictions if necessary (e.g., convert probabilities to class labels)\n",
    "predicted_values = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(targets,axis =1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,accuracy_score\n",
    "\n",
    "conf_mat = confusion_matrix(predicted_values, true_labels)\n",
    "print(conf_mat)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = roc_auc_score(true_labels, predicted_values)\n",
    "accuracy_score = accuracy_score(true_labels, predicted_values)\n",
    "\n",
    "print(\"AUC Score:\", auc_score)\n",
    "print(\"Accuracy Score:\", accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
