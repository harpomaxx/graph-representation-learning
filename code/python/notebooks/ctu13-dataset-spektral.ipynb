{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 14:20:46.655995: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.layers import Dense, Dropout,Input\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy #AUC \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from spektral.layers import GINConv,GCNConv #, GCSConv, GlobalAvgPool\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "from spektral.data import DisjointLoader, BatchLoader, Dataset, Graph\n",
    "from spektral.transforms.normalize_adj import NormalizeAdj\n",
    "from spektral.transforms.normalize_one import NormalizeOne\n",
    "from spektral.transforms.normalize_sphere import NormalizeSphere\n",
    "import gc\n",
    "import spektral.datasets\n",
    "from spektral.data import DisjointLoader, BatchLoader\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from spektral.models.gcn import GCN \n",
    "import glob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import gc\n",
    "from spektral.data import Dataset, Graph\n",
    "\n",
    "# Custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, graph_feature_files, ncol_files, **kwargs):\n",
    "        # Store the graph feature and ncol file lists\n",
    "        self.graph_feature_files = graph_feature_files\n",
    "        self.ncol_files = ncol_files\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def read(self):\n",
    "        # We must return a list of Graph objects\n",
    "        output = []\n",
    "\n",
    "        # Iterate through graph_feature_files and ncol_files\n",
    "        for graph_feature_file, ncol_file in zip(self.graph_feature_files, self.ncol_files):\n",
    "            # Read graph features\n",
    "            x_tmp = pd.read_csv(graph_feature_file, sep=\",\", header=0)\n",
    "            # Replace background for normal\n",
    "            x_tmp['label'] = x_tmp['label'].replace('background','normal')\n",
    "            # Read graph topology\n",
    "            a_tmp = pd.read_csv(ncol_file, sep=\" \", header=None, names=[\"source\", \"target\", \"weight\"])\n",
    "\n",
    "            # Create dictionaries that identify each node and label with an integer\n",
    "            class_idx = {name: idx for idx, name in enumerate(sorted(x_tmp[\"label\"].unique()))}\n",
    "            node_idx = {name: idx for idx, name in enumerate(sorted(x_tmp[\"node\"].unique()))}\n",
    "\n",
    "            # Change node names and label for their corresponding integer\n",
    "            x_tmp[\"node\"] = x_tmp[\"node\"].apply(lambda name: node_idx[name])\n",
    "            x_tmp[\"label\"] = x_tmp[\"label\"].apply(lambda value: class_idx[value])\n",
    "            a_tmp[\"source\"] = a_tmp[\"source\"].apply(lambda name: node_idx[name])\n",
    "            a_tmp[\"target\"] = a_tmp[\"target\"].apply(lambda name: node_idx[name])\n",
    "\n",
    "            # Node features:\n",
    "            x = x_tmp.sort_values(\"node\")[x_tmp.columns.difference([\"node\",\"label\"], sort=False)].to_numpy()\n",
    "            x = x.astype(np.float32)\n",
    "\n",
    "            # Create adjacency matrix from source, target, and weight\n",
    "            a_source = a_tmp[[\"source\"]].to_numpy().T\n",
    "            a_source = np.reshape(a_source, a_source.shape[-1])\n",
    "            a_target = a_tmp[[\"target\"]].to_numpy().T\n",
    "            a_target = np.reshape(a_target, a_target.shape[-1])\n",
    "            a_weight = a_tmp[[\"weight\"]].to_numpy().T\n",
    "            a_weight = np.reshape(a_weight, a_weight.shape[-1])\n",
    "\n",
    "            # Adjacency matrix:\n",
    "            #a = sparse.coo_matrix((a_weight, (a_source, a_target)), shape=(x.shape[0], x.shape[0]))\n",
    "            a = sparse.csr_matrix((a_weight, (a_source, a_target)), shape=(x.shape[0], x.shape[0]), dtype=np.float32)\n",
    "\n",
    "              # Label (CTU13random):\n",
    "            #y = []\n",
    "            #for j in range(x_tmp.shape[0]):\n",
    "            #     np.random.seed(200+j)\n",
    "            #     y_tmp = np.random.randint(0,2)\n",
    "            #     if y_tmp:\n",
    "            #         y.append(np.array([1., 0.]))\n",
    "            #     else:\n",
    "            #         y.append(np.array([0., 1.]))\n",
    "            #y = np.array(y)\n",
    "            #y_one_hot = y.astype(np.int64)\n",
    "\n",
    "            # Label:\n",
    "            y = x_tmp.sort_values(\"node\")[\"label\"].to_numpy()\n",
    "            y.astype(np.int64)\n",
    "            y_one_hot = np.eye(2)[y]\n",
    "            \n",
    "            # Create a Graph object and add it to the output list\n",
    "            output.append(Graph(x=x, a=a, y=y_one_hot))\n",
    "\n",
    "            # Free memory\n",
    "            del x_tmp, x, a_tmp, a_source, a_target, a_weight, a, y\n",
    "            gc.collect()\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance CTU13 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of graph feature files and ncol files\n",
    "graph_feature_files = sorted(glob.glob(\"/mnt/CEPH/ctu13/features/*\"))\n",
    "ncol_files = sorted(glob.glob(\"/mnt/CEPH/ctu13/ncol/*\"))\n",
    "\n",
    "ncol_files\n",
    "# Instantiate the dataset\n",
    "dataset = MyDataset(graph_feature_files, ncol_files,transforms=[NormalizeAdj(symmetric=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(n_nodes=605195, n_node_features=7, n_edge_features=None, n_labels=2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cap  1 : infected: 605194  - normal: 1  -- prop: 0.9999983476400168\n",
      "cap  2 : infected: 440573  - normal: 1  -- prop: 0.9999977302337405\n",
      "cap  3 : infected: 430264  - normal: 1  -- prop: 0.999997675850929\n",
      "cap  4 : infected: 41398  - normal: 1  -- prop: 0.9999758448271697\n",
      "cap  5 : infected: 313677  - normal: 1  -- prop: 0.9999968120174192\n",
      "cap  6 : infected: 184900  - normal: 1  -- prop: 0.9999945917004235\n",
      "cap  7 : infected: 37942  - normal: 1  -- prop: 0.9999736446775427\n",
      "cap  8 : infected: 381449  - normal: 1  -- prop: 0.999997378424433\n",
      "cap  9 : infected: 106579  - normal: 1  -- prop: 0.9999906173766185\n",
      "cap  10 : infected: 365784  - normal: 10  -- prop: 0.9999726622087842\n",
      "cap  11 : infected: 41709  - normal: 3  -- prop: 0.999928078250863\n",
      "cap  12 : infected: 196676  - normal: 10  -- prop: 0.9999491575404452\n",
      "cap  13 : infected: 93827  - normal: 3  -- prop: 0.9999680272833849\n"
     ]
    }
   ],
   "source": [
    "for j in range(13):\n",
    "    suma = 0\n",
    "    for i in range(dataset[j].n_nodes):\n",
    "        if all(dataset[j].y[i] == [0.,1.]):\n",
    "            suma+=1\n",
    "    print(\"cap \",j+1,\": infected:\",suma, \" - normal:\",dataset[j].n_nodes-suma, \" -- prop:\",suma/dataset[j].n_nodes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test2 = dataset[9]\n",
    "dataset = dataset[:8] + dataset[9:]\n",
    "\n",
    "\n",
    "split = int(0.8 * len(dataset))\n",
    "dataset_train, dataset_test = dataset[:split], dataset[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41712"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train\n",
    "dataset_test[0].n_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "loader_train = DisjointLoader(dataset_train, node_level= True,     batch_size=batch_size, epochs=800, shuffle=False, )\n",
    "loader_test = DisjointLoader(dataset_test, node_level = True, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom GCN model (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gcn_model():\n",
    "    # Define input placeholders for node features, adjacency matrix, and segment indices\n",
    "    X_in = Input(shape=(dataset.n_node_features,))\n",
    "    A_in = Input((None,), sparse=True)\n",
    "    I_in = Input(shape=(), dtype=tf.int32)\n",
    "\n",
    "    # Apply the first GINConv layer with 32 units and ReLU activation\n",
    "    X_1 = GINConv(32, activation=\"relu\")([X_in, A_in])\n",
    "    # Apply dropout with a rate of 0.5\n",
    "    X_1 = Dropout(0.5)(X_1)\n",
    "\n",
    "    # Apply the second GINConv layer with 32 units and ReLU activation\n",
    "    X_2 = GINConv(32, activation=\"relu\")([X_1, A_in])\n",
    "    # Apply dropout with a rate of 0.5\n",
    "    X_2 = Dropout(0.5)(X_2)\n",
    "\n",
    "    # Aggregate the node features using the segment_mean function and the segment indices\n",
    "    X_3 = tf.math.segment_mean(X_2, I_in)\n",
    "    # Apply a dense output layer with the number of labels and softmax activation\n",
    "    out = Dense(dataset.n_labels, activation=\"softmax\")(X_3)\n",
    "\n",
    "    # Create and return the model with the defined inputs and outputs\n",
    "    model = Model(inputs=[X_in, A_in, I_in], outputs=out)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load standard GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 14:21:20.951103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-05 14:21:20.955188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-05 14:21:20.958504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-05 14:21:20.959768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-05 14:21:20.960003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-05 14:21:20.960220: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-05 14:21:21.494072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-05 14:21:21.494282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-05 14:21:21.494449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-05 14:21:21.494584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10478 MB memory:  -> device: 0, name: NVIDIA TITAN V, pci bus id: 0000:08:00.0, compute capability: 7.0\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "#model = create_gcn_model()\n",
    "model = GCN(n_labels=dataset.n_labels)\n",
    "optimizer = Adam(lr=0.01)\n",
    "loss_fn = BinaryCrossentropy()\n",
    "auc_metric = AUC()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorate the function with @tf.function to compile as a TensorFlow graph\n",
    "# Use the input_signature from loader_train and relax shapes for varying graph sizes\n",
    "@tf.function(input_signature=loader_train.tf_signature(), experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    #print(inputs.shape)\n",
    "    # Create a GradientTape context to record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute model predictions with the inputs, set training=True for training-specific behaviors\n",
    "        predictions = model(inputs, training=True)\n",
    "        # Calculate the loss using the provided loss_fn and add the model's regularization losses\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "\n",
    "    # Compute gradients of the loss with respect to the model's trainable variables\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Apply the gradients to the model's variables using the optimizer's apply_gradients method\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Compute the accuracy using the categorical_accuracy function from TensorFlow\n",
    "    # Calculate the mean accuracy using tf.reduce_mean\n",
    "    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "     # Update the AUC metric\n",
    "    auc_metric.update_state(target, predictions)\n",
    "    # Get the current AUC value\n",
    "    auc = auc_metric.result()\n",
    "\n",
    "\n",
    "    # Return the loss and accuracy as output\n",
    "    return loss, acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug dataset loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader input signature: ((TensorSpec(shape=(None, 7), dtype=tf.float32, name=None), SparseTensorSpec(TensorShape([None, None]), tf.float32), TensorSpec(shape=(None,), dtype=tf.int64, name=None)), TensorSpec(shape=(None, 2), dtype=tf.float64, name=None))\n",
      "(605195, 7) float32\n",
      "(605195, 605195) <dtype: 'float32'>\n",
      "(605195,) int64\n",
      "Targets shape: (605195, 2) dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Loader input signature:\", loader_train.tf_signature())\n",
    "sample_batch = loader_train.__next__()\n",
    "inputs, targets = sample_batch\n",
    "for tensor in inputs:\n",
    "    print(tensor.shape, tensor.dtype)\n",
    "print(\"Targets shape:\", targets.shape, \"dtype:\", targets.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_auc(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    auc_metric_test = AUC()\n",
    "\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        \n",
    "        # Update the AUC metric with the true labels and predictions\n",
    "        auc_metric_test.update_state(target, pred)\n",
    "        \n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            auc_metric_test.result().numpy(),  # Get the current AUC value\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            tf.reduce_mean(categorical_accuracy(target, pred)),\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN CODE for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 14:21:24.593103: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_3' with dtype int64 and shape [2]\n",
      "\t [[{{node inputs_3}}]]\n",
      "2023-05-05 14:21:24.593194: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_3' with dtype int64 and shape [2]\n",
      "\t [[{{node inputs_3}}]]\n",
      "2023-05-05 14:21:24.604673: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_3' with dtype int64 and shape [2]\n",
      "\t [[{{node inputs_3}}]]\n",
      "2023-05-05 14:21:24.604760: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_3' with dtype int64 and shape [2]\n",
      "\t [[{{node inputs_3}}]]\n",
      "2023-05-05 14:21:24.649139: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_3' with dtype int64 and shape [2]\n",
      "\t [[{{node inputs_3}}]]\n",
      "2023-05-05 14:21:24.649240: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_3' with dtype int64 and shape [2]\n",
      "\t [[{{node inputs_3}}]]\n",
      "2023-05-05 14:21:24.660974: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_3' with dtype int64 and shape [2]\n",
      "\t [[{{node inputs_3}}]]\n",
      "2023-05-05 14:21:24.661065: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_3' with dtype int64 and shape [2]\n",
      "\t [[{{node inputs_3}}]]\n",
      "2023-05-05 14:21:24.799850: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_3' with dtype int64 and shape [2]\n",
      "\t [[{{node inputs_3}}]]\n",
      "2023-05-05 14:21:24.799937: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_3' with dtype int64 and shape [2]\n",
      "\t [[{{node inputs_3}}]]\n",
      "2023-05-05 14:21:24.807026: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_3' with dtype int64 and shape [2]\n",
      "\t [[{{node inputs_3}}]]\n",
      "2023-05-05 14:21:24.807107: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_3' with dtype int64 and shape [2]\n",
      "\t [[{{node inputs_3}}]]\n",
      "2023-05-05 14:21:24.850837: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_3' with dtype int64 and shape [2]\n",
      "\t [[{{node inputs_3}}]]\n",
      "2023-05-05 14:21:24.850938: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_3' with dtype int64 and shape [2]\n",
      "\t [[{{node inputs_3}}]]\n",
      "2023-05-05 14:21:24.862510: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_3' with dtype int64 and shape [2]\n",
      "\t [[{{node inputs_3}}]]\n",
      "2023-05-05 14:21:24.862596: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_3' with dtype int64 and shape [2]\n",
      "\t [[{{node inputs_3}}]]\n",
      "2023-05-05 14:21:25.347173: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f8c7c044d00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-05 14:21:25.347228: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA TITAN V, Compute Capability 7.0\n",
      "2023-05-05 14:21:25.356056: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-05 14:21:25.479440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-05-05 14:21:25.599345: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "/usr/local/lib/python3.8/dist-packages/spektral/data/utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 0 - Loss: 561079.062 - Metric: 0.335 - Test loss: 214691.183 - Test Metric: 0.067\n",
      "Ep. 1 - Loss: 276863.906 - Metric: 0.429 - Test loss: 174769.566 - Test Metric: 0.079\n",
      "Ep. 2 - Loss: 209879.969 - Metric: 0.380 - Test loss: 133365.064 - Test Metric: 0.085\n",
      "Ep. 3 - Loss: 238868.438 - Metric: 0.370 - Test loss: 90568.914 - Test Metric: 0.089\n",
      "Ep. 4 - Loss: 158134.547 - Metric: 0.343 - Test loss: 46962.789 - Test Metric: 0.103\n",
      "Ep. 5 - Loss: 296127.469 - Metric: 0.649 - Test loss: 11806.755 - Test Metric: 0.867\n",
      "Ep. 6 - Loss: 315598.219 - Metric: 0.505 - Test loss: 72.598 - Test Metric: 0.918\n",
      "Ep. 7 - Loss: 133288.266 - Metric: 0.256 - Test loss: 121.977 - Test Metric: 0.918\n",
      "Ep. 8 - Loss: 137229.344 - Metric: 0.593 - Test loss: 164.113 - Test Metric: 0.918\n",
      "Ep. 9 - Loss: 260022.922 - Metric: 0.700 - Test loss: 213.495 - Test Metric: 0.918\n",
      "Ep. 10 - Loss: 132599.953 - Metric: 0.821 - Test loss: 262.536 - Test Metric: 0.918\n",
      "Ep. 11 - Loss: 136943.375 - Metric: 0.805 - Test loss: 301.236 - Test Metric: 0.918\n",
      "Ep. 12 - Loss: 91179.609 - Metric: 0.706 - Test loss: 334.683 - Test Metric: 0.918\n",
      "Ep. 13 - Loss: 233413.688 - Metric: 0.682 - Test loss: 378.305 - Test Metric: 0.918\n",
      "Ep. 14 - Loss: 76252.688 - Metric: 0.520 - Test loss: 414.700 - Test Metric: 0.918\n",
      "Ep. 15 - Loss: 92719.445 - Metric: 0.660 - Test loss: 441.928 - Test Metric: 0.918\n",
      "Ep. 16 - Loss: 75490.180 - Metric: 0.396 - Test loss: 467.943 - Test Metric: 0.918\n",
      "Ep. 17 - Loss: 32362.535 - Metric: 0.735 - Test loss: 487.688 - Test Metric: 0.918\n",
      "Ep. 18 - Loss: 106211.398 - Metric: 0.751 - Test loss: 518.688 - Test Metric: 0.918\n",
      "Ep. 19 - Loss: 101328.305 - Metric: 0.831 - Test loss: 543.491 - Test Metric: 0.918\n",
      "Ep. 20 - Loss: 73609.891 - Metric: 0.763 - Test loss: 566.512 - Test Metric: 0.918\n",
      "Ep. 21 - Loss: 50752.777 - Metric: 0.867 - Test loss: 591.499 - Test Metric: 0.918\n",
      "Ep. 22 - Loss: 149890.422 - Metric: 0.656 - Test loss: 619.664 - Test Metric: 0.918\n",
      "Ep. 23 - Loss: 50759.676 - Metric: 0.820 - Test loss: 644.303 - Test Metric: 0.918\n",
      "Ep. 24 - Loss: 33119.027 - Metric: 0.718 - Test loss: 663.209 - Test Metric: 0.918\n",
      "Ep. 25 - Loss: 70804.852 - Metric: 0.736 - Test loss: 687.228 - Test Metric: 0.918\n",
      "Ep. 26 - Loss: 49657.516 - Metric: 0.811 - Test loss: 708.724 - Test Metric: 0.918\n",
      "Ep. 27 - Loss: 97924.977 - Metric: 0.622 - Test loss: 735.192 - Test Metric: 0.918\n",
      "Ep. 28 - Loss: 44668.578 - Metric: 0.906 - Test loss: 756.815 - Test Metric: 0.918\n",
      "Ep. 29 - Loss: 57314.078 - Metric: 0.731 - Test loss: 775.400 - Test Metric: 0.918\n",
      "Ep. 30 - Loss: 65423.332 - Metric: 0.856 - Test loss: 792.942 - Test Metric: 0.918\n",
      "Ep. 31 - Loss: 22438.199 - Metric: 0.799 - Test loss: 805.587 - Test Metric: 0.918\n",
      "Ep. 32 - Loss: 44506.637 - Metric: 0.852 - Test loss: 820.750 - Test Metric: 0.918\n",
      "Ep. 33 - Loss: 48642.254 - Metric: 0.822 - Test loss: 839.119 - Test Metric: 0.918\n",
      "Ep. 34 - Loss: 26129.766 - Metric: 0.720 - Test loss: 856.274 - Test Metric: 0.918\n",
      "Ep. 35 - Loss: 20693.291 - Metric: 0.807 - Test loss: 869.996 - Test Metric: 0.918\n",
      "Ep. 36 - Loss: 105951.617 - Metric: 0.807 - Test loss: 910.333 - Test Metric: 0.918\n",
      "Ep. 37 - Loss: 25989.445 - Metric: 0.760 - Test loss: 929.165 - Test Metric: 0.918\n",
      "Ep. 38 - Loss: 27900.590 - Metric: 0.828 - Test loss: 941.692 - Test Metric: 0.918\n",
      "Ep. 39 - Loss: 26527.693 - Metric: 0.802 - Test loss: 953.609 - Test Metric: 0.918\n",
      "Ep. 40 - Loss: 30348.211 - Metric: 0.921 - Test loss: 966.569 - Test Metric: 0.918\n",
      "Ep. 41 - Loss: 131698.391 - Metric: 0.833 - Test loss: 984.907 - Test Metric: 0.918\n",
      "Ep. 42 - Loss: 10987.884 - Metric: 0.922 - Test loss: 994.189 - Test Metric: 0.918\n",
      "Ep. 43 - Loss: 28203.139 - Metric: 0.848 - Test loss: 1004.052 - Test Metric: 0.918\n",
      "Ep. 44 - Loss: 23445.852 - Metric: 0.912 - Test loss: 1014.048 - Test Metric: 0.918\n",
      "Ep. 45 - Loss: 85957.742 - Metric: 0.837 - Test loss: 1033.871 - Test Metric: 0.918\n",
      "Ep. 46 - Loss: 25782.914 - Metric: 0.793 - Test loss: 1045.614 - Test Metric: 0.918\n",
      "Ep. 47 - Loss: 12929.087 - Metric: 0.924 - Test loss: 1054.521 - Test Metric: 0.918\n",
      "Ep. 48 - Loss: 19112.557 - Metric: 0.773 - Test loss: 1062.101 - Test Metric: 0.918\n",
      "Ep. 49 - Loss: 49122.906 - Metric: 0.758 - Test loss: 1095.052 - Test Metric: 0.918\n",
      "Ep. 50 - Loss: 29077.094 - Metric: 0.854 - Test loss: 1114.773 - Test Metric: 0.918\n",
      "Ep. 51 - Loss: 43083.012 - Metric: 0.742 - Test loss: 1131.672 - Test Metric: 0.918\n",
      "Ep. 52 - Loss: 14993.056 - Metric: 0.923 - Test loss: 1141.000 - Test Metric: 0.918\n",
      "Ep. 53 - Loss: 16784.635 - Metric: 0.924 - Test loss: 1148.852 - Test Metric: 0.918\n",
      "Ep. 54 - Loss: 8885.033 - Metric: 0.927 - Test loss: 1154.144 - Test Metric: 0.918\n",
      "Ep. 55 - Loss: 5305.272 - Metric: 0.798 - Test loss: 1158.153 - Test Metric: 0.918\n",
      "Ep. 56 - Loss: 15068.812 - Metric: 0.926 - Test loss: 1161.540 - Test Metric: 0.918\n",
      "Ep. 57 - Loss: 16559.893 - Metric: 0.922 - Test loss: 1170.757 - Test Metric: 0.918\n",
      "Ep. 58 - Loss: 11503.353 - Metric: 0.828 - Test loss: 1176.962 - Test Metric: 0.918\n",
      "Ep. 59 - Loss: 12889.944 - Metric: 0.927 - Test loss: 1182.593 - Test Metric: 0.918\n",
      "Ep. 60 - Loss: 20539.273 - Metric: 0.890 - Test loss: 1187.938 - Test Metric: 0.918\n",
      "Ep. 61 - Loss: 16888.055 - Metric: 0.839 - Test loss: 1196.235 - Test Metric: 0.918\n",
      "Ep. 62 - Loss: 7669.443 - Metric: 0.914 - Test loss: 1201.180 - Test Metric: 0.918\n",
      "Ep. 63 - Loss: 11633.918 - Metric: 0.889 - Test loss: 1206.060 - Test Metric: 0.918\n",
      "Ep. 64 - Loss: 24965.621 - Metric: 0.821 - Test loss: 1217.281 - Test Metric: 0.918\n",
      "Ep. 65 - Loss: 8617.480 - Metric: 0.912 - Test loss: 1223.093 - Test Metric: 0.918\n",
      "Ep. 66 - Loss: 7305.971 - Metric: 0.926 - Test loss: 1227.725 - Test Metric: 0.918\n",
      "Ep. 67 - Loss: 6884.953 - Metric: 0.927 - Test loss: 1230.533 - Test Metric: 0.918\n",
      "Ep. 68 - Loss: 10414.082 - Metric: 0.892 - Test loss: 1233.284 - Test Metric: 0.918\n",
      "Ep. 69 - Loss: 6200.633 - Metric: 0.921 - Test loss: 1236.557 - Test Metric: 0.918\n",
      "Ep. 70 - Loss: 27140.822 - Metric: 0.925 - Test loss: 1251.980 - Test Metric: 0.918\n",
      "Ep. 71 - Loss: 9691.795 - Metric: 0.830 - Test loss: 1259.696 - Test Metric: 0.918\n",
      "Ep. 72 - Loss: 8714.361 - Metric: 0.743 - Test loss: 1264.522 - Test Metric: 0.918\n",
      "Ep. 73 - Loss: 4457.662 - Metric: 0.927 - Test loss: 1267.238 - Test Metric: 0.918\n",
      "Ep. 74 - Loss: 10276.551 - Metric: 0.834 - Test loss: 1272.743 - Test Metric: 0.918\n",
      "Ep. 75 - Loss: 4152.590 - Metric: 0.860 - Test loss: 1276.344 - Test Metric: 0.918\n",
      "Ep. 76 - Loss: 19425.672 - Metric: 0.926 - Test loss: 1283.993 - Test Metric: 0.918\n",
      "Ep. 77 - Loss: 8070.613 - Metric: 0.926 - Test loss: 1289.144 - Test Metric: 0.918\n",
      "Ep. 78 - Loss: 10090.027 - Metric: 0.895 - Test loss: 1292.855 - Test Metric: 0.918\n",
      "Ep. 79 - Loss: 13434.572 - Metric: 0.851 - Test loss: 1295.203 - Test Metric: 0.918\n",
      "Ep. 80 - Loss: 22568.873 - Metric: 0.927 - Test loss: 1303.133 - Test Metric: 0.918\n",
      "Ep. 81 - Loss: 4832.708 - Metric: 0.925 - Test loss: 1307.390 - Test Metric: 0.918\n",
      "Ep. 82 - Loss: 6829.125 - Metric: 0.928 - Test loss: 1310.486 - Test Metric: 0.918\n",
      "Ep. 83 - Loss: 5722.969 - Metric: 0.927 - Test loss: 1313.084 - Test Metric: 0.918\n",
      "Ep. 84 - Loss: 5564.031 - Metric: 0.830 - Test loss: 1315.230 - Test Metric: 0.918\n",
      "Ep. 85 - Loss: 8474.472 - Metric: 0.823 - Test loss: 1318.282 - Test Metric: 0.918\n",
      "Ep. 86 - Loss: 4801.114 - Metric: 0.926 - Test loss: 1321.043 - Test Metric: 0.918\n",
      "Ep. 87 - Loss: 15976.512 - Metric: 0.925 - Test loss: 1323.361 - Test Metric: 0.918\n",
      "Ep. 88 - Loss: 8588.427 - Metric: 0.928 - Test loss: 1326.696 - Test Metric: 0.918\n",
      "Ep. 89 - Loss: 10421.395 - Metric: 0.842 - Test loss: 1345.138 - Test Metric: 0.918\n",
      "Ep. 90 - Loss: 4186.006 - Metric: 0.918 - Test loss: 1378.437 - Test Metric: 0.918\n",
      "Ep. 91 - Loss: 17271.229 - Metric: 0.840 - Test loss: 1407.216 - Test Metric: 0.918\n",
      "Ep. 92 - Loss: 7805.375 - Metric: 0.927 - Test loss: 1421.357 - Test Metric: 0.918\n",
      "Ep. 93 - Loss: 4465.637 - Metric: 0.927 - Test loss: 1430.601 - Test Metric: 0.918\n",
      "Ep. 94 - Loss: 6000.025 - Metric: 0.921 - Test loss: 1438.788 - Test Metric: 0.918\n",
      "Ep. 95 - Loss: 3381.361 - Metric: 0.880 - Test loss: 1444.104 - Test Metric: 0.918\n",
      "Ep. 96 - Loss: 13575.964 - Metric: 0.876 - Test loss: 1449.399 - Test Metric: 0.918\n",
      "Ep. 97 - Loss: 4979.968 - Metric: 0.928 - Test loss: 1456.380 - Test Metric: 0.918\n",
      "Ep. 98 - Loss: 4849.911 - Metric: 0.834 - Test loss: 1459.767 - Test Metric: 0.918\n",
      "Ep. 99 - Loss: 5512.811 - Metric: 0.927 - Test loss: 1462.660 - Test Metric: 0.918\n",
      "Ep. 100 - Loss: 5626.045 - Metric: 0.929 - Test loss: 1465.894 - Test Metric: 0.918\n",
      "Ep. 101 - Loss: 15792.290 - Metric: 0.829 - Test loss: 1470.814 - Test Metric: 0.918\n",
      "Ep. 102 - Loss: 8832.782 - Metric: 0.929 - Test loss: 1482.387 - Test Metric: 0.918\n",
      "Ep. 103 - Loss: 4590.696 - Metric: 0.833 - Test loss: 1487.828 - Test Metric: 0.918\n",
      "Ep. 104 - Loss: 4004.076 - Metric: 0.929 - Test loss: 1491.804 - Test Metric: 0.918\n",
      "Ep. 105 - Loss: 4164.809 - Metric: 0.928 - Test loss: 1493.969 - Test Metric: 0.918\n",
      "Ep. 106 - Loss: 4151.463 - Metric: 0.928 - Test loss: 1496.180 - Test Metric: 0.918\n",
      "Ep. 107 - Loss: 5184.779 - Metric: 0.836 - Test loss: 1497.778 - Test Metric: 0.918\n",
      "Ep. 108 - Loss: 4893.566 - Metric: 0.879 - Test loss: 1498.571 - Test Metric: 0.918\n",
      "Ep. 109 - Loss: 7251.058 - Metric: 0.927 - Test loss: 1499.953 - Test Metric: 0.918\n",
      "Ep. 110 - Loss: 10030.753 - Metric: 0.924 - Test loss: 1504.443 - Test Metric: 0.918\n",
      "Ep. 111 - Loss: 5299.732 - Metric: 0.784 - Test loss: 1509.352 - Test Metric: 0.918\n",
      "Ep. 112 - Loss: 4239.193 - Metric: 0.928 - Test loss: 1513.011 - Test Metric: 0.918\n",
      "Ep. 113 - Loss: 4282.061 - Metric: 0.929 - Test loss: 1514.836 - Test Metric: 0.918\n",
      "Ep. 114 - Loss: 3923.860 - Metric: 0.923 - Test loss: 1516.385 - Test Metric: 0.918\n",
      "Ep. 115 - Loss: 6857.646 - Metric: 0.918 - Test loss: 1518.461 - Test Metric: 0.918\n",
      "Ep. 116 - Loss: 3861.658 - Metric: 0.928 - Test loss: 1522.061 - Test Metric: 0.918\n",
      "Ep. 117 - Loss: 4119.488 - Metric: 0.874 - Test loss: 1524.384 - Test Metric: 0.918\n",
      "Ep. 118 - Loss: 11210.896 - Metric: 0.927 - Test loss: 1527.073 - Test Metric: 0.918\n",
      "Ep. 119 - Loss: 7325.644 - Metric: 0.929 - Test loss: 1529.351 - Test Metric: 0.918\n",
      "Ep. 120 - Loss: 5060.586 - Metric: 0.929 - Test loss: 1531.876 - Test Metric: 0.918\n",
      "Ep. 121 - Loss: 6353.203 - Metric: 0.929 - Test loss: 1533.646 - Test Metric: 0.918\n",
      "Ep. 122 - Loss: 4246.165 - Metric: 0.852 - Test loss: 1537.006 - Test Metric: 0.918\n",
      "Ep. 123 - Loss: 3078.854 - Metric: 0.928 - Test loss: 1538.271 - Test Metric: 0.918\n",
      "Ep. 124 - Loss: 8094.982 - Metric: 0.925 - Test loss: 1539.973 - Test Metric: 0.918\n",
      "Ep. 125 - Loss: 4423.337 - Metric: 0.925 - Test loss: 1542.684 - Test Metric: 0.918\n",
      "Ep. 126 - Loss: 10188.791 - Metric: 0.929 - Test loss: 1545.103 - Test Metric: 0.918\n",
      "Ep. 127 - Loss: 3568.512 - Metric: 0.921 - Test loss: 1554.537 - Test Metric: 0.918\n",
      "Ep. 128 - Loss: 4620.146 - Metric: 0.929 - Test loss: 1557.683 - Test Metric: 0.918\n",
      "Ep. 129 - Loss: 5530.653 - Metric: 0.929 - Test loss: 1559.060 - Test Metric: 0.918\n",
      "Ep. 130 - Loss: 3118.780 - Metric: 0.895 - Test loss: 1560.125 - Test Metric: 0.918\n",
      "Ep. 131 - Loss: 7334.683 - Metric: 0.928 - Test loss: 1567.025 - Test Metric: 0.918\n",
      "Ep. 132 - Loss: 3595.366 - Metric: 0.928 - Test loss: 1570.039 - Test Metric: 0.918\n",
      "Ep. 133 - Loss: 4957.712 - Metric: 0.841 - Test loss: 1572.131 - Test Metric: 0.918\n",
      "Ep. 134 - Loss: 3587.011 - Metric: 0.929 - Test loss: 1572.465 - Test Metric: 0.918\n",
      "Ep. 135 - Loss: 7246.936 - Metric: 0.919 - Test loss: 1573.821 - Test Metric: 0.918\n",
      "Ep. 136 - Loss: 2794.422 - Metric: 0.929 - Test loss: 1574.535 - Test Metric: 0.918\n",
      "Ep. 137 - Loss: 11849.840 - Metric: 0.929 - Test loss: 1594.593 - Test Metric: 0.918\n",
      "Ep. 138 - Loss: 3124.814 - Metric: 0.929 - Test loss: 1601.287 - Test Metric: 0.918\n",
      "Ep. 139 - Loss: 6666.209 - Metric: 0.930 - Test loss: 1604.488 - Test Metric: 0.918\n",
      "Ep. 140 - Loss: 11399.463 - Metric: 0.928 - Test loss: 1607.142 - Test Metric: 0.918\n",
      "Ep. 141 - Loss: 2477.649 - Metric: 0.929 - Test loss: 1608.350 - Test Metric: 0.918\n",
      "Ep. 142 - Loss: 6398.242 - Metric: 0.929 - Test loss: 1609.233 - Test Metric: 0.918\n",
      "Ep. 143 - Loss: 7061.171 - Metric: 0.926 - Test loss: 1614.564 - Test Metric: 0.918\n",
      "Ep. 144 - Loss: 6029.345 - Metric: 0.925 - Test loss: 1617.090 - Test Metric: 0.918\n",
      "Ep. 145 - Loss: 2270.782 - Metric: 0.929 - Test loss: 1618.391 - Test Metric: 0.918\n",
      "Ep. 146 - Loss: 4625.542 - Metric: 0.841 - Test loss: 1618.218 - Test Metric: 0.918\n",
      "Ep. 147 - Loss: 3901.884 - Metric: 0.928 - Test loss: 1618.884 - Test Metric: 0.918\n",
      "Ep. 148 - Loss: 6259.739 - Metric: 0.926 - Test loss: 1620.466 - Test Metric: 0.918\n",
      "Ep. 149 - Loss: 8778.659 - Metric: 0.927 - Test loss: 1622.086 - Test Metric: 0.918\n",
      "Ep. 150 - Loss: 3968.181 - Metric: 0.929 - Test loss: 1623.486 - Test Metric: 0.918\n",
      "Ep. 151 - Loss: 1785.168 - Metric: 0.929 - Test loss: 1624.212 - Test Metric: 0.918\n",
      "Ep. 152 - Loss: 3514.649 - Metric: 0.928 - Test loss: 1625.264 - Test Metric: 0.918\n",
      "Ep. 153 - Loss: 3225.594 - Metric: 0.921 - Test loss: 1624.993 - Test Metric: 0.918\n",
      "Ep. 154 - Loss: 3888.628 - Metric: 0.929 - Test loss: 1624.837 - Test Metric: 0.918\n",
      "Ep. 155 - Loss: 4596.246 - Metric: 0.928 - Test loss: 1624.669 - Test Metric: 0.918\n",
      "Ep. 156 - Loss: 4726.772 - Metric: 0.930 - Test loss: 1624.556 - Test Metric: 0.918\n",
      "Ep. 157 - Loss: 3746.777 - Metric: 0.829 - Test loss: 1623.748 - Test Metric: 0.918\n",
      "Ep. 158 - Loss: 3363.149 - Metric: 0.928 - Test loss: 1623.046 - Test Metric: 0.918\n",
      "Ep. 159 - Loss: 3178.718 - Metric: 0.927 - Test loss: 1622.233 - Test Metric: 0.918\n",
      "Ep. 160 - Loss: 3666.179 - Metric: 0.929 - Test loss: 1626.790 - Test Metric: 0.918\n",
      "Ep. 161 - Loss: 4892.086 - Metric: 0.917 - Test loss: 1627.274 - Test Metric: 0.918\n",
      "Ep. 162 - Loss: 1893.439 - Metric: 0.929 - Test loss: 1626.815 - Test Metric: 0.918\n",
      "Ep. 163 - Loss: 4677.259 - Metric: 0.927 - Test loss: 1626.135 - Test Metric: 0.918\n",
      "Ep. 164 - Loss: 3663.583 - Metric: 0.920 - Test loss: 1625.364 - Test Metric: 0.918\n",
      "Ep. 165 - Loss: 5136.968 - Metric: 0.929 - Test loss: 1626.661 - Test Metric: 0.918\n",
      "Ep. 166 - Loss: 6115.565 - Metric: 0.929 - Test loss: 1628.736 - Test Metric: 0.918\n",
      "Ep. 167 - Loss: 4871.427 - Metric: 0.928 - Test loss: 1630.848 - Test Metric: 0.918\n",
      "Ep. 168 - Loss: 4264.424 - Metric: 0.836 - Test loss: 1630.856 - Test Metric: 0.918\n",
      "Ep. 169 - Loss: 3384.298 - Metric: 0.929 - Test loss: 1630.714 - Test Metric: 0.918\n",
      "Ep. 170 - Loss: 2363.740 - Metric: 0.929 - Test loss: 1630.425 - Test Metric: 0.918\n",
      "Ep. 171 - Loss: 5025.568 - Metric: 0.929 - Test loss: 1629.977 - Test Metric: 0.918\n",
      "Ep. 172 - Loss: 3804.437 - Metric: 0.929 - Test loss: 1629.260 - Test Metric: 0.918\n",
      "Ep. 173 - Loss: 3084.330 - Metric: 0.929 - Test loss: 1628.747 - Test Metric: 0.918\n",
      "Ep. 174 - Loss: 4764.915 - Metric: 0.835 - Test loss: 1627.458 - Test Metric: 0.918\n",
      "Ep. 175 - Loss: 6660.750 - Metric: 0.841 - Test loss: 1626.227 - Test Metric: 0.918\n",
      "Ep. 176 - Loss: 2062.232 - Metric: 0.929 - Test loss: 1625.287 - Test Metric: 0.918\n",
      "Ep. 177 - Loss: 8569.291 - Metric: 0.841 - Test loss: 1638.627 - Test Metric: 0.918\n",
      "Ep. 178 - Loss: 3493.632 - Metric: 0.928 - Test loss: 1644.816 - Test Metric: 0.918\n",
      "Ep. 179 - Loss: 9465.568 - Metric: 0.927 - Test loss: 1648.262 - Test Metric: 0.918\n",
      "Ep. 180 - Loss: 3873.433 - Metric: 0.926 - Test loss: 1648.660 - Test Metric: 0.918\n",
      "Ep. 181 - Loss: 4860.170 - Metric: 0.929 - Test loss: 1648.533 - Test Metric: 0.918\n",
      "Ep. 182 - Loss: 3869.761 - Metric: 0.929 - Test loss: 1646.925 - Test Metric: 0.918\n",
      "Ep. 183 - Loss: 4652.272 - Metric: 0.929 - Test loss: 1645.051 - Test Metric: 0.918\n",
      "Ep. 184 - Loss: 9273.268 - Metric: 0.929 - Test loss: 1652.162 - Test Metric: 0.918\n",
      "Ep. 185 - Loss: 4009.485 - Metric: 0.929 - Test loss: 1654.908 - Test Metric: 0.918\n",
      "Ep. 186 - Loss: 2712.356 - Metric: 0.930 - Test loss: 1654.499 - Test Metric: 0.918\n",
      "Ep. 187 - Loss: 6892.319 - Metric: 0.929 - Test loss: 1653.831 - Test Metric: 0.918\n",
      "Ep. 188 - Loss: 3940.922 - Metric: 0.929 - Test loss: 1652.623 - Test Metric: 0.918\n",
      "Ep. 189 - Loss: 2290.502 - Metric: 0.928 - Test loss: 1651.340 - Test Metric: 0.918\n",
      "Ep. 190 - Loss: 2947.825 - Metric: 0.929 - Test loss: 1649.525 - Test Metric: 0.918\n",
      "Ep. 191 - Loss: 3777.163 - Metric: 0.929 - Test loss: 1647.709 - Test Metric: 0.918\n",
      "Ep. 192 - Loss: 3153.297 - Metric: 0.928 - Test loss: 1646.249 - Test Metric: 0.918\n",
      "Ep. 193 - Loss: 3348.626 - Metric: 0.929 - Test loss: 1644.708 - Test Metric: 0.918\n",
      "Ep. 194 - Loss: 5673.919 - Metric: 0.930 - Test loss: 1645.757 - Test Metric: 0.918\n",
      "Ep. 195 - Loss: 2627.258 - Metric: 0.929 - Test loss: 1645.318 - Test Metric: 0.918\n",
      "Ep. 196 - Loss: 3884.119 - Metric: 0.850 - Test loss: 1647.268 - Test Metric: 0.918\n",
      "Ep. 197 - Loss: 2886.380 - Metric: 0.929 - Test loss: 1646.693 - Test Metric: 0.918\n",
      "Ep. 198 - Loss: 4822.115 - Metric: 0.930 - Test loss: 1646.236 - Test Metric: 0.918\n",
      "Ep. 199 - Loss: 5353.701 - Metric: 0.930 - Test loss: 1645.513 - Test Metric: 0.918\n",
      "Ep. 200 - Loss: 4645.820 - Metric: 0.930 - Test loss: 1644.349 - Test Metric: 0.918\n",
      "Ep. 201 - Loss: 2685.315 - Metric: 0.928 - Test loss: 1643.402 - Test Metric: 0.918\n",
      "Ep. 202 - Loss: 3971.845 - Metric: 0.927 - Test loss: 1641.970 - Test Metric: 0.918\n",
      "Ep. 203 - Loss: 2386.779 - Metric: 0.929 - Test loss: 1639.528 - Test Metric: 0.918\n",
      "Ep. 204 - Loss: 3569.009 - Metric: 0.929 - Test loss: 1636.615 - Test Metric: 0.918\n",
      "Ep. 205 - Loss: 3952.771 - Metric: 0.928 - Test loss: 1634.803 - Test Metric: 0.918\n",
      "Ep. 206 - Loss: 2678.946 - Metric: 0.895 - Test loss: 1633.358 - Test Metric: 0.918\n",
      "Ep. 207 - Loss: 3319.342 - Metric: 0.930 - Test loss: 1633.406 - Test Metric: 0.918\n",
      "Ep. 208 - Loss: 4144.961 - Metric: 0.929 - Test loss: 1631.950 - Test Metric: 0.918\n",
      "Ep. 209 - Loss: 4971.219 - Metric: 0.927 - Test loss: 1637.919 - Test Metric: 0.918\n",
      "Ep. 210 - Loss: 4832.553 - Metric: 0.929 - Test loss: 1640.224 - Test Metric: 0.918\n",
      "Ep. 211 - Loss: 3420.526 - Metric: 0.930 - Test loss: 1640.066 - Test Metric: 0.918\n",
      "Ep. 212 - Loss: 5346.456 - Metric: 0.836 - Test loss: 1638.858 - Test Metric: 0.918\n",
      "Ep. 213 - Loss: 3416.952 - Metric: 0.929 - Test loss: 1637.287 - Test Metric: 0.918\n",
      "Ep. 214 - Loss: 3850.052 - Metric: 0.929 - Test loss: 1644.580 - Test Metric: 0.918\n",
      "Ep. 215 - Loss: 3718.227 - Metric: 0.929 - Test loss: 1647.220 - Test Metric: 0.918\n",
      "Ep. 216 - Loss: 2745.863 - Metric: 0.930 - Test loss: 1646.913 - Test Metric: 0.918\n",
      "Ep. 217 - Loss: 2982.092 - Metric: 0.930 - Test loss: 1644.966 - Test Metric: 0.918\n",
      "Ep. 218 - Loss: 3577.873 - Metric: 0.930 - Test loss: 1641.908 - Test Metric: 0.918\n",
      "Ep. 219 - Loss: 2723.080 - Metric: 0.929 - Test loss: 1638.745 - Test Metric: 0.918\n",
      "Ep. 220 - Loss: 3379.733 - Metric: 0.928 - Test loss: 1635.492 - Test Metric: 0.918\n",
      "Ep. 221 - Loss: 2824.638 - Metric: 0.928 - Test loss: 1632.966 - Test Metric: 0.918\n",
      "Ep. 222 - Loss: 3999.462 - Metric: 0.929 - Test loss: 1631.176 - Test Metric: 0.918\n",
      "Ep. 223 - Loss: 3281.111 - Metric: 0.929 - Test loss: 1629.055 - Test Metric: 0.918\n",
      "Ep. 224 - Loss: 5391.511 - Metric: 0.929 - Test loss: 1625.831 - Test Metric: 0.918\n",
      "Ep. 225 - Loss: 2719.849 - Metric: 0.929 - Test loss: 1623.146 - Test Metric: 0.918\n",
      "Ep. 226 - Loss: 2403.716 - Metric: 0.929 - Test loss: 1621.426 - Test Metric: 0.918\n",
      "Ep. 227 - Loss: 2150.548 - Metric: 0.929 - Test loss: 1620.108 - Test Metric: 0.918\n",
      "Ep. 228 - Loss: 5386.043 - Metric: 0.929 - Test loss: 1618.865 - Test Metric: 0.918\n",
      "Ep. 229 - Loss: 3585.274 - Metric: 0.930 - Test loss: 1616.615 - Test Metric: 0.918\n",
      "Ep. 230 - Loss: 2305.694 - Metric: 0.929 - Test loss: 1614.473 - Test Metric: 0.918\n",
      "Ep. 231 - Loss: 2351.073 - Metric: 0.929 - Test loss: 1612.695 - Test Metric: 0.918\n",
      "Ep. 232 - Loss: 2199.505 - Metric: 0.929 - Test loss: 1611.217 - Test Metric: 0.918\n",
      "Ep. 233 - Loss: 3400.981 - Metric: 0.929 - Test loss: 1609.634 - Test Metric: 0.918\n",
      "Ep. 234 - Loss: 2691.279 - Metric: 0.807 - Test loss: 1608.515 - Test Metric: 0.918\n",
      "Ep. 235 - Loss: 2929.325 - Metric: 0.929 - Test loss: 1607.137 - Test Metric: 0.918\n",
      "Ep. 236 - Loss: 2253.591 - Metric: 0.930 - Test loss: 1606.196 - Test Metric: 0.918\n",
      "Ep. 237 - Loss: 3338.820 - Metric: 0.930 - Test loss: 1605.813 - Test Metric: 0.918\n",
      "Ep. 238 - Loss: 2842.175 - Metric: 0.929 - Test loss: 1605.195 - Test Metric: 0.918\n",
      "Ep. 239 - Loss: 2743.408 - Metric: 0.930 - Test loss: 1605.474 - Test Metric: 0.918\n",
      "Ep. 240 - Loss: 4265.123 - Metric: 0.929 - Test loss: 1604.341 - Test Metric: 0.918\n",
      "Ep. 241 - Loss: 3807.251 - Metric: 0.923 - Test loss: 1603.099 - Test Metric: 0.918\n",
      "Ep. 242 - Loss: 3713.873 - Metric: 0.929 - Test loss: 1601.494 - Test Metric: 0.918\n",
      "Ep. 243 - Loss: 3506.957 - Metric: 0.885 - Test loss: 1598.972 - Test Metric: 0.918\n",
      "Ep. 244 - Loss: 2440.439 - Metric: 0.927 - Test loss: 1596.512 - Test Metric: 0.918\n",
      "Ep. 245 - Loss: 5107.266 - Metric: 0.927 - Test loss: 1607.741 - Test Metric: 0.918\n",
      "Ep. 246 - Loss: 2540.205 - Metric: 0.928 - Test loss: 1614.894 - Test Metric: 0.918\n",
      "Ep. 247 - Loss: 2858.581 - Metric: 0.929 - Test loss: 1616.931 - Test Metric: 0.918\n",
      "Ep. 248 - Loss: 2900.792 - Metric: 0.928 - Test loss: 1616.431 - Test Metric: 0.918\n",
      "Ep. 249 - Loss: 3467.857 - Metric: 0.924 - Test loss: 1613.862 - Test Metric: 0.918\n",
      "Ep. 250 - Loss: 3480.236 - Metric: 0.930 - Test loss: 1611.023 - Test Metric: 0.918\n",
      "Ep. 251 - Loss: 3181.736 - Metric: 0.930 - Test loss: 1608.571 - Test Metric: 0.918\n",
      "Ep. 252 - Loss: 5307.152 - Metric: 0.929 - Test loss: 1608.263 - Test Metric: 0.918\n",
      "Ep. 253 - Loss: 2657.758 - Metric: 0.930 - Test loss: 1608.864 - Test Metric: 0.918\n",
      "Ep. 254 - Loss: 4162.228 - Metric: 0.930 - Test loss: 1608.848 - Test Metric: 0.918\n",
      "Ep. 255 - Loss: 4291.603 - Metric: 0.930 - Test loss: 1605.669 - Test Metric: 0.918\n",
      "Ep. 256 - Loss: 3555.296 - Metric: 0.929 - Test loss: 1601.954 - Test Metric: 0.918\n",
      "Ep. 257 - Loss: 2795.982 - Metric: 0.930 - Test loss: 1599.553 - Test Metric: 0.918\n",
      "Ep. 258 - Loss: 1900.479 - Metric: 0.930 - Test loss: 1598.035 - Test Metric: 0.918\n",
      "Ep. 259 - Loss: 2439.118 - Metric: 0.929 - Test loss: 1596.448 - Test Metric: 0.918\n",
      "Ep. 260 - Loss: 3458.615 - Metric: 0.929 - Test loss: 1594.193 - Test Metric: 0.918\n",
      "Ep. 261 - Loss: 2732.663 - Metric: 0.862 - Test loss: 1591.298 - Test Metric: 0.918\n",
      "Ep. 262 - Loss: 3617.084 - Metric: 0.929 - Test loss: 1588.988 - Test Metric: 0.918\n",
      "Ep. 263 - Loss: 2792.646 - Metric: 0.928 - Test loss: 1586.555 - Test Metric: 0.918\n",
      "Ep. 264 - Loss: 3522.865 - Metric: 0.928 - Test loss: 1583.471 - Test Metric: 0.918\n",
      "Ep. 265 - Loss: 3567.594 - Metric: 0.929 - Test loss: 1580.019 - Test Metric: 0.918\n",
      "Ep. 266 - Loss: 1650.496 - Metric: 0.929 - Test loss: 1576.984 - Test Metric: 0.918\n",
      "Ep. 267 - Loss: 3299.274 - Metric: 0.929 - Test loss: 1574.167 - Test Metric: 0.918\n",
      "Ep. 268 - Loss: 3176.235 - Metric: 0.929 - Test loss: 1572.390 - Test Metric: 0.918\n",
      "Ep. 269 - Loss: 3536.556 - Metric: 0.930 - Test loss: 1569.593 - Test Metric: 0.918\n",
      "Ep. 270 - Loss: 2387.979 - Metric: 0.929 - Test loss: 1567.417 - Test Metric: 0.918\n",
      "Ep. 271 - Loss: 2031.573 - Metric: 0.930 - Test loss: 1565.864 - Test Metric: 0.918\n",
      "Ep. 272 - Loss: 3555.446 - Metric: 0.929 - Test loss: 1563.016 - Test Metric: 0.918\n",
      "Ep. 273 - Loss: 5559.898 - Metric: 0.929 - Test loss: 1559.811 - Test Metric: 0.918\n",
      "Ep. 274 - Loss: 2780.397 - Metric: 0.930 - Test loss: 1557.184 - Test Metric: 0.918\n",
      "Ep. 275 - Loss: 2996.236 - Metric: 0.929 - Test loss: 1553.766 - Test Metric: 0.918\n",
      "Ep. 276 - Loss: 4443.058 - Metric: 0.929 - Test loss: 1550.209 - Test Metric: 0.918\n",
      "Ep. 277 - Loss: 2783.698 - Metric: 0.929 - Test loss: 1551.133 - Test Metric: 0.918\n",
      "Ep. 278 - Loss: 3148.672 - Metric: 0.929 - Test loss: 1549.921 - Test Metric: 0.918\n",
      "Ep. 279 - Loss: 1276.560 - Metric: 0.922 - Test loss: 1547.934 - Test Metric: 0.918\n",
      "Ep. 280 - Loss: 4266.517 - Metric: 0.929 - Test loss: 1544.469 - Test Metric: 0.918\n",
      "Ep. 281 - Loss: 2253.510 - Metric: 0.930 - Test loss: 1541.577 - Test Metric: 0.918\n",
      "Ep. 282 - Loss: 5096.946 - Metric: 0.929 - Test loss: 1537.672 - Test Metric: 0.918\n",
      "Ep. 283 - Loss: 2627.346 - Metric: 0.930 - Test loss: 1533.848 - Test Metric: 0.918\n",
      "Ep. 284 - Loss: 2980.861 - Metric: 0.929 - Test loss: 1530.453 - Test Metric: 0.918\n",
      "Ep. 285 - Loss: 2599.317 - Metric: 0.930 - Test loss: 1528.082 - Test Metric: 0.918\n",
      "Ep. 286 - Loss: 2918.539 - Metric: 0.929 - Test loss: 1524.909 - Test Metric: 0.918\n",
      "Ep. 287 - Loss: 7240.794 - Metric: 0.929 - Test loss: 1522.052 - Test Metric: 0.918\n",
      "Ep. 288 - Loss: 3037.202 - Metric: 0.930 - Test loss: 1518.953 - Test Metric: 0.918\n",
      "Ep. 289 - Loss: 2456.184 - Metric: 0.921 - Test loss: 1515.530 - Test Metric: 0.918\n",
      "Ep. 290 - Loss: 4826.650 - Metric: 0.930 - Test loss: 1510.993 - Test Metric: 0.918\n",
      "Ep. 291 - Loss: 2528.232 - Metric: 0.929 - Test loss: 1508.081 - Test Metric: 0.918\n",
      "Ep. 292 - Loss: 4590.483 - Metric: 0.929 - Test loss: 1504.812 - Test Metric: 0.918\n",
      "Ep. 293 - Loss: 2572.571 - Metric: 0.930 - Test loss: 1501.156 - Test Metric: 0.918\n",
      "Ep. 294 - Loss: 3718.451 - Metric: 0.928 - Test loss: 1498.640 - Test Metric: 0.918\n",
      "Ep. 295 - Loss: 2578.020 - Metric: 0.895 - Test loss: 1495.993 - Test Metric: 0.918\n",
      "Ep. 296 - Loss: 2188.978 - Metric: 0.929 - Test loss: 1493.664 - Test Metric: 0.918\n",
      "Ep. 297 - Loss: 2047.052 - Metric: 0.929 - Test loss: 1491.179 - Test Metric: 0.918\n",
      "Ep. 298 - Loss: 3152.682 - Metric: 0.929 - Test loss: 1488.590 - Test Metric: 0.918\n",
      "Ep. 299 - Loss: 2448.080 - Metric: 0.929 - Test loss: 1487.336 - Test Metric: 0.918\n",
      "Ep. 300 - Loss: 5590.601 - Metric: 0.928 - Test loss: 1489.145 - Test Metric: 0.918\n",
      "Ep. 301 - Loss: 3150.700 - Metric: 0.929 - Test loss: 1487.341 - Test Metric: 0.918\n",
      "Ep. 302 - Loss: 3502.216 - Metric: 0.930 - Test loss: 1483.769 - Test Metric: 0.918\n",
      "Ep. 303 - Loss: 2773.195 - Metric: 0.928 - Test loss: 1479.693 - Test Metric: 0.918\n",
      "Ep. 304 - Loss: 2293.728 - Metric: 0.930 - Test loss: 1476.258 - Test Metric: 0.918\n",
      "Ep. 305 - Loss: 3617.296 - Metric: 0.930 - Test loss: 1472.862 - Test Metric: 0.918\n",
      "Ep. 306 - Loss: 2803.859 - Metric: 0.929 - Test loss: 1468.812 - Test Metric: 0.918\n",
      "Ep. 307 - Loss: 4047.489 - Metric: 0.929 - Test loss: 1463.864 - Test Metric: 0.918\n",
      "Ep. 308 - Loss: 3857.373 - Metric: 0.928 - Test loss: 1465.798 - Test Metric: 0.918\n",
      "Ep. 309 - Loss: 2961.130 - Metric: 0.853 - Test loss: 1465.166 - Test Metric: 0.918\n",
      "Ep. 310 - Loss: 2248.458 - Metric: 0.928 - Test loss: 1463.487 - Test Metric: 0.918\n",
      "Ep. 311 - Loss: 2982.003 - Metric: 0.929 - Test loss: 1460.280 - Test Metric: 0.918\n",
      "Ep. 312 - Loss: 3453.906 - Metric: 0.929 - Test loss: 1456.731 - Test Metric: 0.918\n",
      "Ep. 313 - Loss: 3521.753 - Metric: 0.929 - Test loss: 1453.254 - Test Metric: 0.918\n",
      "Ep. 314 - Loss: 3547.784 - Metric: 0.930 - Test loss: 1450.166 - Test Metric: 0.918\n",
      "Ep. 315 - Loss: 2021.258 - Metric: 0.929 - Test loss: 1448.661 - Test Metric: 0.918\n",
      "Ep. 316 - Loss: 3215.164 - Metric: 0.928 - Test loss: 1447.543 - Test Metric: 0.918\n",
      "Ep. 317 - Loss: 2296.700 - Metric: 0.929 - Test loss: 1445.471 - Test Metric: 0.918\n",
      "Ep. 318 - Loss: 3991.624 - Metric: 0.896 - Test loss: 1442.817 - Test Metric: 0.918\n",
      "Ep. 319 - Loss: 6528.299 - Metric: 0.926 - Test loss: 1438.995 - Test Metric: 0.918\n",
      "Ep. 320 - Loss: 4050.740 - Metric: 0.928 - Test loss: 1434.634 - Test Metric: 0.918\n",
      "Ep. 321 - Loss: 1138.029 - Metric: 0.929 - Test loss: 1431.176 - Test Metric: 0.918\n",
      "Ep. 322 - Loss: 3735.867 - Metric: 0.930 - Test loss: 1428.190 - Test Metric: 0.918\n",
      "Ep. 323 - Loss: 1860.367 - Metric: 0.930 - Test loss: 1424.240 - Test Metric: 0.918\n",
      "Ep. 324 - Loss: 2058.609 - Metric: 0.929 - Test loss: 1422.555 - Test Metric: 0.918\n",
      "Ep. 325 - Loss: 4172.719 - Metric: 0.929 - Test loss: 1420.193 - Test Metric: 0.918\n",
      "Ep. 326 - Loss: 6350.135 - Metric: 0.928 - Test loss: 1419.549 - Test Metric: 0.918\n",
      "Ep. 327 - Loss: 3819.536 - Metric: 0.928 - Test loss: 1416.271 - Test Metric: 0.918\n",
      "Ep. 328 - Loss: 3090.122 - Metric: 0.928 - Test loss: 1413.257 - Test Metric: 0.918\n",
      "Ep. 329 - Loss: 2309.949 - Metric: 0.930 - Test loss: 1410.044 - Test Metric: 0.918\n",
      "Ep. 330 - Loss: 4711.693 - Metric: 0.929 - Test loss: 1407.177 - Test Metric: 0.918\n",
      "Ep. 331 - Loss: 829.562 - Metric: 0.930 - Test loss: 1404.946 - Test Metric: 0.918\n",
      "Ep. 332 - Loss: 3215.100 - Metric: 0.929 - Test loss: 1401.928 - Test Metric: 0.918\n",
      "Ep. 333 - Loss: 1479.214 - Metric: 0.926 - Test loss: 1403.266 - Test Metric: 0.918\n",
      "Ep. 334 - Loss: 3979.859 - Metric: 0.929 - Test loss: 1402.456 - Test Metric: 0.918\n",
      "Ep. 335 - Loss: 3251.047 - Metric: 0.928 - Test loss: 1401.247 - Test Metric: 0.918\n",
      "Ep. 336 - Loss: 2832.858 - Metric: 0.929 - Test loss: 1398.829 - Test Metric: 0.918\n",
      "Ep. 337 - Loss: 3709.329 - Metric: 0.930 - Test loss: 1394.734 - Test Metric: 0.918\n",
      "Ep. 338 - Loss: 3354.602 - Metric: 0.930 - Test loss: 1389.649 - Test Metric: 0.918\n",
      "Ep. 339 - Loss: 2611.052 - Metric: 0.930 - Test loss: 1384.609 - Test Metric: 0.918\n",
      "Ep. 340 - Loss: 1886.194 - Metric: 0.929 - Test loss: 1381.006 - Test Metric: 0.918\n",
      "Ep. 341 - Loss: 1229.239 - Metric: 0.930 - Test loss: 1379.224 - Test Metric: 0.918\n",
      "Ep. 342 - Loss: 1665.059 - Metric: 0.929 - Test loss: 1377.943 - Test Metric: 0.918\n",
      "Ep. 343 - Loss: 2010.676 - Metric: 0.930 - Test loss: 1377.306 - Test Metric: 0.918\n",
      "Ep. 344 - Loss: 2391.952 - Metric: 0.930 - Test loss: 1376.520 - Test Metric: 0.918\n",
      "Ep. 345 - Loss: 2458.184 - Metric: 0.930 - Test loss: 1373.377 - Test Metric: 0.918\n",
      "Ep. 346 - Loss: 2583.145 - Metric: 0.929 - Test loss: 1370.807 - Test Metric: 0.918\n",
      "Ep. 347 - Loss: 2974.966 - Metric: 0.876 - Test loss: 1367.983 - Test Metric: 0.918\n",
      "Ep. 348 - Loss: 3118.913 - Metric: 0.928 - Test loss: 1364.489 - Test Metric: 0.918\n",
      "Ep. 349 - Loss: 2850.732 - Metric: 0.929 - Test loss: 1380.516 - Test Metric: 0.918\n",
      "Ep. 350 - Loss: 2829.870 - Metric: 0.863 - Test loss: 1383.405 - Test Metric: 0.918\n",
      "Ep. 351 - Loss: 1933.341 - Metric: 0.929 - Test loss: 1381.736 - Test Metric: 0.918\n",
      "Ep. 352 - Loss: 1405.698 - Metric: 0.930 - Test loss: 1379.540 - Test Metric: 0.918\n",
      "Ep. 353 - Loss: 3722.271 - Metric: 0.930 - Test loss: 1376.295 - Test Metric: 0.918\n",
      "Ep. 354 - Loss: 1223.376 - Metric: 0.930 - Test loss: 1371.946 - Test Metric: 0.918\n",
      "Ep. 355 - Loss: 3525.670 - Metric: 0.837 - Test loss: 1367.807 - Test Metric: 0.918\n",
      "Ep. 356 - Loss: 5665.233 - Metric: 0.928 - Test loss: 1364.634 - Test Metric: 0.918\n",
      "Ep. 357 - Loss: 2979.266 - Metric: 0.930 - Test loss: 1360.762 - Test Metric: 0.918\n",
      "Ep. 358 - Loss: 1464.911 - Metric: 0.929 - Test loss: 1356.532 - Test Metric: 0.918\n",
      "Ep. 359 - Loss: 2745.335 - Metric: 0.930 - Test loss: 1353.016 - Test Metric: 0.918\n",
      "Ep. 360 - Loss: 3071.840 - Metric: 0.929 - Test loss: 1350.069 - Test Metric: 0.918\n",
      "Ep. 361 - Loss: 2766.260 - Metric: 0.929 - Test loss: 1345.592 - Test Metric: 0.918\n",
      "Ep. 362 - Loss: 3226.460 - Metric: 0.930 - Test loss: 1341.858 - Test Metric: 0.918\n",
      "Ep. 363 - Loss: 2061.160 - Metric: 0.929 - Test loss: 1337.556 - Test Metric: 0.918\n",
      "Ep. 364 - Loss: 3770.974 - Metric: 0.930 - Test loss: 1332.015 - Test Metric: 0.918\n",
      "Ep. 365 - Loss: 3916.216 - Metric: 0.930 - Test loss: 1327.585 - Test Metric: 0.918\n",
      "Ep. 366 - Loss: 5464.410 - Metric: 0.929 - Test loss: 1325.070 - Test Metric: 0.918\n",
      "Ep. 367 - Loss: 1515.691 - Metric: 0.926 - Test loss: 1321.698 - Test Metric: 0.918\n",
      "Ep. 368 - Loss: 2076.209 - Metric: 0.930 - Test loss: 1319.764 - Test Metric: 0.918\n",
      "Ep. 369 - Loss: 1771.471 - Metric: 0.929 - Test loss: 1316.512 - Test Metric: 0.918\n",
      "Ep. 370 - Loss: 2564.188 - Metric: 0.833 - Test loss: 1313.804 - Test Metric: 0.918\n",
      "Ep. 371 - Loss: 1824.466 - Metric: 0.929 - Test loss: 1312.402 - Test Metric: 0.918\n",
      "Ep. 372 - Loss: 3433.652 - Metric: 0.929 - Test loss: 1310.018 - Test Metric: 0.918\n",
      "Ep. 373 - Loss: 3517.085 - Metric: 0.837 - Test loss: 1306.723 - Test Metric: 0.918\n",
      "Ep. 374 - Loss: 2488.119 - Metric: 0.929 - Test loss: 1301.746 - Test Metric: 0.918\n",
      "Ep. 375 - Loss: 2375.905 - Metric: 0.928 - Test loss: 1297.482 - Test Metric: 0.918\n",
      "Ep. 376 - Loss: 2635.491 - Metric: 0.929 - Test loss: 1292.883 - Test Metric: 0.918\n",
      "Ep. 377 - Loss: 3251.872 - Metric: 0.929 - Test loss: 1296.002 - Test Metric: 0.918\n",
      "Ep. 378 - Loss: 2339.031 - Metric: 0.863 - Test loss: 1293.236 - Test Metric: 0.918\n",
      "Ep. 379 - Loss: 7267.677 - Metric: 0.930 - Test loss: 1297.462 - Test Metric: 0.918\n",
      "Ep. 380 - Loss: 3220.052 - Metric: 0.930 - Test loss: 1296.738 - Test Metric: 0.918\n",
      "Ep. 381 - Loss: 2935.321 - Metric: 0.928 - Test loss: 1292.987 - Test Metric: 0.918\n",
      "Ep. 382 - Loss: 2059.835 - Metric: 0.929 - Test loss: 1289.499 - Test Metric: 0.918\n",
      "Ep. 383 - Loss: 2095.232 - Metric: 0.929 - Test loss: 1286.465 - Test Metric: 0.918\n",
      "Ep. 384 - Loss: 2537.608 - Metric: 0.929 - Test loss: 1281.515 - Test Metric: 0.918\n",
      "Ep. 385 - Loss: 1662.360 - Metric: 0.929 - Test loss: 1276.936 - Test Metric: 0.918\n",
      "Ep. 386 - Loss: 1345.403 - Metric: 0.927 - Test loss: 1273.002 - Test Metric: 0.918\n",
      "Ep. 387 - Loss: 2841.642 - Metric: 0.930 - Test loss: 1267.977 - Test Metric: 0.918\n",
      "Ep. 388 - Loss: 1479.456 - Metric: 0.930 - Test loss: 1264.323 - Test Metric: 0.918\n",
      "Ep. 389 - Loss: 2551.238 - Metric: 0.927 - Test loss: 1260.640 - Test Metric: 0.918\n",
      "Ep. 390 - Loss: 2521.372 - Metric: 0.929 - Test loss: 1256.147 - Test Metric: 0.918\n",
      "Ep. 391 - Loss: 1312.400 - Metric: 0.930 - Test loss: 1253.459 - Test Metric: 0.918\n",
      "Ep. 392 - Loss: 1170.889 - Metric: 0.930 - Test loss: 1251.808 - Test Metric: 0.918\n",
      "Ep. 393 - Loss: 2136.290 - Metric: 0.930 - Test loss: 1249.346 - Test Metric: 0.918\n",
      "Ep. 394 - Loss: 2738.327 - Metric: 0.930 - Test loss: 1244.906 - Test Metric: 0.918\n",
      "Ep. 395 - Loss: 1980.098 - Metric: 0.929 - Test loss: 1240.839 - Test Metric: 0.918\n",
      "Ep. 396 - Loss: 2508.943 - Metric: 0.929 - Test loss: 1238.085 - Test Metric: 0.918\n",
      "Ep. 397 - Loss: 2419.230 - Metric: 0.927 - Test loss: 1235.876 - Test Metric: 0.918\n",
      "Ep. 398 - Loss: 2080.819 - Metric: 0.930 - Test loss: 1232.864 - Test Metric: 0.918\n",
      "Ep. 399 - Loss: 3162.182 - Metric: 0.929 - Test loss: 1229.086 - Test Metric: 0.918\n",
      "Ep. 400 - Loss: 3592.865 - Metric: 0.930 - Test loss: 1223.042 - Test Metric: 0.918\n",
      "Ep. 401 - Loss: 1441.329 - Metric: 0.926 - Test loss: 1216.780 - Test Metric: 0.918\n",
      "Ep. 402 - Loss: 2119.215 - Metric: 0.929 - Test loss: 1212.562 - Test Metric: 0.918\n",
      "Ep. 403 - Loss: 2289.346 - Metric: 0.930 - Test loss: 1208.333 - Test Metric: 0.918\n",
      "Ep. 404 - Loss: 3280.372 - Metric: 0.930 - Test loss: 1222.940 - Test Metric: 0.918\n",
      "Ep. 405 - Loss: 2469.380 - Metric: 0.929 - Test loss: 1226.866 - Test Metric: 0.918\n",
      "Ep. 406 - Loss: 1950.145 - Metric: 0.929 - Test loss: 1227.114 - Test Metric: 0.918\n",
      "Ep. 407 - Loss: 1848.297 - Metric: 0.929 - Test loss: 1228.383 - Test Metric: 0.918\n",
      "Ep. 408 - Loss: 3772.242 - Metric: 0.928 - Test loss: 1226.430 - Test Metric: 0.918\n",
      "Ep. 409 - Loss: 1787.890 - Metric: 0.930 - Test loss: 1222.232 - Test Metric: 0.918\n",
      "Ep. 410 - Loss: 2147.271 - Metric: 0.930 - Test loss: 1217.360 - Test Metric: 0.918\n",
      "Ep. 411 - Loss: 1373.413 - Metric: 0.930 - Test loss: 1213.035 - Test Metric: 0.918\n",
      "Ep. 412 - Loss: 3003.696 - Metric: 0.930 - Test loss: 1208.052 - Test Metric: 0.918\n",
      "Ep. 413 - Loss: 2194.473 - Metric: 0.929 - Test loss: 1201.574 - Test Metric: 0.918\n",
      "Ep. 414 - Loss: 1231.905 - Metric: 0.930 - Test loss: 1197.335 - Test Metric: 0.918\n",
      "Ep. 415 - Loss: 1804.173 - Metric: 0.930 - Test loss: 1194.026 - Test Metric: 0.918\n",
      "Ep. 416 - Loss: 1664.286 - Metric: 0.930 - Test loss: 1190.956 - Test Metric: 0.918\n",
      "Ep. 417 - Loss: 1990.060 - Metric: 0.929 - Test loss: 1187.676 - Test Metric: 0.918\n",
      "Ep. 418 - Loss: 4098.242 - Metric: 0.929 - Test loss: 1183.318 - Test Metric: 0.918\n",
      "Ep. 419 - Loss: 2734.735 - Metric: 0.930 - Test loss: 1178.575 - Test Metric: 0.918\n",
      "Ep. 420 - Loss: 1447.030 - Metric: 0.930 - Test loss: 1174.999 - Test Metric: 0.918\n",
      "Ep. 421 - Loss: 2274.486 - Metric: 0.930 - Test loss: 1171.065 - Test Metric: 0.918\n",
      "Ep. 422 - Loss: 2730.462 - Metric: 0.929 - Test loss: 1168.281 - Test Metric: 0.918\n",
      "Ep. 423 - Loss: 1427.252 - Metric: 0.927 - Test loss: 1164.725 - Test Metric: 0.918\n",
      "Ep. 424 - Loss: 2841.323 - Metric: 0.929 - Test loss: 1160.057 - Test Metric: 0.918\n",
      "Ep. 425 - Loss: 1608.573 - Metric: 0.895 - Test loss: 1158.117 - Test Metric: 0.918\n",
      "Ep. 426 - Loss: 2243.699 - Metric: 0.930 - Test loss: 1156.696 - Test Metric: 0.918\n",
      "Ep. 427 - Loss: 2772.996 - Metric: 0.930 - Test loss: 1153.187 - Test Metric: 0.918\n",
      "Ep. 428 - Loss: 2600.886 - Metric: 0.930 - Test loss: 1150.615 - Test Metric: 0.918\n",
      "Ep. 429 - Loss: 1813.376 - Metric: 0.930 - Test loss: 1148.062 - Test Metric: 0.918\n",
      "Ep. 430 - Loss: 3801.397 - Metric: 0.929 - Test loss: 1145.278 - Test Metric: 0.918\n",
      "Ep. 431 - Loss: 3657.873 - Metric: 0.930 - Test loss: 1138.896 - Test Metric: 0.918\n",
      "Ep. 432 - Loss: 2183.104 - Metric: 0.930 - Test loss: 1131.720 - Test Metric: 0.918\n",
      "Ep. 433 - Loss: 1893.270 - Metric: 0.929 - Test loss: 1125.786 - Test Metric: 0.918\n",
      "Ep. 434 - Loss: 1699.443 - Metric: 0.930 - Test loss: 1121.483 - Test Metric: 0.918\n",
      "Ep. 435 - Loss: 1887.400 - Metric: 0.927 - Test loss: 1117.649 - Test Metric: 0.918\n",
      "Ep. 436 - Loss: 2149.778 - Metric: 0.930 - Test loss: 1113.438 - Test Metric: 0.918\n",
      "Ep. 437 - Loss: 2059.271 - Metric: 0.930 - Test loss: 1108.999 - Test Metric: 0.918\n",
      "Ep. 438 - Loss: 1667.259 - Metric: 0.929 - Test loss: 1105.283 - Test Metric: 0.918\n",
      "Ep. 439 - Loss: 2615.252 - Metric: 0.929 - Test loss: 1101.365 - Test Metric: 0.918\n",
      "Ep. 440 - Loss: 1655.287 - Metric: 0.833 - Test loss: 1096.669 - Test Metric: 0.918\n",
      "Ep. 441 - Loss: 1969.433 - Metric: 0.928 - Test loss: 1094.047 - Test Metric: 0.918\n",
      "Ep. 442 - Loss: 2140.313 - Metric: 0.929 - Test loss: 1090.089 - Test Metric: 0.918\n",
      "Ep. 443 - Loss: 2205.134 - Metric: 0.853 - Test loss: 1085.003 - Test Metric: 0.918\n",
      "Ep. 444 - Loss: 4190.991 - Metric: 0.928 - Test loss: 1079.861 - Test Metric: 0.918\n",
      "Ep. 445 - Loss: 2368.583 - Metric: 0.930 - Test loss: 1074.536 - Test Metric: 0.918\n",
      "Ep. 446 - Loss: 2437.957 - Metric: 0.929 - Test loss: 1071.107 - Test Metric: 0.918\n",
      "Ep. 447 - Loss: 3160.088 - Metric: 0.929 - Test loss: 1065.522 - Test Metric: 0.918\n",
      "Ep. 448 - Loss: 1355.454 - Metric: 0.929 - Test loss: 1060.007 - Test Metric: 0.918\n",
      "Ep. 449 - Loss: 2021.805 - Metric: 0.930 - Test loss: 1055.166 - Test Metric: 0.918\n",
      "Ep. 450 - Loss: 1436.166 - Metric: 0.929 - Test loss: 1051.163 - Test Metric: 0.918\n",
      "Ep. 451 - Loss: 1879.397 - Metric: 0.929 - Test loss: 1047.098 - Test Metric: 0.918\n",
      "Ep. 452 - Loss: 1908.097 - Metric: 0.927 - Test loss: 1042.386 - Test Metric: 0.918\n",
      "Ep. 453 - Loss: 5864.971 - Metric: 0.929 - Test loss: 1040.515 - Test Metric: 0.918\n",
      "Ep. 454 - Loss: 1674.385 - Metric: 0.836 - Test loss: 1038.809 - Test Metric: 0.918\n",
      "Ep. 455 - Loss: 2124.885 - Metric: 0.930 - Test loss: 1036.793 - Test Metric: 0.918\n",
      "Ep. 456 - Loss: 1693.932 - Metric: 0.929 - Test loss: 1033.117 - Test Metric: 0.918\n",
      "Ep. 457 - Loss: 1587.549 - Metric: 0.929 - Test loss: 1028.987 - Test Metric: 0.918\n",
      "Ep. 458 - Loss: 1848.251 - Metric: 0.929 - Test loss: 1025.427 - Test Metric: 0.918\n",
      "Ep. 459 - Loss: 1866.023 - Metric: 0.930 - Test loss: 1021.961 - Test Metric: 0.918\n",
      "Ep. 460 - Loss: 2739.404 - Metric: 0.930 - Test loss: 1018.422 - Test Metric: 0.918\n",
      "Ep. 461 - Loss: 2825.002 - Metric: 0.929 - Test loss: 1015.844 - Test Metric: 0.918\n",
      "Ep. 462 - Loss: 2120.587 - Metric: 0.924 - Test loss: 1013.591 - Test Metric: 0.918\n",
      "Ep. 463 - Loss: 1527.177 - Metric: 0.862 - Test loss: 1010.672 - Test Metric: 0.918\n",
      "Ep. 464 - Loss: 1940.818 - Metric: 0.833 - Test loss: 1007.694 - Test Metric: 0.918\n",
      "Ep. 465 - Loss: 887.428 - Metric: 0.929 - Test loss: 1018.021 - Test Metric: 0.918\n",
      "Ep. 466 - Loss: 1941.468 - Metric: 0.929 - Test loss: 1020.337 - Test Metric: 0.918\n",
      "Ep. 467 - Loss: 1373.536 - Metric: 0.928 - Test loss: 1018.801 - Test Metric: 0.918\n",
      "Ep. 468 - Loss: 1177.126 - Metric: 0.930 - Test loss: 1015.208 - Test Metric: 0.918\n",
      "Ep. 469 - Loss: 979.866 - Metric: 0.929 - Test loss: 1011.796 - Test Metric: 0.918\n",
      "Ep. 470 - Loss: 2123.438 - Metric: 0.929 - Test loss: 1008.299 - Test Metric: 0.918\n",
      "Ep. 471 - Loss: 1699.535 - Metric: 0.929 - Test loss: 1004.442 - Test Metric: 0.918\n",
      "Ep. 472 - Loss: 2839.465 - Metric: 0.929 - Test loss: 1006.752 - Test Metric: 0.918\n",
      "Ep. 473 - Loss: 2335.373 - Metric: 0.929 - Test loss: 1003.455 - Test Metric: 0.918\n",
      "Ep. 474 - Loss: 1986.134 - Metric: 0.842 - Test loss: 998.742 - Test Metric: 0.918\n",
      "Ep. 475 - Loss: 1337.719 - Metric: 0.930 - Test loss: 995.503 - Test Metric: 0.918\n",
      "Ep. 476 - Loss: 975.801 - Metric: 0.930 - Test loss: 993.284 - Test Metric: 0.918\n",
      "Ep. 477 - Loss: 1556.026 - Metric: 0.930 - Test loss: 992.730 - Test Metric: 0.918\n",
      "Ep. 478 - Loss: 1706.719 - Metric: 0.930 - Test loss: 989.598 - Test Metric: 0.918\n",
      "Ep. 479 - Loss: 2618.559 - Metric: 0.930 - Test loss: 983.636 - Test Metric: 0.918\n",
      "Ep. 480 - Loss: 2214.573 - Metric: 0.929 - Test loss: 977.207 - Test Metric: 0.918\n",
      "Ep. 481 - Loss: 1154.065 - Metric: 0.929 - Test loss: 973.912 - Test Metric: 0.918\n",
      "Ep. 482 - Loss: 2042.453 - Metric: 0.834 - Test loss: 969.262 - Test Metric: 0.918\n",
      "Ep. 483 - Loss: 1973.220 - Metric: 0.930 - Test loss: 966.667 - Test Metric: 0.918\n",
      "Ep. 484 - Loss: 2651.742 - Metric: 0.929 - Test loss: 963.145 - Test Metric: 0.918\n",
      "Ep. 485 - Loss: 2925.348 - Metric: 0.930 - Test loss: 958.615 - Test Metric: 0.918\n",
      "Ep. 486 - Loss: 8271.197 - Metric: 0.914 - Test loss: 954.280 - Test Metric: 0.918\n",
      "Ep. 487 - Loss: 2378.243 - Metric: 0.930 - Test loss: 949.574 - Test Metric: 0.918\n",
      "Ep. 488 - Loss: 869.304 - Metric: 0.929 - Test loss: 944.677 - Test Metric: 0.918\n",
      "Ep. 489 - Loss: 1894.182 - Metric: 0.835 - Test loss: 944.226 - Test Metric: 0.918\n",
      "Ep. 490 - Loss: 1972.307 - Metric: 0.928 - Test loss: 943.591 - Test Metric: 0.918\n",
      "Ep. 491 - Loss: 1587.106 - Metric: 0.930 - Test loss: 939.272 - Test Metric: 0.918\n",
      "Ep. 492 - Loss: 1473.812 - Metric: 0.930 - Test loss: 934.149 - Test Metric: 0.918\n",
      "Ep. 493 - Loss: 1575.932 - Metric: 0.930 - Test loss: 928.982 - Test Metric: 0.918\n",
      "Ep. 494 - Loss: 1288.886 - Metric: 0.925 - Test loss: 924.107 - Test Metric: 0.918\n",
      "Ep. 495 - Loss: 2512.804 - Metric: 0.930 - Test loss: 917.185 - Test Metric: 0.918\n",
      "Ep. 496 - Loss: 1503.879 - Metric: 0.929 - Test loss: 910.192 - Test Metric: 0.918\n",
      "Ep. 497 - Loss: 1695.685 - Metric: 0.929 - Test loss: 905.867 - Test Metric: 0.918\n",
      "Ep. 498 - Loss: 1877.373 - Metric: 0.930 - Test loss: 901.659 - Test Metric: 0.918\n",
      "Ep. 499 - Loss: 1351.945 - Metric: 0.929 - Test loss: 896.132 - Test Metric: 0.918\n",
      "Ep. 500 - Loss: 1456.656 - Metric: 0.852 - Test loss: 891.136 - Test Metric: 0.918\n",
      "Ep. 501 - Loss: 1822.260 - Metric: 0.929 - Test loss: 884.841 - Test Metric: 0.918\n",
      "Ep. 502 - Loss: 1189.914 - Metric: 0.929 - Test loss: 878.882 - Test Metric: 0.918\n",
      "Ep. 503 - Loss: 1671.973 - Metric: 0.851 - Test loss: 874.393 - Test Metric: 0.918\n",
      "Ep. 504 - Loss: 2523.640 - Metric: 0.882 - Test loss: 868.540 - Test Metric: 0.918\n",
      "Ep. 505 - Loss: 1841.915 - Metric: 0.927 - Test loss: 866.796 - Test Metric: 0.918\n",
      "Ep. 506 - Loss: 2034.172 - Metric: 0.930 - Test loss: 863.019 - Test Metric: 0.918\n",
      "Ep. 507 - Loss: 2624.779 - Metric: 0.930 - Test loss: 856.449 - Test Metric: 0.918\n",
      "Ep. 508 - Loss: 1855.932 - Metric: 0.929 - Test loss: 848.620 - Test Metric: 0.918\n",
      "Ep. 509 - Loss: 1931.764 - Metric: 0.929 - Test loss: 841.333 - Test Metric: 0.918\n",
      "Ep. 510 - Loss: 1600.653 - Metric: 0.929 - Test loss: 836.939 - Test Metric: 0.918\n",
      "Ep. 511 - Loss: 2101.157 - Metric: 0.851 - Test loss: 830.975 - Test Metric: 0.918\n",
      "Ep. 512 - Loss: 1724.532 - Metric: 0.929 - Test loss: 825.026 - Test Metric: 0.918\n",
      "Ep. 513 - Loss: 1646.076 - Metric: 0.929 - Test loss: 819.653 - Test Metric: 0.918\n",
      "Ep. 514 - Loss: 2155.522 - Metric: 0.929 - Test loss: 814.493 - Test Metric: 0.918\n",
      "Ep. 515 - Loss: 2153.091 - Metric: 0.925 - Test loss: 806.954 - Test Metric: 0.918\n",
      "Ep. 516 - Loss: 1477.118 - Metric: 0.928 - Test loss: 800.019 - Test Metric: 0.918\n",
      "Ep. 517 - Loss: 1971.433 - Metric: 0.929 - Test loss: 793.470 - Test Metric: 0.918\n",
      "Ep. 518 - Loss: 1884.075 - Metric: 0.836 - Test loss: 786.342 - Test Metric: 0.918\n",
      "Ep. 519 - Loss: 712.730 - Metric: 0.929 - Test loss: 781.482 - Test Metric: 0.918\n",
      "Ep. 520 - Loss: 1495.518 - Metric: 0.930 - Test loss: 778.881 - Test Metric: 0.918\n",
      "Ep. 521 - Loss: 1809.074 - Metric: 0.928 - Test loss: 780.694 - Test Metric: 0.918\n",
      "Ep. 522 - Loss: 2535.522 - Metric: 0.930 - Test loss: 775.923 - Test Metric: 0.918\n",
      "Ep. 523 - Loss: 1242.500 - Metric: 0.930 - Test loss: 770.641 - Test Metric: 0.918\n",
      "Ep. 524 - Loss: 4518.182 - Metric: 0.929 - Test loss: 793.176 - Test Metric: 0.918\n",
      "Ep. 525 - Loss: 1096.649 - Metric: 0.928 - Test loss: 799.025 - Test Metric: 0.918\n",
      "Ep. 526 - Loss: 1567.444 - Metric: 0.929 - Test loss: 797.197 - Test Metric: 0.918\n",
      "Ep. 527 - Loss: 993.925 - Metric: 0.929 - Test loss: 793.983 - Test Metric: 0.918\n",
      "Ep. 528 - Loss: 1574.350 - Metric: 0.927 - Test loss: 789.019 - Test Metric: 0.918\n",
      "Ep. 529 - Loss: 1110.496 - Metric: 0.930 - Test loss: 785.464 - Test Metric: 0.918\n",
      "Ep. 530 - Loss: 2418.524 - Metric: 0.925 - Test loss: 778.625 - Test Metric: 0.918\n",
      "Ep. 531 - Loss: 1841.497 - Metric: 0.929 - Test loss: 771.228 - Test Metric: 0.918\n",
      "Ep. 532 - Loss: 1577.887 - Metric: 0.930 - Test loss: 763.994 - Test Metric: 0.918\n",
      "Ep. 533 - Loss: 1247.570 - Metric: 0.929 - Test loss: 757.462 - Test Metric: 0.918\n",
      "Ep. 534 - Loss: 1836.349 - Metric: 0.930 - Test loss: 749.986 - Test Metric: 0.918\n",
      "Ep. 535 - Loss: 1811.599 - Metric: 0.929 - Test loss: 743.665 - Test Metric: 0.918\n",
      "Ep. 536 - Loss: 1352.920 - Metric: 0.928 - Test loss: 736.933 - Test Metric: 0.918\n",
      "Ep. 537 - Loss: 881.436 - Metric: 0.930 - Test loss: 731.762 - Test Metric: 0.918\n",
      "Ep. 538 - Loss: 1950.655 - Metric: 0.929 - Test loss: 725.896 - Test Metric: 0.918\n",
      "Ep. 539 - Loss: 3321.747 - Metric: 0.929 - Test loss: 736.607 - Test Metric: 0.918\n",
      "Ep. 540 - Loss: 1176.205 - Metric: 0.929 - Test loss: 737.438 - Test Metric: 0.918\n",
      "Ep. 541 - Loss: 1475.339 - Metric: 0.930 - Test loss: 733.894 - Test Metric: 0.918\n",
      "Ep. 542 - Loss: 1055.317 - Metric: 0.929 - Test loss: 729.060 - Test Metric: 0.918\n",
      "Ep. 543 - Loss: 1702.339 - Metric: 0.929 - Test loss: 724.779 - Test Metric: 0.918\n",
      "Ep. 544 - Loss: 1564.042 - Metric: 0.929 - Test loss: 722.271 - Test Metric: 0.918\n",
      "Ep. 545 - Loss: 1957.540 - Metric: 0.927 - Test loss: 716.038 - Test Metric: 0.918\n",
      "Ep. 546 - Loss: 1997.302 - Metric: 0.927 - Test loss: 708.889 - Test Metric: 0.918\n",
      "Ep. 547 - Loss: 1428.467 - Metric: 0.930 - Test loss: 701.436 - Test Metric: 0.918\n",
      "Ep. 548 - Loss: 952.567 - Metric: 0.929 - Test loss: 695.491 - Test Metric: 0.918\n",
      "Ep. 549 - Loss: 1347.295 - Metric: 0.930 - Test loss: 690.220 - Test Metric: 0.918\n",
      "Ep. 550 - Loss: 1615.512 - Metric: 0.930 - Test loss: 683.531 - Test Metric: 0.918\n",
      "Ep. 551 - Loss: 352.271 - Metric: 0.929 - Test loss: 678.330 - Test Metric: 0.918\n",
      "Ep. 552 - Loss: 2122.181 - Metric: 0.928 - Test loss: 674.045 - Test Metric: 0.918\n",
      "Ep. 553 - Loss: 1833.265 - Metric: 0.929 - Test loss: 669.290 - Test Metric: 0.918\n",
      "Ep. 554 - Loss: 1175.625 - Metric: 0.918 - Test loss: 663.726 - Test Metric: 0.918\n",
      "Ep. 555 - Loss: 1065.597 - Metric: 0.930 - Test loss: 658.135 - Test Metric: 0.918\n",
      "Ep. 556 - Loss: 442.836 - Metric: 0.930 - Test loss: 653.873 - Test Metric: 0.918\n",
      "Ep. 557 - Loss: 838.684 - Metric: 0.929 - Test loss: 649.049 - Test Metric: 0.918\n",
      "Ep. 558 - Loss: 1171.785 - Metric: 0.930 - Test loss: 644.785 - Test Metric: 0.918\n",
      "Ep. 559 - Loss: 932.199 - Metric: 0.930 - Test loss: 640.103 - Test Metric: 0.918\n",
      "Ep. 560 - Loss: 585.963 - Metric: 0.929 - Test loss: 636.899 - Test Metric: 0.918\n",
      "Ep. 561 - Loss: 1518.142 - Metric: 0.927 - Test loss: 637.195 - Test Metric: 0.918\n",
      "Ep. 562 - Loss: 749.176 - Metric: 0.930 - Test loss: 636.591 - Test Metric: 0.918\n",
      "Ep. 563 - Loss: 821.554 - Metric: 0.930 - Test loss: 633.911 - Test Metric: 0.918\n",
      "Ep. 564 - Loss: 1215.595 - Metric: 0.852 - Test loss: 629.936 - Test Metric: 0.918\n",
      "Ep. 565 - Loss: 1335.052 - Metric: 0.930 - Test loss: 624.586 - Test Metric: 0.918\n",
      "Ep. 566 - Loss: 1113.886 - Metric: 0.921 - Test loss: 619.266 - Test Metric: 0.918\n",
      "Ep. 567 - Loss: 1228.984 - Metric: 0.929 - Test loss: 615.520 - Test Metric: 0.918\n",
      "Ep. 568 - Loss: 1034.588 - Metric: 0.930 - Test loss: 611.330 - Test Metric: 0.918\n",
      "Ep. 569 - Loss: 3560.089 - Metric: 0.929 - Test loss: 610.778 - Test Metric: 0.918\n",
      "Ep. 570 - Loss: 1191.550 - Metric: 0.930 - Test loss: 606.161 - Test Metric: 0.918\n",
      "Ep. 571 - Loss: 1180.375 - Metric: 0.930 - Test loss: 601.189 - Test Metric: 0.918\n",
      "Ep. 572 - Loss: 787.034 - Metric: 0.930 - Test loss: 596.230 - Test Metric: 0.918\n",
      "Ep. 573 - Loss: 1162.544 - Metric: 0.930 - Test loss: 590.969 - Test Metric: 0.918\n",
      "Ep. 574 - Loss: 1337.696 - Metric: 0.930 - Test loss: 584.310 - Test Metric: 0.918\n",
      "Ep. 575 - Loss: 773.985 - Metric: 0.929 - Test loss: 579.324 - Test Metric: 0.918\n",
      "Ep. 576 - Loss: 1498.374 - Metric: 0.929 - Test loss: 575.553 - Test Metric: 0.918\n",
      "Ep. 577 - Loss: 1428.934 - Metric: 0.929 - Test loss: 574.249 - Test Metric: 0.918\n",
      "Ep. 578 - Loss: 1580.734 - Metric: 0.929 - Test loss: 571.948 - Test Metric: 0.918\n",
      "Ep. 579 - Loss: 692.847 - Metric: 0.930 - Test loss: 567.785 - Test Metric: 0.918\n",
      "Ep. 580 - Loss: 1404.383 - Metric: 0.929 - Test loss: 565.107 - Test Metric: 0.918\n",
      "Ep. 581 - Loss: 1299.755 - Metric: 0.929 - Test loss: 560.264 - Test Metric: 0.918\n",
      "Ep. 582 - Loss: 848.141 - Metric: 0.928 - Test loss: 555.407 - Test Metric: 0.918\n",
      "Ep. 583 - Loss: 1082.234 - Metric: 0.928 - Test loss: 550.364 - Test Metric: 0.918\n",
      "Ep. 584 - Loss: 492.081 - Metric: 0.930 - Test loss: 547.120 - Test Metric: 0.918\n",
      "Ep. 585 - Loss: 1003.064 - Metric: 0.929 - Test loss: 543.954 - Test Metric: 0.918\n",
      "Ep. 586 - Loss: 1227.712 - Metric: 0.929 - Test loss: 541.807 - Test Metric: 0.918\n",
      "Ep. 587 - Loss: 993.266 - Metric: 0.929 - Test loss: 538.111 - Test Metric: 0.918\n",
      "Ep. 588 - Loss: 659.579 - Metric: 0.929 - Test loss: 534.414 - Test Metric: 0.918\n",
      "Ep. 589 - Loss: 1129.315 - Metric: 0.930 - Test loss: 530.502 - Test Metric: 0.918\n",
      "Ep. 590 - Loss: 868.499 - Metric: 0.930 - Test loss: 525.696 - Test Metric: 0.918\n",
      "Ep. 591 - Loss: 875.942 - Metric: 0.929 - Test loss: 522.610 - Test Metric: 0.918\n",
      "Ep. 592 - Loss: 1109.950 - Metric: 0.929 - Test loss: 522.248 - Test Metric: 0.918\n",
      "Ep. 593 - Loss: 523.727 - Metric: 0.929 - Test loss: 520.907 - Test Metric: 0.918\n",
      "Ep. 594 - Loss: 475.697 - Metric: 0.929 - Test loss: 519.461 - Test Metric: 0.918\n",
      "Ep. 595 - Loss: 9047.266 - Metric: 0.851 - Test loss: 557.448 - Test Metric: 0.918\n",
      "Ep. 596 - Loss: 1187.036 - Metric: 0.920 - Test loss: 569.405 - Test Metric: 0.918\n",
      "Ep. 597 - Loss: 772.247 - Metric: 0.930 - Test loss: 571.117 - Test Metric: 0.918\n",
      "Ep. 598 - Loss: 833.065 - Metric: 0.930 - Test loss: 569.675 - Test Metric: 0.918\n",
      "Ep. 599 - Loss: 1279.760 - Metric: 0.930 - Test loss: 565.969 - Test Metric: 0.918\n",
      "Ep. 600 - Loss: 885.588 - Metric: 0.930 - Test loss: 560.323 - Test Metric: 0.918\n",
      "Ep. 601 - Loss: 1041.622 - Metric: 0.929 - Test loss: 555.749 - Test Metric: 0.918\n",
      "Ep. 602 - Loss: 1160.292 - Metric: 0.930 - Test loss: 553.258 - Test Metric: 0.918\n",
      "Ep. 603 - Loss: 890.795 - Metric: 0.930 - Test loss: 549.558 - Test Metric: 0.918\n",
      "Ep. 604 - Loss: 1022.795 - Metric: 0.930 - Test loss: 545.153 - Test Metric: 0.918\n",
      "Ep. 605 - Loss: 444.929 - Metric: 0.929 - Test loss: 541.417 - Test Metric: 0.918\n",
      "Ep. 606 - Loss: 324.198 - Metric: 0.930 - Test loss: 538.732 - Test Metric: 0.918\n",
      "Ep. 607 - Loss: 943.853 - Metric: 0.930 - Test loss: 535.242 - Test Metric: 0.918\n",
      "Ep. 608 - Loss: 948.255 - Metric: 0.851 - Test loss: 530.935 - Test Metric: 0.918\n",
      "Ep. 609 - Loss: 655.537 - Metric: 0.928 - Test loss: 526.844 - Test Metric: 0.918\n",
      "Ep. 610 - Loss: 788.281 - Metric: 0.896 - Test loss: 522.984 - Test Metric: 0.918\n",
      "Ep. 611 - Loss: 737.344 - Metric: 0.930 - Test loss: 519.181 - Test Metric: 0.918\n",
      "Ep. 612 - Loss: 553.641 - Metric: 0.930 - Test loss: 515.539 - Test Metric: 0.918\n",
      "Ep. 613 - Loss: 840.012 - Metric: 0.928 - Test loss: 511.425 - Test Metric: 0.918\n",
      "Ep. 614 - Loss: 447.883 - Metric: 0.930 - Test loss: 508.107 - Test Metric: 0.918\n",
      "Ep. 615 - Loss: 962.950 - Metric: 0.844 - Test loss: 504.329 - Test Metric: 0.918\n",
      "Ep. 616 - Loss: 1101.038 - Metric: 0.930 - Test loss: 500.137 - Test Metric: 0.918\n",
      "Ep. 617 - Loss: 786.017 - Metric: 0.929 - Test loss: 496.016 - Test Metric: 0.918\n",
      "Ep. 618 - Loss: 991.158 - Metric: 0.929 - Test loss: 491.315 - Test Metric: 0.918\n",
      "Ep. 619 - Loss: 951.644 - Metric: 0.929 - Test loss: 486.078 - Test Metric: 0.918\n",
      "Ep. 620 - Loss: 926.968 - Metric: 0.930 - Test loss: 481.812 - Test Metric: 0.918\n",
      "Ep. 621 - Loss: 831.571 - Metric: 0.922 - Test loss: 477.937 - Test Metric: 0.918\n",
      "Ep. 622 - Loss: 650.639 - Metric: 0.930 - Test loss: 473.903 - Test Metric: 0.918\n",
      "Ep. 623 - Loss: 1011.293 - Metric: 0.930 - Test loss: 469.170 - Test Metric: 0.918\n",
      "Ep. 624 - Loss: 1217.921 - Metric: 0.929 - Test loss: 463.906 - Test Metric: 0.918\n",
      "Ep. 625 - Loss: 907.659 - Metric: 0.930 - Test loss: 458.909 - Test Metric: 0.918\n",
      "Ep. 626 - Loss: 1091.196 - Metric: 0.930 - Test loss: 453.765 - Test Metric: 0.918\n",
      "Ep. 627 - Loss: 696.608 - Metric: 0.925 - Test loss: 448.283 - Test Metric: 0.918\n",
      "Ep. 628 - Loss: 471.452 - Metric: 0.930 - Test loss: 444.030 - Test Metric: 0.918\n",
      "Ep. 629 - Loss: 1315.566 - Metric: 0.930 - Test loss: 438.227 - Test Metric: 0.918\n",
      "Ep. 630 - Loss: 385.971 - Metric: 0.930 - Test loss: 433.773 - Test Metric: 0.918\n",
      "Ep. 631 - Loss: 1025.081 - Metric: 0.930 - Test loss: 429.387 - Test Metric: 0.918\n",
      "Ep. 632 - Loss: 979.083 - Metric: 0.930 - Test loss: 424.166 - Test Metric: 0.918\n",
      "Ep. 633 - Loss: 492.243 - Metric: 0.930 - Test loss: 419.520 - Test Metric: 0.918\n",
      "Ep. 634 - Loss: 827.423 - Metric: 0.930 - Test loss: 414.610 - Test Metric: 0.918\n",
      "Ep. 635 - Loss: 664.532 - Metric: 0.930 - Test loss: 410.193 - Test Metric: 0.918\n",
      "Ep. 636 - Loss: 780.295 - Metric: 0.929 - Test loss: 405.484 - Test Metric: 0.918\n",
      "Ep. 637 - Loss: 768.261 - Metric: 0.929 - Test loss: 399.688 - Test Metric: 0.918\n",
      "Ep. 638 - Loss: 576.867 - Metric: 0.930 - Test loss: 395.012 - Test Metric: 0.918\n",
      "Ep. 639 - Loss: 469.529 - Metric: 0.927 - Test loss: 391.196 - Test Metric: 0.918\n",
      "Ep. 640 - Loss: 867.872 - Metric: 0.930 - Test loss: 387.892 - Test Metric: 0.918\n",
      "Ep. 641 - Loss: 499.416 - Metric: 0.930 - Test loss: 385.141 - Test Metric: 0.918\n",
      "Ep. 642 - Loss: 515.612 - Metric: 0.928 - Test loss: 382.392 - Test Metric: 0.918\n",
      "Ep. 643 - Loss: 755.104 - Metric: 0.832 - Test loss: 378.559 - Test Metric: 0.918\n",
      "Ep. 644 - Loss: 578.102 - Metric: 0.928 - Test loss: 375.100 - Test Metric: 0.918\n",
      "Ep. 645 - Loss: 570.802 - Metric: 0.928 - Test loss: 374.620 - Test Metric: 0.918\n",
      "Ep. 646 - Loss: 1162.655 - Metric: 0.930 - Test loss: 369.950 - Test Metric: 0.918\n",
      "Ep. 647 - Loss: 663.791 - Metric: 0.930 - Test loss: 364.981 - Test Metric: 0.918\n",
      "Ep. 648 - Loss: 582.052 - Metric: 0.930 - Test loss: 361.140 - Test Metric: 0.918\n",
      "Ep. 649 - Loss: 844.372 - Metric: 0.930 - Test loss: 356.403 - Test Metric: 0.918\n",
      "Ep. 650 - Loss: 518.929 - Metric: 0.930 - Test loss: 352.068 - Test Metric: 0.918\n",
      "Ep. 651 - Loss: 548.964 - Metric: 0.930 - Test loss: 346.954 - Test Metric: 0.918\n",
      "Ep. 652 - Loss: 719.937 - Metric: 0.929 - Test loss: 342.286 - Test Metric: 0.918\n",
      "Ep. 653 - Loss: 364.627 - Metric: 0.930 - Test loss: 338.040 - Test Metric: 0.918\n",
      "Ep. 654 - Loss: 5857.942 - Metric: 0.851 - Test loss: 340.873 - Test Metric: 0.918\n",
      "Ep. 655 - Loss: 541.319 - Metric: 0.929 - Test loss: 338.898 - Test Metric: 0.918\n",
      "Ep. 656 - Loss: 597.037 - Metric: 0.930 - Test loss: 335.623 - Test Metric: 0.918\n",
      "Ep. 657 - Loss: 488.452 - Metric: 0.929 - Test loss: 331.857 - Test Metric: 0.918\n",
      "Ep. 658 - Loss: 842.641 - Metric: 0.930 - Test loss: 326.958 - Test Metric: 0.918\n",
      "Ep. 659 - Loss: 648.861 - Metric: 0.929 - Test loss: 321.663 - Test Metric: 0.918\n",
      "Ep. 660 - Loss: 796.237 - Metric: 0.930 - Test loss: 315.996 - Test Metric: 0.918\n",
      "Ep. 661 - Loss: 628.752 - Metric: 0.929 - Test loss: 311.072 - Test Metric: 0.918\n",
      "Ep. 662 - Loss: 587.334 - Metric: 0.930 - Test loss: 306.421 - Test Metric: 0.918\n",
      "Ep. 663 - Loss: 706.502 - Metric: 0.929 - Test loss: 301.132 - Test Metric: 0.918\n",
      "Ep. 664 - Loss: 682.796 - Metric: 0.929 - Test loss: 296.583 - Test Metric: 0.918\n",
      "Ep. 665 - Loss: 670.711 - Metric: 0.930 - Test loss: 291.187 - Test Metric: 0.918\n",
      "Ep. 666 - Loss: 615.620 - Metric: 0.930 - Test loss: 285.392 - Test Metric: 0.918\n",
      "Ep. 667 - Loss: 653.962 - Metric: 0.929 - Test loss: 286.953 - Test Metric: 0.918\n",
      "Ep. 668 - Loss: 167.671 - Metric: 0.930 - Test loss: 285.694 - Test Metric: 0.918\n",
      "Ep. 669 - Loss: 464.439 - Metric: 0.926 - Test loss: 282.771 - Test Metric: 0.918\n",
      "Ep. 670 - Loss: 487.281 - Metric: 0.930 - Test loss: 279.050 - Test Metric: 0.918\n",
      "Ep. 671 - Loss: 635.353 - Metric: 0.930 - Test loss: 275.267 - Test Metric: 0.918\n",
      "Ep. 672 - Loss: 408.943 - Metric: 0.930 - Test loss: 271.396 - Test Metric: 0.918\n",
      "Ep. 673 - Loss: 577.737 - Metric: 0.930 - Test loss: 267.116 - Test Metric: 0.918\n",
      "Ep. 674 - Loss: 591.691 - Metric: 0.929 - Test loss: 262.981 - Test Metric: 0.918\n",
      "Ep. 675 - Loss: 579.530 - Metric: 0.930 - Test loss: 258.621 - Test Metric: 0.918\n",
      "Ep. 676 - Loss: 324.628 - Metric: 0.930 - Test loss: 254.315 - Test Metric: 0.918\n",
      "Ep. 677 - Loss: 635.276 - Metric: 0.921 - Test loss: 251.826 - Test Metric: 0.918\n",
      "Ep. 678 - Loss: 323.649 - Metric: 0.930 - Test loss: 249.534 - Test Metric: 0.918\n",
      "Ep. 679 - Loss: 556.002 - Metric: 0.930 - Test loss: 246.222 - Test Metric: 0.918\n",
      "Ep. 680 - Loss: 786.732 - Metric: 0.930 - Test loss: 240.681 - Test Metric: 0.918\n",
      "Ep. 681 - Loss: 535.154 - Metric: 0.928 - Test loss: 234.331 - Test Metric: 0.918\n",
      "Ep. 682 - Loss: 333.371 - Metric: 0.929 - Test loss: 229.002 - Test Metric: 0.918\n",
      "Ep. 683 - Loss: 574.251 - Metric: 0.929 - Test loss: 243.084 - Test Metric: 0.918\n",
      "Ep. 684 - Loss: 516.945 - Metric: 0.930 - Test loss: 246.224 - Test Metric: 0.918\n",
      "Ep. 685 - Loss: 326.709 - Metric: 0.930 - Test loss: 245.059 - Test Metric: 0.918\n",
      "Ep. 686 - Loss: 531.941 - Metric: 0.922 - Test loss: 242.171 - Test Metric: 0.918\n",
      "Ep. 687 - Loss: 701.361 - Metric: 0.929 - Test loss: 239.961 - Test Metric: 0.918\n",
      "Ep. 688 - Loss: 285.245 - Metric: 0.930 - Test loss: 237.760 - Test Metric: 0.918\n",
      "Ep. 689 - Loss: 482.523 - Metric: 0.929 - Test loss: 235.050 - Test Metric: 0.918\n",
      "Ep. 690 - Loss: 300.216 - Metric: 0.930 - Test loss: 233.124 - Test Metric: 0.918\n",
      "Ep. 691 - Loss: 374.397 - Metric: 0.921 - Test loss: 231.606 - Test Metric: 0.918\n",
      "Ep. 692 - Loss: 326.913 - Metric: 0.851 - Test loss: 228.455 - Test Metric: 0.918\n",
      "Ep. 693 - Loss: 440.990 - Metric: 0.930 - Test loss: 225.411 - Test Metric: 0.918\n",
      "Ep. 694 - Loss: 308.444 - Metric: 0.881 - Test loss: 222.314 - Test Metric: 0.918\n",
      "Ep. 695 - Loss: 461.640 - Metric: 0.925 - Test loss: 218.389 - Test Metric: 0.918\n",
      "Ep. 696 - Loss: 512.183 - Metric: 0.930 - Test loss: 213.801 - Test Metric: 0.918\n",
      "Ep. 697 - Loss: 412.174 - Metric: 0.922 - Test loss: 208.969 - Test Metric: 0.918\n",
      "Ep. 698 - Loss: 569.996 - Metric: 0.928 - Test loss: 203.262 - Test Metric: 0.918\n",
      "Ep. 699 - Loss: 308.241 - Metric: 0.930 - Test loss: 198.983 - Test Metric: 0.918\n",
      "Ep. 700 - Loss: 318.488 - Metric: 0.930 - Test loss: 194.891 - Test Metric: 0.918\n",
      "Ep. 701 - Loss: 501.467 - Metric: 0.928 - Test loss: 190.411 - Test Metric: 0.918\n",
      "Ep. 702 - Loss: 376.159 - Metric: 0.928 - Test loss: 186.208 - Test Metric: 0.918\n",
      "Ep. 703 - Loss: 390.921 - Metric: 0.924 - Test loss: 181.893 - Test Metric: 0.918\n",
      "Ep. 704 - Loss: 215.866 - Metric: 0.930 - Test loss: 181.555 - Test Metric: 0.918\n",
      "Ep. 705 - Loss: 215.314 - Metric: 0.930 - Test loss: 180.303 - Test Metric: 0.918\n",
      "Ep. 706 - Loss: 305.325 - Metric: 0.927 - Test loss: 178.539 - Test Metric: 0.918\n",
      "Ep. 707 - Loss: 141.751 - Metric: 0.929 - Test loss: 176.722 - Test Metric: 0.918\n",
      "Ep. 708 - Loss: 478.410 - Metric: 0.837 - Test loss: 174.302 - Test Metric: 0.918\n",
      "Ep. 709 - Loss: 343.237 - Metric: 0.930 - Test loss: 170.592 - Test Metric: 0.918\n",
      "Ep. 710 - Loss: 293.243 - Metric: 0.833 - Test loss: 168.775 - Test Metric: 0.918\n",
      "Ep. 711 - Loss: 344.306 - Metric: 0.929 - Test loss: 165.827 - Test Metric: 0.918\n",
      "Ep. 712 - Loss: 272.196 - Metric: 0.930 - Test loss: 161.722 - Test Metric: 0.918\n",
      "Ep. 713 - Loss: 236.485 - Metric: 0.929 - Test loss: 157.892 - Test Metric: 0.918\n",
      "Ep. 714 - Loss: 345.930 - Metric: 0.930 - Test loss: 154.704 - Test Metric: 0.918\n",
      "Ep. 715 - Loss: 340.348 - Metric: 0.921 - Test loss: 153.069 - Test Metric: 0.918\n",
      "Ep. 716 - Loss: 322.087 - Metric: 0.930 - Test loss: 150.634 - Test Metric: 0.918\n",
      "Ep. 717 - Loss: 662.546 - Metric: 0.927 - Test loss: 148.796 - Test Metric: 0.918\n",
      "Ep. 718 - Loss: 286.689 - Metric: 0.929 - Test loss: 145.717 - Test Metric: 0.918\n",
      "Ep. 719 - Loss: 207.461 - Metric: 0.930 - Test loss: 141.902 - Test Metric: 0.918\n",
      "Ep. 720 - Loss: 242.525 - Metric: 0.929 - Test loss: 138.434 - Test Metric: 0.918\n",
      "Ep. 721 - Loss: 201.269 - Metric: 0.930 - Test loss: 135.258 - Test Metric: 0.918\n",
      "Ep. 722 - Loss: 287.041 - Metric: 0.930 - Test loss: 132.974 - Test Metric: 0.918\n",
      "Ep. 723 - Loss: 213.030 - Metric: 0.862 - Test loss: 130.014 - Test Metric: 0.918\n",
      "Ep. 724 - Loss: 327.621 - Metric: 0.925 - Test loss: 134.334 - Test Metric: 0.918\n",
      "Ep. 725 - Loss: 202.506 - Metric: 0.930 - Test loss: 135.463 - Test Metric: 0.918\n",
      "Ep. 726 - Loss: 147.192 - Metric: 0.929 - Test loss: 134.278 - Test Metric: 0.918\n",
      "Ep. 727 - Loss: 145.262 - Metric: 0.929 - Test loss: 131.994 - Test Metric: 0.918\n",
      "Ep. 728 - Loss: 279.054 - Metric: 0.929 - Test loss: 128.326 - Test Metric: 0.918\n",
      "Ep. 729 - Loss: 182.234 - Metric: 0.930 - Test loss: 124.896 - Test Metric: 0.918\n",
      "Ep. 730 - Loss: 217.412 - Metric: 0.929 - Test loss: 121.378 - Test Metric: 0.918\n",
      "Ep. 731 - Loss: 255.823 - Metric: 0.930 - Test loss: 118.281 - Test Metric: 0.918\n",
      "Ep. 732 - Loss: 171.635 - Metric: 0.930 - Test loss: 114.058 - Test Metric: 0.918\n",
      "Ep. 733 - Loss: 138.833 - Metric: 0.929 - Test loss: 110.789 - Test Metric: 0.918\n",
      "Ep. 734 - Loss: 287.505 - Metric: 0.930 - Test loss: 108.535 - Test Metric: 0.918\n",
      "Ep. 735 - Loss: 169.684 - Metric: 0.929 - Test loss: 106.125 - Test Metric: 0.918\n",
      "Ep. 736 - Loss: 182.777 - Metric: 0.930 - Test loss: 102.663 - Test Metric: 0.918\n",
      "Ep. 737 - Loss: 230.598 - Metric: 0.930 - Test loss: 98.845 - Test Metric: 0.918\n",
      "Ep. 738 - Loss: 235.762 - Metric: 0.852 - Test loss: 96.196 - Test Metric: 0.918\n",
      "Ep. 739 - Loss: 147.070 - Metric: 0.928 - Test loss: 93.021 - Test Metric: 0.918\n",
      "Ep. 740 - Loss: 123.961 - Metric: 0.929 - Test loss: 91.869 - Test Metric: 0.918\n",
      "Ep. 741 - Loss: 188.037 - Metric: 0.930 - Test loss: 91.537 - Test Metric: 0.918\n",
      "Ep. 742 - Loss: 165.587 - Metric: 0.929 - Test loss: 93.039 - Test Metric: 0.918\n",
      "Ep. 743 - Loss: 158.748 - Metric: 0.930 - Test loss: 92.950 - Test Metric: 0.918\n",
      "Ep. 744 - Loss: 138.720 - Metric: 0.929 - Test loss: 90.647 - Test Metric: 0.918\n",
      "Ep. 745 - Loss: 183.856 - Metric: 0.929 - Test loss: 88.697 - Test Metric: 0.918\n",
      "Ep. 746 - Loss: 120.531 - Metric: 0.930 - Test loss: 85.316 - Test Metric: 0.918\n",
      "Ep. 747 - Loss: 191.697 - Metric: 0.930 - Test loss: 85.101 - Test Metric: 0.918\n",
      "Ep. 748 - Loss: 267.687 - Metric: 0.929 - Test loss: 90.574 - Test Metric: 0.918\n",
      "Ep. 749 - Loss: 164.470 - Metric: 0.929 - Test loss: 88.346 - Test Metric: 0.918\n",
      "Ep. 750 - Loss: 108.151 - Metric: 0.881 - Test loss: 86.630 - Test Metric: 0.918\n",
      "Ep. 751 - Loss: 147.876 - Metric: 0.930 - Test loss: 84.393 - Test Metric: 0.918\n",
      "Ep. 752 - Loss: 179.146 - Metric: 0.929 - Test loss: 80.624 - Test Metric: 0.918\n",
      "Ep. 753 - Loss: 142.496 - Metric: 0.930 - Test loss: 76.466 - Test Metric: 0.918\n",
      "Ep. 754 - Loss: 149.361 - Metric: 0.930 - Test loss: 72.566 - Test Metric: 0.918\n",
      "Ep. 755 - Loss: 133.367 - Metric: 0.930 - Test loss: 72.075 - Test Metric: 0.918\n",
      "Ep. 756 - Loss: 132.270 - Metric: 0.930 - Test loss: 69.962 - Test Metric: 0.918\n",
      "Ep. 757 - Loss: 144.267 - Metric: 0.930 - Test loss: 67.892 - Test Metric: 0.918\n",
      "Ep. 758 - Loss: 121.863 - Metric: 0.882 - Test loss: 64.680 - Test Metric: 0.918\n",
      "Ep. 759 - Loss: 192.410 - Metric: 0.928 - Test loss: 63.031 - Test Metric: 0.918\n",
      "Ep. 760 - Loss: 126.623 - Metric: 0.930 - Test loss: 62.947 - Test Metric: 0.918\n",
      "Ep. 761 - Loss: 170.065 - Metric: 0.930 - Test loss: 64.130 - Test Metric: 0.918\n",
      "Ep. 762 - Loss: 132.132 - Metric: 0.929 - Test loss: 69.040 - Test Metric: 0.918\n",
      "Ep. 763 - Loss: 156.769 - Metric: 0.929 - Test loss: 68.153 - Test Metric: 0.918\n",
      "Ep. 764 - Loss: 117.283 - Metric: 0.930 - Test loss: 65.348 - Test Metric: 0.918\n",
      "Ep. 765 - Loss: 65.212 - Metric: 0.930 - Test loss: 62.003 - Test Metric: 0.918\n",
      "Ep. 766 - Loss: 126.761 - Metric: 0.834 - Test loss: 58.819 - Test Metric: 0.918\n",
      "Ep. 767 - Loss: 97.999 - Metric: 0.930 - Test loss: 54.869 - Test Metric: 0.918\n",
      "Ep. 768 - Loss: 78.016 - Metric: 0.929 - Test loss: 51.412 - Test Metric: 0.918\n",
      "Ep. 769 - Loss: 102.711 - Metric: 0.930 - Test loss: 48.168 - Test Metric: 0.918\n",
      "Ep. 770 - Loss: 100.943 - Metric: 0.929 - Test loss: 50.110 - Test Metric: 0.918\n",
      "Ep. 771 - Loss: 105.933 - Metric: 0.929 - Test loss: 49.217 - Test Metric: 0.918\n",
      "Ep. 772 - Loss: 106.215 - Metric: 0.930 - Test loss: 45.257 - Test Metric: 0.918\n",
      "Ep. 773 - Loss: 61.561 - Metric: 0.929 - Test loss: 40.946 - Test Metric: 0.918\n",
      "Ep. 774 - Loss: 102.278 - Metric: 0.930 - Test loss: 37.568 - Test Metric: 0.918\n",
      "Ep. 775 - Loss: 62.488 - Metric: 0.930 - Test loss: 35.364 - Test Metric: 0.918\n",
      "Ep. 776 - Loss: 51.883 - Metric: 0.930 - Test loss: 35.161 - Test Metric: 0.918\n",
      "Ep. 777 - Loss: 71.817 - Metric: 0.930 - Test loss: 33.477 - Test Metric: 0.918\n",
      "Ep. 778 - Loss: 51.717 - Metric: 0.930 - Test loss: 34.519 - Test Metric: 0.918\n",
      "Ep. 779 - Loss: 52.911 - Metric: 0.929 - Test loss: 35.785 - Test Metric: 0.918\n",
      "Ep. 780 - Loss: 59.085 - Metric: 0.930 - Test loss: 33.961 - Test Metric: 0.918\n",
      "Ep. 781 - Loss: 53.901 - Metric: 0.930 - Test loss: 31.402 - Test Metric: 0.918\n",
      "Ep. 782 - Loss: 52.966 - Metric: 0.930 - Test loss: 29.756 - Test Metric: 0.918\n",
      "Ep. 783 - Loss: 491.876 - Metric: 0.929 - Test loss: 28.411 - Test Metric: 0.918\n",
      "Ep. 784 - Loss: 26.002 - Metric: 0.929 - Test loss: 26.155 - Test Metric: 0.918\n",
      "Ep. 785 - Loss: 48.798 - Metric: 0.930 - Test loss: 23.995 - Test Metric: 0.918\n",
      "Ep. 786 - Loss: 49.035 - Metric: 0.929 - Test loss: 21.543 - Test Metric: 0.918\n",
      "Ep. 787 - Loss: 19.255 - Metric: 0.929 - Test loss: 19.364 - Test Metric: 0.918\n",
      "Ep. 788 - Loss: 21.829 - Metric: 0.839 - Test loss: 18.436 - Test Metric: 0.918\n",
      "Ep. 789 - Loss: 21.461 - Metric: 0.930 - Test loss: 17.450 - Test Metric: 0.918\n",
      "Ep. 790 - Loss: 34.083 - Metric: 0.928 - Test loss: 16.220 - Test Metric: 0.918\n",
      "Ep. 791 - Loss: 19.049 - Metric: 0.929 - Test loss: 14.984 - Test Metric: 0.918\n",
      "Ep. 792 - Loss: 20.344 - Metric: 0.929 - Test loss: 13.953 - Test Metric: 0.918\n",
      "Ep. 793 - Loss: 30.148 - Metric: 0.875 - Test loss: 12.436 - Test Metric: 0.918\n",
      "Ep. 794 - Loss: 22.824 - Metric: 0.860 - Test loss: 11.031 - Test Metric: 0.918\n",
      "Ep. 795 - Loss: 6.174 - Metric: 0.930 - Test loss: 9.871 - Test Metric: 0.918\n",
      "Ep. 796 - Loss: 12.909 - Metric: 0.927 - Test loss: 9.058 - Test Metric: 0.918\n",
      "Ep. 797 - Loss: 13.841 - Metric: 0.929 - Test loss: 7.986 - Test Metric: 0.918\n",
      "Ep. 798 - Loss: 11.757 - Metric: 0.928 - Test loss: 6.719 - Test Metric: 0.918\n"
     ]
    }
   ],
   "source": [
    "# Initialize the epoch and step counters to -1\n",
    "# Create an empty list for storing training results\n",
    "epoch = step = -1\n",
    "results = []\n",
    "results_train = []\n",
    "# Iterate through the batches in the loader_train data loader\n",
    "for batch in loader_train:\n",
    "    inputs,targets = batch\n",
    "    targets = tf.convert_to_tensor(targets)\n",
    "    # Increment the step counter\n",
    "    step += 1\n",
    "\n",
    "    # Execute the train_step function with the current batch\n",
    "    # Obtain the loss and accuracy\n",
    "    loss, metric = train_step(inputs,targets)\n",
    "\n",
    "    # Append the loss and accuracy to the results list\n",
    "    results.append((loss, metric))\n",
    "    results_train.append( np.mean(results,0) )\n",
    "    # Check if the current step is equal to the number of steps per epoch (loader_train.steps_per_epoch)\n",
    "    if step == loader_train.steps_per_epoch:\n",
    "        # Reset the step counter to 0\n",
    "        # Increment the epoch counter\n",
    "        step = 0\n",
    "        epoch += 1\n",
    "\n",
    "        # Evaluate the model on the test set using the evaluate function (which should be defined beforehand)\n",
    "        # Store the test results in results_te\n",
    "        results_te = evaluate(loader_test)\n",
    "\n",
    "        # Print the epoch number, mean training loss and accuracy, and test loss and accuracy\n",
    "        print(\n",
    "            \"Ep. {} - Loss: {:.3f} - Metric: {:.3f} - Test loss: {:.3f} - Test Metric: {:.3f}\".format(\n",
    "                epoch, *np.mean(results, 0), *results_te\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Reset the results list to start collecting results for the next epoch\n",
    "        results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "losses_train = [item[0] for item in results_train]\n",
    "auc_train = [item[1] for item in results_train]\n",
    "\n",
    "# Create a list of epoch numbers\n",
    "epochs = list(range(1, len(losses_train) + 1))\n",
    "\n",
    "# Create a new figure and axis\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot losses_train on the first y-axis\n",
    "ax1.plot(epochs, losses_train, 'b-', label='Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "\n",
    "# Create a second y-axis to plot auc_train\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, auc_train, 'r-', label='AUC')\n",
    "ax2.set_ylabel('AUC', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "# Add legends for both y-axes\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spektral.models.gcn.GCN at 0x7f8db20a2820>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93830, 7) float32\n",
      "(93830, 93830) <dtype: 'float32'>\n",
      "(93830,) int64\n",
      "Targets shape: (93830, 2) dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/spektral/data/utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    }
   ],
   "source": [
    "sample_batch = loader_test.__next__()\n",
    "inputs, targets = sample_batch\n",
    "for tensor in inputs:\n",
    "    print(tensor.shape, tensor.dtype)\n",
    "print(\"Targets shape:\", targets.shape, \"dtype:\", targets.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "step = 0\n",
    "while step < loader_test.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader_test.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        predictions.append(pred)\n",
    "        targets.append(target)\n",
    "    \n",
    "predictions = np.vstack(predictions)\n",
    "    \n",
    "targets = np.vstack(targets)\n",
    "# Post-process the predictions if necessary (e.g., convert probabilities to class labels)\n",
    "predicted_values = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(targets,axis =1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0  21132]\n",
      " [    16 258962]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,accuracy_score\n",
    "\n",
    "conf_mat = confusion_matrix(predicted_values, true_labels)\n",
    "print(conf_mat)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.46227694988111134\n",
      "Accuracy Score: 0.9245010888579487\n"
     ]
    }
   ],
   "source": [
    "auc_score = roc_auc_score(true_labels, predicted_values)\n",
    "accuracy_score = accuracy_score(true_labels, predicted_values)\n",
    "\n",
    "print(\"AUC Score:\", auc_score)\n",
    "print(\"Accuracy Score:\", accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9245010888579487"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
