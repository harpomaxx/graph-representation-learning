{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "from os import path as osp\n",
    "from urllib.error import URLError\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.layers import Dense, Dropout,Input\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#from spektral.layers import GINConv,GCNConv #, GCSConv, GlobalAvgPool\n",
    "from spektral.data import Dataset, Graph, DisjointLoader\n",
    "from spektral.datasets.utils import download_file\n",
    "from spektral.utils import io, sparse\n",
    "from spektral.models.gcn import GCN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración de los datos de PROTEINS\n",
    "\n",
    "(Ignorar hasta el siguiente título)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"PROTEINS\"\n",
    "path = \"/root/spektral/datasets/TUDataset/PROTEINS\"\n",
    "fname_template = osp.join(path, \"{}_{{}}.txt\".format(name))\n",
    "available = [\n",
    "            f.split(os.sep)[-1][len(name) + 1 : -4]  # Remove leading name\n",
    "            for f in glob.glob(fname_template.format(\"*\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graph_indicator', 'node_attributes', 'A', 'node_labels', 'graph_labels']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch index\n",
    "node_batch_index = (\n",
    "            io.load_txt(fname_template.format(\"graph_indicator\")).astype(int) - 1\n",
    ")\n",
    "n_nodes = np.bincount(node_batch_index)\n",
    "n_nodes_cum = np.concatenate(([0], np.cumsum(n_nodes)[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read edge lists\n",
    "edges = io.load_txt(fname_template.format(\"A\"), delimiter=\",\").astype(int) - 1\n",
    "# Remove duplicates and self-loops from edges\n",
    "_, mask = np.unique(edges, axis=0, return_index=True)\n",
    "mask = mask[edges[mask, 0] != edges[mask, 1]]\n",
    "edges = edges[mask]\n",
    "# Split edges into separate edge lists\n",
    "edge_batch_idx = node_batch_index[edges[:, 0]]\n",
    "n_edges = np.bincount(edge_batch_idx)\n",
    "n_edges_cum = np.cumsum(n_edges[:-1])\n",
    "el_list = np.split(edges - n_nodes_cum[edge_batch_idx, None], n_edges_cum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(x, norm=None):\n",
    "    \"\"\"\n",
    "    Apply one-hot encoding or z-score to a list of node features\n",
    "    \"\"\"\n",
    "    if norm == \"ohe\":\n",
    "        fnorm = OneHotEncoder(sparse=False, categories=\"auto\")\n",
    "    elif norm == \"zscore\":\n",
    "        fnorm = StandardScaler()\n",
    "    else:\n",
    "        return x\n",
    "    return fnorm.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node features\n",
    "x_list = []\n",
    "if \"node_attributes\" in available:\n",
    "    x_attr = io.load_txt(fname_template.format(\"node_attributes\"), delimiter=\",\")\n",
    "    if x_attr.ndim == 1:\n",
    "        x_attr = x_attr[:, None]\n",
    "    x_list.append(x_attr)\n",
    "#if \"node_labels\" in available:\n",
    "#    x_labs = io.load_txt(fname_template.format(\"node_labels\"))\n",
    "#    if x_labs.ndim == 1:\n",
    "#        x_labs = x_labs[:, None]\n",
    "#    x_labs = np.concatenate([_normalize(xl_[:, None], \"ohe\") for xl_ in x_labs.T], -1)\n",
    "#    x_list.append(x_labs)\n",
    "if len(x_list) > 0:\n",
    "    x_list = np.concatenate(x_list, -1)\n",
    "    x_list = np.split(x_list, n_nodes_cum[1:])\n",
    "else:\n",
    "    print(\n",
    "            \"WARNING: this dataset doesn't have node attributes.\"\n",
    "            \"Consider creating manual features before using it with a \"\n",
    "            \"Loader.\"\n",
    "        )\n",
    "    x_list = [None] * len(n_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1113"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge features\n",
    "e_list = []\n",
    "if \"edge_attributes\" in available:\n",
    "    e_attr = io.load_txt(fname_template.format(\"edge_attributes\"))\n",
    "    if e_attr.ndim == 1:\n",
    "        e_attr = e_attr[:, None]\n",
    "    e_attr = e_attr[mask]\n",
    "    e_list.append(e_attr)\n",
    "if \"edge_labels\" in available:\n",
    "    e_labs = io.load_txt(fname_template.format(\"edge_labels\"))\n",
    "    if e_labs.ndim == 1:\n",
    "        e_labs = e_labs[:, None]\n",
    "    e_labs = e_labs[mask]\n",
    "    e_labs = np.concatenate(\n",
    "        [_normalize(el_[:, None], \"ohe\") for el_ in e_labs.T], -1\n",
    "    )\n",
    "    e_list.append(e_labs)\n",
    "if len(e_list) > 0:\n",
    "    e_available = True\n",
    "    e_list = np.concatenate(e_list, -1)\n",
    "    e_list = np.split(e_list, n_edges_cum)\n",
    "else:\n",
    "    e_available = False\n",
    "    e_list = [None] * len(n_nodes)\n",
    "\n",
    "# Create sparse adjacency matrices and re-sort edge attributes in lexicographic\n",
    "# order\n",
    "a_e_list = [\n",
    "    sparse.edge_index_to_matrix(\n",
    "        edge_index=el,\n",
    "        edge_weight=np.ones(el.shape[0]),\n",
    "        edge_features=e,\n",
    "        shape=(n, n),\n",
    "    )\n",
    "    for el, e, n in zip(el_list, e_list, n_nodes)\n",
    "]\n",
    "if e_available:\n",
    "    a_list, e_list = list(zip(*a_e_list))\n",
    "else:\n",
    "    a_list = a_e_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1113"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Labels\n",
    "if \"graph_attributes\" in available:\n",
    "    labels = io.load_txt(fname_template.format(\"graph_attributes\"))\n",
    "elif \"graph_labels\" in available:\n",
    "    labels = io.load_txt(fname_template.format(\"graph_labels\"))\n",
    "    labels = _normalize(labels[:, None], \"ohe\")\n",
    "else:\n",
    "    raise ValueError(\"No labels available for dataset {}\".format(self.name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1113, (1113, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1113"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels\n",
    "labels = []\n",
    "if \"node_labels\" in available:\n",
    "    x_labs = io.load_txt(fname_template.format(\"node_labels\"))\n",
    "    if x_labs.ndim == 1:\n",
    "        x_labs = x_labs[:, None]\n",
    "    x_labs = np.concatenate([_normalize(xl_[:, None], \"ohe\") for xl_ in x_labs.T], -1)\n",
    "    labels.append(x_labs)\n",
    "if len(labels) > 0:\n",
    "    labels = np.concatenate(labels, -1)\n",
    "    labels = np.split(labels, n_nodes_cum[1:])\n",
    "    \n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if \"node_labels\" in available:\n",
    "    x_labs = io.load_txt(fname_template.format(\"node_labels\"))\n",
    "    if x_labs.ndim == 1:\n",
    "        x_labs = x_labs[:, None]\n",
    "    x_labs = np.concatenate([_normalize(xl_[:, None], \"ohe\") for xl_ in x_labs.T], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43471, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_labs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43471, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if \"node_labels\" in available:\n",
    "    labels = io.load_txt(fname_template.format(\"node_labels\"))\n",
    "    labels = _normalize(labels[:, None], \"ohe\")\n",
    "\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=[Graph(x=x, a=a, e=e, y=y) for x, a, e, y in zip(x_list, a_list, e_list, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[0].y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptación de PROTEINS a clasificación de nodos\n",
    "\n",
    "Defino una clase nueva `MisProteinas` que tiene una clase por nodo, en lugar de una clase por grafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(x, norm=None):\n",
    "    \"\"\"\n",
    "    Apply one-hot encoding or z-score to a list of node features\n",
    "    \"\"\"\n",
    "    if norm == \"ohe\":\n",
    "        fnorm = OneHotEncoder(sparse=False, categories=\"auto\")\n",
    "    elif norm == \"zscore\":\n",
    "        fnorm = StandardScaler()\n",
    "    else:\n",
    "        return x\n",
    "    return fnorm.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MisProteinas(Dataset):\n",
    "    \"\"\"\n",
    "    Adaptado de https://github.com/danielegrattarola/spektral/blob/master/spektral/datasets/tudataset.py\n",
    "    \n",
    "    The Benchmark Data Sets for Graph Kernels from TU Dortmund\n",
    "    ([link](https://chrsmrrs.github.io/datasets/docs/datasets/)).\n",
    "    Node features are computed by concatenating the following features for\n",
    "    each node:\n",
    "    - node attributes, if available;\n",
    "    - node labels, if available, one-hot encoded.\n",
    "    Some datasets might not have node features at all. In this case, attempting\n",
    "    to use the dataset with a Loader will result in a crash. You can create\n",
    "    node features using some of the transforms available in `spektral.transforms`\n",
    "    or you can define your own features by accessing the individual samples in\n",
    "    the `graph` attribute of the dataset (which is a list of `Graph` objects).\n",
    "    Edge features are computed by concatenating the following features for\n",
    "    each node:\n",
    "    - edge attributes, if available;\n",
    "    - edge labels, if available, one-hot encoded.\n",
    "    Graph labels are provided for each dataset.\n",
    "    Specific details about each individual dataset can be found in\n",
    "    `~/spektral/datasets/TUDataset/<dataset name>/README.md`, after the dataset\n",
    "    has been downloaded locally (datasets are downloaded automatically upon\n",
    "    calling `TUDataset('<dataset name>')` the first time).\n",
    "    **Arguments**\n",
    "    - `name`: str, name of the dataset to load (see `TUD.available_datasets`).\n",
    "    - `clean`: if `True`, rload a version of the dataset with no isomorphic\n",
    "               graphs.\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://www.chrsmrrs.com/graphkerneldatasets\"\n",
    "    url_clean = (\n",
    "        \"https://raw.githubusercontent.com/nd7141/graph_datasets/master/datasets\"\n",
    "    )\n",
    "\n",
    "    def __init__(self, name, clean=False, **kwargs):\n",
    "        if name not in self.available_datasets():\n",
    "            raise ValueError(\n",
    "                \"Unknown dataset {}. See {}.available_datasets() for a complete list of\"\n",
    "                \"available datasets.\".format(name, self.__class__.__name__)\n",
    "            )\n",
    "        self.name = name\n",
    "        self.clean = clean\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def path(self):\n",
    "        return osp.join(super().path, self.name + (\"_clean\" if self.clean else \"\"))\n",
    "\n",
    "    def download(self):\n",
    "        print(\n",
    "            \"Downloading {} dataset{}.\".format(\n",
    "                self.name, \" (clean)\" if self.clean else \"\"\n",
    "            )\n",
    "        )\n",
    "        url = \"{}/{}.zip\".format(self.url_clean if self.clean else self.url, self.name)\n",
    "        download_file(url, self.path, self.name + \".zip\")\n",
    "\n",
    "        # Datasets are zipped in a folder: unpack them\n",
    "        parent = self.path\n",
    "        subfolder = osp.join(self.path, self.name)\n",
    "        for filename in os.listdir(subfolder):\n",
    "            shutil.move(osp.join(subfolder, filename), osp.join(parent, filename))\n",
    "        os.rmdir(subfolder)\n",
    "\n",
    "    def read(self):\n",
    "        fname_template = osp.join(self.path, \"{}_{{}}.txt\".format(self.name))\n",
    "        available = [\n",
    "            f.split(os.sep)[-1][len(self.name) + 1 : -4]  # Remove leading name\n",
    "            for f in glob.glob(fname_template.format(\"*\"))\n",
    "        ]\n",
    "\n",
    "        # Batch index\n",
    "        node_batch_index = (\n",
    "            io.load_txt(fname_template.format(\"graph_indicator\")).astype(int) - 1\n",
    "        )\n",
    "        n_nodes = np.bincount(node_batch_index)\n",
    "        n_nodes_cum = np.concatenate(([0], np.cumsum(n_nodes)[:-1]))\n",
    "\n",
    "        # Read edge lists\n",
    "        edges = io.load_txt(fname_template.format(\"A\"), delimiter=\",\").astype(int) - 1\n",
    "        # Remove duplicates and self-loops from edges\n",
    "        _, mask = np.unique(edges, axis=0, return_index=True)\n",
    "        mask = mask[edges[mask, 0] != edges[mask, 1]]\n",
    "        edges = edges[mask]\n",
    "        # Split edges into separate edge lists\n",
    "        edge_batch_idx = node_batch_index[edges[:, 0]]\n",
    "        n_edges = np.bincount(edge_batch_idx)\n",
    "        n_edges_cum = np.cumsum(n_edges[:-1])\n",
    "        el_list = np.split(edges - n_nodes_cum[edge_batch_idx, None], n_edges_cum)\n",
    "\n",
    "        # Node features\n",
    "        x_list = []\n",
    "        if \"node_attributes\" in available:\n",
    "            x_attr = io.load_txt(\n",
    "                fname_template.format(\"node_attributes\"), delimiter=\",\"\n",
    "            )\n",
    "            if x_attr.ndim == 1:\n",
    "                x_attr = x_attr[:, None]\n",
    "            x_list.append(x_attr)\n",
    "        #if \"node_labels\" in available:\n",
    "        #    x_labs = io.load_txt(fname_template.format(\"node_labels\"))\n",
    "        #    if x_labs.ndim == 1:\n",
    "        #        x_labs = x_labs[:, None]\n",
    "        #    x_labs = np.concatenate(\n",
    "        #        [_normalize(xl_[:, None], \"ohe\") for xl_ in x_labs.T], -1\n",
    "        #    )\n",
    "        #    x_list.append(x_labs)\n",
    "        if len(x_list) > 0:\n",
    "            x_list = np.concatenate(x_list, -1)\n",
    "            x_list = np.split(x_list, n_nodes_cum[1:])\n",
    "        else:\n",
    "            print(\n",
    "                \"WARNING: this dataset doesn't have node attributes.\"\n",
    "                \"Consider creating manual features before using it with a \"\n",
    "                \"Loader.\"\n",
    "            )\n",
    "            x_list = [None] * len(n_nodes)\n",
    "\n",
    "        # Edge features\n",
    "        e_list = []\n",
    "        if \"edge_attributes\" in available:\n",
    "            e_attr = io.load_txt(fname_template.format(\"edge_attributes\"))\n",
    "            if e_attr.ndim == 1:\n",
    "                e_attr = e_attr[:, None]\n",
    "            e_attr = e_attr[mask]\n",
    "            e_list.append(e_attr)\n",
    "        if \"edge_labels\" in available:\n",
    "            e_labs = io.load_txt(fname_template.format(\"edge_labels\"))\n",
    "            if e_labs.ndim == 1:\n",
    "                e_labs = e_labs[:, None]\n",
    "            e_labs = e_labs[mask]\n",
    "            e_labs = np.concatenate(\n",
    "                [_normalize(el_[:, None], \"ohe\") for el_ in e_labs.T], -1\n",
    "            )\n",
    "            e_list.append(e_labs)\n",
    "        if len(e_list) > 0:\n",
    "            e_available = True\n",
    "            e_list = np.concatenate(e_list, -1)\n",
    "            e_list = np.split(e_list, n_edges_cum)\n",
    "        else:\n",
    "            e_available = False\n",
    "            e_list = [None] * len(n_nodes)\n",
    "\n",
    "        # Create sparse adjacency matrices and re-sort edge attributes in lexicographic\n",
    "        # order\n",
    "        a_e_list = [\n",
    "            sparse.edge_index_to_matrix(\n",
    "                edge_index=el,\n",
    "                edge_weight=np.ones(el.shape[0]),\n",
    "                edge_features=e,\n",
    "                shape=(n, n),\n",
    "            )\n",
    "            for el, e, n in zip(el_list, e_list, n_nodes)\n",
    "        ]\n",
    "        if e_available:\n",
    "            a_list, e_list = list(zip(*a_e_list))\n",
    "        else:\n",
    "            a_list = a_e_list\n",
    "\n",
    "        # Labels\n",
    "        labels = []\n",
    "        if \"node_labels\" in available:\n",
    "            x_labs = io.load_txt(fname_template.format(\"node_labels\"))\n",
    "            if x_labs.ndim == 1:\n",
    "                x_labs = x_labs[:, None]\n",
    "            x_labs = np.concatenate([_normalize(xl_[:, None], \"ohe\") for xl_ in x_labs.T], -1)\n",
    "            labels.append(x_labs)\n",
    "        if len(labels) > 0:\n",
    "            labels = np.concatenate(labels, -1)\n",
    "            labels = np.split(labels, n_nodes_cum[1:])\n",
    "        #if \"graph_attributes\" in available:\n",
    "        #    labels = io.load_txt(fname_template.format(\"graph_attributes\"))\n",
    "        #elif \"graph_labels\" in available:\n",
    "        #    labels = io.load_txt(fname_template.format(\"graph_labels\"))\n",
    "        #    labels = _normalize(labels[:, None], \"ohe\")\n",
    "        else:\n",
    "            raise ValueError(\"No labels available for dataset {}\".format(self.name))\n",
    "\n",
    "        # Convert to Graph\n",
    "        print(\"Successfully loaded {}.\".format(self.name))\n",
    "        return [\n",
    "            Graph(x=x, a=a, e=e, y=y)\n",
    "            for x, a, e, y in zip(x_list, a_list, e_list, labels)\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def available_datasets():\n",
    "        url = \"https://chrsmrrs.github.io/datasets/docs/datasets/\"\n",
    "        try:\n",
    "            tables = pd.read_html(url)\n",
    "            names = []\n",
    "            for table in tables:\n",
    "                names.extend(table.Name[1:].values.tolist())\n",
    "            return names\n",
    "        except URLError:\n",
    "            # No internet, don't panic\n",
    "            print(\"Could not read URL {}\".format(url))\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded PROTEINS.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "proteinas=MisProteinas(\"PROTEINS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteinas[0].y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.8 * len(proteinas))\n",
    "dataset_tosplit, dataset_test = proteinas[:split], proteinas[split:]\n",
    "\n",
    "split = int(0.8 * len(dataset_tosplit))\n",
    "dataset_train, dataset_val = dataset_tosplit[:split], dataset_tosplit[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "loader_train = DisjointLoader(dataset_train, node_level=True,batch_size=batch_size, epochs=10, shuffle=False)\n",
    "loader_val = DisjointLoader(dataset_val, node_level=True,batch_size=batch_size,shuffle=False)\n",
    "loader_test = DisjointLoader(dataset_test, node_level=True,batch_size=batch_size,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(n_labels=proteinas.n_labels)\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "loss_fn = CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorate the function with @tf.function to compile as a TensorFlow graph\n",
    "# Use the input_signature from loader_train and relax shapes for varying graph sizes\n",
    "@tf.function(input_signature=loader_train.tf_signature(), experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    print(\"target:\",str(target))\n",
    "    # Create a GradientTape context to record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute model predictions with the inputs, set training=True for training-specific behaviors\n",
    "        predictions = model(inputs, training=True)\n",
    "        print(\"pred:\",str(predictions))\n",
    "        #predictions = tf.argmax(predictions1,axis=1)\n",
    "        # Calculate the loss using the provided loss_fn and add the model's regularization losses\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "\n",
    "    # Compute gradients of the loss with respect to the model's trainable variables\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Apply the gradients to the model's variables using the optimizer's apply_gradients method\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Compute the accuracy using the categorical_accuracy function from TensorFlow\n",
    "    # Calculate the mean accuracy using tf.reduce_mean\n",
    "    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "\n",
    "    # Return the loss and accuracy as output\n",
    "    return loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            tf.reduce_mean(categorical_accuracy(target, pred)),\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: Tensor(\"target:0\", shape=(None, 3), dtype=float64)\n",
      "pred: Tensor(\"gcn/gcn_conv_1/Softmax:0\", shape=(None, 3), dtype=float32)\n",
      "target: Tensor(\"target:0\", shape=(None, 3), dtype=float64)\n",
      "pred: Tensor(\"gcn/gcn_conv_1/Softmax:0\", shape=(None, 3), dtype=float32)\n",
      "Ep. 0 - Loss: 6.310 - Acc: 0.454 - Val loss: 1.047 - Val acc: 0.447\n",
      "Ep. 1 - Loss: 1.818 - Acc: 0.473 - Val loss: 0.886 - Val acc: 0.539\n",
      "Ep. 2 - Loss: 1.216 - Acc: 0.474 - Val loss: 0.807 - Val acc: 0.446\n",
      "Ep. 3 - Loss: 1.035 - Acc: 0.476 - Val loss: 0.776 - Val acc: 0.539\n",
      "Ep. 4 - Loss: 0.951 - Acc: 0.481 - Val loss: 0.793 - Val acc: 0.446\n",
      "Ep. 5 - Loss: 0.929 - Acc: 0.490 - Val loss: 0.779 - Val acc: 0.447\n",
      "Ep. 6 - Loss: 0.883 - Acc: 0.497 - Val loss: 0.787 - Val acc: 0.446\n",
      "Ep. 7 - Loss: 0.888 - Acc: 0.498 - Val loss: 0.793 - Val acc: 0.446\n",
      "Ep. 8 - Loss: 0.864 - Acc: 0.490 - Val loss: 0.798 - Val acc: 0.447\n"
     ]
    }
   ],
   "source": [
    "# Initialize the epoch and step counters to -1\n",
    "# Create an empty list for storing training results\n",
    "epoch = step = -1\n",
    "results = []\n",
    "\n",
    "# Iterate through the batches in the loader_train data loader\n",
    "for batch in loader_train:\n",
    "    # Increment the step counter\n",
    "    step += 1\n",
    "\n",
    "    # Execute the train_step function with the current batch\n",
    "    # Obtain the loss and accuracy\n",
    "    loss, acc = train_step(*batch)\n",
    "\n",
    "    # Append the loss and accuracy to the results list\n",
    "    results.append((loss, acc))\n",
    "\n",
    "    # Check if the current step is equal to the number of steps per epoch (loader_train.steps_per_epoch)\n",
    "    if step == loader_train.steps_per_epoch:\n",
    "        # Reset the step counter to 0\n",
    "        # Increment the epoch counter\n",
    "        step = 0\n",
    "        epoch += 1\n",
    "\n",
    "        # Evaluate the model on the test set using the evaluate function (which should be defined beforehand)\n",
    "        # Store the test results in results_te\n",
    "        results_te = evaluate(loader_val) # CAMBIO A loader_val\n",
    "\n",
    "        # Print the epoch number, mean training loss and accuracy, and test loss and accuracy\n",
    "        print(\n",
    "            \"Ep. {} - Loss: {:.3f} - Acc: {:.3f} - Val loss: {:.3f} - Val acc: {:.3f}\".format(\n",
    "                epoch, *np.mean(results, 0), *results_te\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Reset the results list to start collecting results for the next epoch\n",
    "        results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_test.steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[309   0   0]\n",
      " [347   0   0]\n",
      " [  0   0   0]], shape=(3, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[292   0   0]\n",
      " [346   0   0]\n",
      " [ 33   0   0]], shape=(3, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[205   0   0]\n",
      " [330   0   0]\n",
      " [ 18   0   0]], shape=(3, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[378   0   0]\n",
      " [484   0   0]\n",
      " [  0   0   0]], shape=(3, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[252   0   0]\n",
      " [427   0   0]\n",
      " [ 15   0   0]], shape=(3, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[230   0   0]\n",
      " [321   0   0]\n",
      " [ 78   0   0]], shape=(3, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[171   0   0]\n",
      " [247   0   0]\n",
      " [ 78   0   0]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#Predict\n",
    "for _ in range(loader_test.steps_per_epoch):\n",
    "    inputs,target = loader_test.__next__()\n",
    "    y_prediction = model(inputs, training=False)\n",
    "    y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "    y_test=np.argmax(target, axis=1)\n",
    "    #Create confusion matrix and normalizes it over predicted (columns)\n",
    "    result = tf.math.confusion_matrix(y_test, y_prediction, num_classes=3)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[656, 671, 553, 862, 694, 629, 496]\n"
     ]
    }
   ],
   "source": [
    "suma = []\n",
    "sumatemp = 0\n",
    "contador = 0\n",
    "for i in range(len(dataset_test)):\n",
    "    sumatemp += dataset_test[i].n_nodes\n",
    "    contador += 1\n",
    "    if contador == 32:\n",
    "        suma.append(sumatemp)\n",
    "        sumatemp = 0\n",
    "        contador = 0\n",
    "    elif i == len(dataset_test)-1:\n",
    "        suma.append(sumatemp)\n",
    "        \n",
    "print(suma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOBRE LA PREDICCIÓN\n",
    "\n",
    "Para predecir hay que ejecutar `model(inputs,training=False)`, donde en un paso previo se define `inputs` como `inputs,target = loader_test.__next__()`.\n",
    "\n",
    "Había intentado predecir con `model.predict(loader_test)`, pero esto NO REALIZA LA PREDICCIÓN. Probablemente se quedaba en un loop infinito o algo así, tratando de \"desempaquetar\" `loader_test`\n",
    "\n",
    "En el caso de las proteínas, al hacer `model.predict` lo corté en \"653683/Unknown - 7336s 11ms/step\", mientras que para CTU13 se quedaba sin memoria mucho antes, no pudiendo calcular ninguna predicción\n",
    "\n",
    "```\n",
    "#Predict\n",
    "y_prediction = model.predict(loader_test)\n",
    "y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "y_test=np.argmax(dataset_test[0].y, axis=1)\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
    "print(result)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
