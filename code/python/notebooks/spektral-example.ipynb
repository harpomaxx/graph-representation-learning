{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTALL spektral library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spektral\n",
      "  Downloading spektral-1.2.0-py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.5 MB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.3 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 67 kB/s eta 0:00:014\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from spektral) (1.23.4)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from spektral) (2.22.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lxml\n",
      "  Downloading lxml-4.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from spektral) (2.9.3)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.8 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[K     |████████████████████████████████| 341 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->spektral) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[K     |████████████████████████████████| 502 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (0.27.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (1.14.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (1.1.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (1.50.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (2.9.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (3.19.6)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (2.9.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (21.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (2.1.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (65.5.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (1.3.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (3.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (14.0.6)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (2.9.1)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (1.12)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow>=2.2.0->spektral) (1.14.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (4.4.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->spektral) (0.34.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow>=2.2.0->spektral) (3.0.9)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (2.14.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (2.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (5.0.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (3.2.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (3.10.0)\n",
      "Installing collected packages: scipy, tzdata, pytz, pandas, networkx, tqdm, lxml, joblib, threadpoolctl, scikit-learn, spektral\n",
      "Successfully installed joblib-1.2.0 lxml-4.9.2 networkx-3.1 pandas-2.0.0 pytz-2023.3 scikit-learn-1.2.2 scipy-1.10.1 spektral-1.2.0 threadpoolctl-3.1.0 tqdm-4.65.0 tzdata-2023.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spektral"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout,Input\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from spektral.layers import GINConv,GCNConv\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "from spektral.data import DisjointLoader, BatchLoader\n",
    "from spektral.datasets import TUDataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TUDataset\n",
    "https://chrsmrrs.github.io/datasets/docs/datasets/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TUDataset class from the Spektral library provides access to several benchmark graph datasets for graph classification tasks. One such dataset is the \"PROTEINS\" dataset. It contains a collection of protein structures, represented as graphs. In these graphs, nodes represent amino acids and edges represent connections between them based on spatial distance.\n",
    "\n",
    "Each graph in the \"PROTEINS\" dataset has the following properties:\n",
    "\n",
    "**Graph**: A graph representing the protein structure, with nodes as amino acids and edges as their spatial connections.\n",
    "\n",
    "**Node features**: Each node in the graph has a feature vector with 4 dimensions, representing the amino acid type, secondary structure, and other properties. These features are used as input to the graph neural network.\n",
    "\n",
    "**Graph label**: Each graph in the dataset is labeled as either \"enzymatic\" or \"non-enzymatic.\" The goal of the graph classification task is to predict this label based on the graph structure and node features.\n",
    "\n",
    "When you load the \"PROTEINS\" dataset using the TUDataset class, it preprocesses the raw data and creates a dataset object with the following properties:\n",
    "\n",
    "`n_graphs`: Number of graphs in the dataset.\n",
    "`n_node_features`: Dimension of the node features (4 for the PROTEINS dataset).\n",
    "`n_labels`: Number of unique labels in the dataset (2 for the PROTEINS dataset, i.e., enzymatic and non-enzymatic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(\"PROTEINS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(n_nodes=42, n_node_features=4, n_edge_features=None, n_labels=2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g=dataset[0]\n",
    "g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPLIT in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.8 * len(dataset))\n",
    "dataset_train, dataset_test = dataset[:split], dataset[split:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data loader for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "loader_train = DisjointLoader(dataset_train, batch_size=batch_size, epochs=200, shuffle=False)\n",
    "loader_test = DisjointLoader(dataset_test, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output for `DisjointLoader()`\n",
    "\n",
    "For each `batch`, returns a tuple (inputs, labels).\n",
    "\n",
    "inputs is a tuple containing:\n",
    "\n",
    "x: node attributes of shape [n_nodes, n_node_features];\n",
    "a: adjacency matrices of shape [n_nodes, n_nodes];\n",
    "e: edge attributes of shape [n_edges, n_edge_features];\n",
    "i: batch index of shape [n_nodes]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the GCN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gcn_model():\n",
    "    # Define input placeholders for node features, adjacency matrix, and segment indices\n",
    "    X_in = Input(shape=(dataset.n_node_features,))\n",
    "    A_in = Input((None,), sparse=True)\n",
    "    I_in = Input(shape=(), dtype=tf.int32)\n",
    "\n",
    "    # Apply the first GINConv layer with 32 units and ReLU activation\n",
    "    X_1 = GINConv(32, activation=\"relu\")([X_in, A_in])\n",
    "    # Apply dropout with a rate of 0.5\n",
    "    X_1 = Dropout(0.5)(X_1)\n",
    "\n",
    "    # Apply the second GINConv layer with 32 units and ReLU activation\n",
    "    X_2 = GINConv(32, activation=\"relu\")([X_1, A_in])\n",
    "    # Apply dropout with a rate of 0.5\n",
    "    X_2 = Dropout(0.5)(X_2)\n",
    "\n",
    "    # Aggregate the node features using the segment_mean function and the segment indices\n",
    "    X_3 = tf.math.segment_mean(X_2, I_in)\n",
    "    # Apply a dense output layer with the number of labels and softmax activation\n",
    "    out = Dense(dataset.n_labels, activation=\"softmax\")(X_3)\n",
    "\n",
    "    # Create and return the model with the defined inputs and outputs\n",
    "    model = Model(inputs=[X_in, A_in, I_in], outputs=out)\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = create_gcn_model()\n",
    "optimizer = Adam(lr=0.01)\n",
    "loss_fn = CategoricalCrossentropy()\n",
    "#model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model:\n",
    "Train the model using the data loader for the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorate the function with @tf.function to compile as a TensorFlow graph\n",
    "# Use the input_signature from loader_train and relax shapes for varying graph sizes\n",
    "@tf.function(input_signature=loader_train.tf_signature(), experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    # Create a GradientTape context to record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute model predictions with the inputs, set training=True for training-specific behaviors\n",
    "        predictions = model(inputs, training=True)\n",
    "        # Calculate the loss using the provided loss_fn and add the model's regularization losses\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "\n",
    "    # Compute gradients of the loss with respect to the model's trainable variables\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Apply the gradients to the model's variables using the optimizer's apply_gradients method\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Compute the accuracy using the categorical_accuracy function from TensorFlow\n",
    "    # Calculate the mean accuracy using tf.reduce_mean\n",
    "    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "\n",
    "    # Return the loss and accuracy as output\n",
    "    return loss, acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            tf.reduce_mean(categorical_accuracy(target, pred)),\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 0 - Loss: 0.672 - Acc: 0.708 - Test loss: 0.827 - Test acc: 0.000\n",
      "Ep. 1 - Loss: 0.773 - Acc: 0.661 - Test loss: 0.889 - Test acc: 0.004\n",
      "Ep. 2 - Loss: 0.706 - Acc: 0.716 - Test loss: 0.921 - Test acc: 0.004\n",
      "Ep. 3 - Loss: 0.684 - Acc: 0.729 - Test loss: 0.992 - Test acc: 0.000\n",
      "Ep. 4 - Loss: 0.656 - Acc: 0.735 - Test loss: 0.976 - Test acc: 0.000\n",
      "Ep. 5 - Loss: 0.650 - Acc: 0.734 - Test loss: 0.949 - Test acc: 0.000\n",
      "Ep. 6 - Loss: 0.645 - Acc: 0.738 - Test loss: 0.951 - Test acc: 0.000\n",
      "Ep. 7 - Loss: 0.631 - Acc: 0.738 - Test loss: 0.965 - Test acc: 0.000\n",
      "Ep. 8 - Loss: 0.626 - Acc: 0.738 - Test loss: 0.975 - Test acc: 0.000\n",
      "Ep. 9 - Loss: 0.628 - Acc: 0.741 - Test loss: 1.001 - Test acc: 0.000\n",
      "Ep. 10 - Loss: 0.605 - Acc: 0.735 - Test loss: 1.029 - Test acc: 0.000\n",
      "Ep. 11 - Loss: 0.620 - Acc: 0.740 - Test loss: 1.057 - Test acc: 0.000\n",
      "Ep. 12 - Loss: 0.606 - Acc: 0.740 - Test loss: 1.080 - Test acc: 0.004\n",
      "Ep. 13 - Loss: 0.606 - Acc: 0.740 - Test loss: 1.108 - Test acc: 0.004\n",
      "Ep. 14 - Loss: 0.602 - Acc: 0.739 - Test loss: 1.138 - Test acc: 0.000\n",
      "Ep. 15 - Loss: 0.591 - Acc: 0.740 - Test loss: 1.159 - Test acc: 0.000\n",
      "Ep. 16 - Loss: 0.598 - Acc: 0.740 - Test loss: 1.179 - Test acc: 0.000\n",
      "Ep. 17 - Loss: 0.588 - Acc: 0.740 - Test loss: 1.200 - Test acc: 0.000\n",
      "Ep. 18 - Loss: 0.588 - Acc: 0.744 - Test loss: 1.217 - Test acc: 0.000\n",
      "Ep. 19 - Loss: 0.588 - Acc: 0.740 - Test loss: 1.230 - Test acc: 0.000\n",
      "Ep. 20 - Loss: 0.582 - Acc: 0.742 - Test loss: 1.242 - Test acc: 0.000\n",
      "Ep. 21 - Loss: 0.595 - Acc: 0.740 - Test loss: 1.253 - Test acc: 0.000\n",
      "Ep. 22 - Loss: 0.587 - Acc: 0.740 - Test loss: 1.263 - Test acc: 0.000\n",
      "Ep. 23 - Loss: 0.589 - Acc: 0.740 - Test loss: 1.270 - Test acc: 0.000\n",
      "Ep. 24 - Loss: 0.593 - Acc: 0.740 - Test loss: 1.276 - Test acc: 0.000\n",
      "Ep. 25 - Loss: 0.590 - Acc: 0.740 - Test loss: 1.281 - Test acc: 0.000\n",
      "Ep. 26 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.287 - Test acc: 0.000\n",
      "Ep. 27 - Loss: 0.586 - Acc: 0.740 - Test loss: 1.292 - Test acc: 0.000\n",
      "Ep. 28 - Loss: 0.586 - Acc: 0.740 - Test loss: 1.297 - Test acc: 0.000\n",
      "Ep. 29 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.299 - Test acc: 0.000\n",
      "Ep. 30 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.302 - Test acc: 0.000\n",
      "Ep. 31 - Loss: 0.582 - Acc: 0.741 - Test loss: 1.305 - Test acc: 0.000\n",
      "Ep. 32 - Loss: 0.587 - Acc: 0.741 - Test loss: 1.306 - Test acc: 0.000\n",
      "Ep. 33 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.309 - Test acc: 0.000\n",
      "Ep. 34 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.311 - Test acc: 0.000\n",
      "Ep. 35 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.313 - Test acc: 0.000\n",
      "Ep. 36 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.314 - Test acc: 0.000\n",
      "Ep. 37 - Loss: 0.583 - Acc: 0.741 - Test loss: 1.316 - Test acc: 0.000\n",
      "Ep. 38 - Loss: 0.585 - Acc: 0.740 - Test loss: 1.318 - Test acc: 0.000\n",
      "Ep. 39 - Loss: 0.584 - Acc: 0.740 - Test loss: 1.320 - Test acc: 0.000\n",
      "Ep. 40 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.321 - Test acc: 0.000\n",
      "Ep. 41 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.322 - Test acc: 0.000\n",
      "Ep. 42 - Loss: 0.583 - Acc: 0.741 - Test loss: 1.323 - Test acc: 0.000\n",
      "Ep. 43 - Loss: 0.581 - Acc: 0.740 - Test loss: 1.325 - Test acc: 0.000\n",
      "Ep. 44 - Loss: 0.587 - Acc: 0.740 - Test loss: 1.325 - Test acc: 0.000\n",
      "Ep. 45 - Loss: 0.585 - Acc: 0.740 - Test loss: 1.324 - Test acc: 0.000\n",
      "Ep. 46 - Loss: 0.581 - Acc: 0.740 - Test loss: 1.325 - Test acc: 0.000\n",
      "Ep. 47 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.326 - Test acc: 0.000\n",
      "Ep. 48 - Loss: 1.680 - Acc: 0.740 - Test loss: 1.284 - Test acc: 0.000\n",
      "Ep. 49 - Loss: 0.643 - Acc: 0.740 - Test loss: 1.244 - Test acc: 0.000\n",
      "Ep. 50 - Loss: 0.596 - Acc: 0.740 - Test loss: 1.250 - Test acc: 0.000\n",
      "Ep. 51 - Loss: 0.587 - Acc: 0.740 - Test loss: 1.264 - Test acc: 0.000\n",
      "Ep. 52 - Loss: 0.584 - Acc: 0.740 - Test loss: 1.277 - Test acc: 0.000\n",
      "Ep. 53 - Loss: 0.585 - Acc: 0.740 - Test loss: 1.286 - Test acc: 0.000\n",
      "Ep. 54 - Loss: 0.588 - Acc: 0.740 - Test loss: 1.293 - Test acc: 0.000\n",
      "Ep. 55 - Loss: 0.610 - Acc: 0.740 - Test loss: 1.298 - Test acc: 0.000\n",
      "Ep. 56 - Loss: 0.584 - Acc: 0.741 - Test loss: 1.305 - Test acc: 0.000\n",
      "Ep. 57 - Loss: 0.587 - Acc: 0.740 - Test loss: 1.311 - Test acc: 0.000\n",
      "Ep. 58 - Loss: 0.581 - Acc: 0.740 - Test loss: 1.317 - Test acc: 0.000\n",
      "Ep. 59 - Loss: 0.584 - Acc: 0.741 - Test loss: 1.322 - Test acc: 0.000\n",
      "Ep. 60 - Loss: 0.585 - Acc: 0.739 - Test loss: 1.326 - Test acc: 0.000\n",
      "Ep. 61 - Loss: 0.587 - Acc: 0.740 - Test loss: 1.328 - Test acc: 0.000\n",
      "Ep. 62 - Loss: 0.584 - Acc: 0.741 - Test loss: 1.329 - Test acc: 0.000\n",
      "Ep. 63 - Loss: 0.581 - Acc: 0.740 - Test loss: 1.332 - Test acc: 0.000\n",
      "Ep. 64 - Loss: 0.582 - Acc: 0.741 - Test loss: 1.333 - Test acc: 0.000\n",
      "Ep. 65 - Loss: 0.589 - Acc: 0.739 - Test loss: 1.335 - Test acc: 0.000\n",
      "Ep. 66 - Loss: 0.584 - Acc: 0.740 - Test loss: 1.336 - Test acc: 0.000\n",
      "Ep. 67 - Loss: 0.837 - Acc: 0.739 - Test loss: 1.318 - Test acc: 0.000\n",
      "Ep. 68 - Loss: 0.590 - Acc: 0.740 - Test loss: 1.323 - Test acc: 0.000\n",
      "Ep. 69 - Loss: 0.588 - Acc: 0.740 - Test loss: 1.332 - Test acc: 0.000\n",
      "Ep. 70 - Loss: 0.584 - Acc: 0.740 - Test loss: 1.334 - Test acc: 0.000\n",
      "Ep. 71 - Loss: 0.585 - Acc: 0.740 - Test loss: 1.336 - Test acc: 0.000\n",
      "Ep. 72 - Loss: 0.600 - Acc: 0.739 - Test loss: 1.354 - Test acc: 0.000\n",
      "Ep. 73 - Loss: 0.586 - Acc: 0.740 - Test loss: 1.340 - Test acc: 0.000\n",
      "Ep. 74 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.336 - Test acc: 0.000\n",
      "Ep. 75 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.335 - Test acc: 0.000\n",
      "Ep. 76 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.335 - Test acc: 0.004\n",
      "Ep. 77 - Loss: 0.582 - Acc: 0.741 - Test loss: 1.335 - Test acc: 0.004\n",
      "Ep. 78 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.335 - Test acc: 0.004\n",
      "Ep. 79 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.336 - Test acc: 0.004\n",
      "Ep. 80 - Loss: 0.580 - Acc: 0.742 - Test loss: 1.337 - Test acc: 0.000\n",
      "Ep. 81 - Loss: 0.585 - Acc: 0.740 - Test loss: 1.337 - Test acc: 0.004\n",
      "Ep. 82 - Loss: 0.580 - Acc: 0.742 - Test loss: 1.336 - Test acc: 0.004\n",
      "Ep. 83 - Loss: 0.582 - Acc: 0.741 - Test loss: 1.337 - Test acc: 0.004\n",
      "Ep. 84 - Loss: 0.585 - Acc: 0.739 - Test loss: 1.339 - Test acc: 0.000\n",
      "Ep. 85 - Loss: 0.580 - Acc: 0.741 - Test loss: 1.334 - Test acc: 0.004\n",
      "Ep. 86 - Loss: 0.590 - Acc: 0.739 - Test loss: 1.343 - Test acc: 0.000\n",
      "Ep. 87 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.338 - Test acc: 0.000\n",
      "Ep. 88 - Loss: 0.593 - Acc: 0.733 - Test loss: 1.339 - Test acc: 0.000\n",
      "Ep. 89 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.343 - Test acc: 0.004\n",
      "Ep. 90 - Loss: 0.584 - Acc: 0.740 - Test loss: 1.341 - Test acc: 0.004\n",
      "Ep. 91 - Loss: 0.584 - Acc: 0.739 - Test loss: 1.343 - Test acc: 0.000\n",
      "Ep. 92 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.340 - Test acc: 0.000\n",
      "Ep. 93 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.338 - Test acc: 0.000\n",
      "Ep. 94 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.337 - Test acc: 0.000\n",
      "Ep. 95 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.337 - Test acc: 0.000\n",
      "Ep. 96 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.336 - Test acc: 0.000\n",
      "Ep. 97 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.335 - Test acc: 0.004\n",
      "Ep. 98 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.334 - Test acc: 0.000\n",
      "Ep. 99 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.335 - Test acc: 0.000\n",
      "Ep. 100 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.333 - Test acc: 0.004\n",
      "Ep. 101 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.331 - Test acc: 0.004\n",
      "Ep. 102 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.333 - Test acc: 0.004\n",
      "Ep. 103 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.331 - Test acc: 0.004\n",
      "Ep. 104 - Loss: 0.580 - Acc: 0.741 - Test loss: 1.331 - Test acc: 0.004\n",
      "Ep. 105 - Loss: 0.591 - Acc: 0.739 - Test loss: 1.343 - Test acc: 0.000\n",
      "Ep. 106 - Loss: 0.583 - Acc: 0.741 - Test loss: 1.334 - Test acc: 0.000\n",
      "Ep. 107 - Loss: 0.582 - Acc: 0.741 - Test loss: 1.335 - Test acc: 0.000\n",
      "Ep. 108 - Loss: 0.582 - Acc: 0.741 - Test loss: 1.335 - Test acc: 0.000\n",
      "Ep. 109 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.334 - Test acc: 0.000\n",
      "Ep. 110 - Loss: 0.581 - Acc: 0.740 - Test loss: 1.332 - Test acc: 0.004\n",
      "Ep. 111 - Loss: 0.594 - Acc: 0.739 - Test loss: 1.332 - Test acc: 0.004\n",
      "Ep. 112 - Loss: 0.623 - Acc: 0.739 - Test loss: 1.443 - Test acc: 0.000\n",
      "Ep. 113 - Loss: 0.592 - Acc: 0.740 - Test loss: 1.359 - Test acc: 0.000\n",
      "Ep. 114 - Loss: 0.582 - Acc: 0.741 - Test loss: 1.328 - Test acc: 0.004\n",
      "Ep. 115 - Loss: 0.596 - Acc: 0.739 - Test loss: 1.328 - Test acc: 0.004\n",
      "Ep. 116 - Loss: 0.611 - Acc: 0.740 - Test loss: 1.453 - Test acc: 0.000\n",
      "Ep. 117 - Loss: 0.596 - Acc: 0.740 - Test loss: 1.399 - Test acc: 0.000\n",
      "Ep. 118 - Loss: 0.586 - Acc: 0.740 - Test loss: 1.370 - Test acc: 0.000\n",
      "Ep. 119 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.359 - Test acc: 0.000\n",
      "Ep. 120 - Loss: 0.587 - Acc: 0.740 - Test loss: 1.343 - Test acc: 0.000\n",
      "Ep. 121 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.330 - Test acc: 0.000\n",
      "Ep. 122 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.325 - Test acc: 0.004\n",
      "Ep. 123 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.325 - Test acc: 0.004\n",
      "Ep. 124 - Loss: 0.583 - Acc: 0.739 - Test loss: 1.330 - Test acc: 0.000\n",
      "Ep. 125 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.327 - Test acc: 0.004\n",
      "Ep. 126 - Loss: 0.586 - Acc: 0.739 - Test loss: 1.342 - Test acc: 0.000\n",
      "Ep. 127 - Loss: 0.584 - Acc: 0.740 - Test loss: 1.334 - Test acc: 0.000\n",
      "Ep. 128 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.331 - Test acc: 0.000\n",
      "Ep. 129 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.328 - Test acc: 0.004\n",
      "Ep. 130 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.331 - Test acc: 0.000\n",
      "Ep. 131 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.330 - Test acc: 0.004\n",
      "Ep. 132 - Loss: 0.585 - Acc: 0.739 - Test loss: 1.349 - Test acc: 0.000\n",
      "Ep. 133 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.344 - Test acc: 0.000\n",
      "Ep. 134 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.340 - Test acc: 0.000\n",
      "Ep. 135 - Loss: 0.586 - Acc: 0.740 - Test loss: 1.335 - Test acc: 0.000\n",
      "Ep. 136 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.331 - Test acc: 0.000\n",
      "Ep. 137 - Loss: 0.581 - Acc: 0.740 - Test loss: 1.329 - Test acc: 0.000\n",
      "Ep. 138 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.329 - Test acc: 0.004\n",
      "Ep. 139 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.331 - Test acc: 0.000\n",
      "Ep. 140 - Loss: 0.582 - Acc: 0.741 - Test loss: 1.330 - Test acc: 0.000\n",
      "Ep. 141 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.330 - Test acc: 0.004\n",
      "Ep. 142 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.329 - Test acc: 0.004\n",
      "Ep. 143 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.329 - Test acc: 0.004\n",
      "Ep. 144 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.329 - Test acc: 0.004\n",
      "Ep. 145 - Loss: 0.580 - Acc: 0.741 - Test loss: 1.330 - Test acc: 0.004\n",
      "Ep. 146 - Loss: 0.582 - Acc: 0.741 - Test loss: 1.330 - Test acc: 0.004\n",
      "Ep. 147 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.330 - Test acc: 0.004\n",
      "Ep. 148 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.329 - Test acc: 0.004\n",
      "Ep. 149 - Loss: 0.581 - Acc: 0.740 - Test loss: 1.328 - Test acc: 0.004\n",
      "Ep. 150 - Loss: 0.585 - Acc: 0.739 - Test loss: 1.342 - Test acc: 0.000\n",
      "Ep. 151 - Loss: 0.581 - Acc: 0.740 - Test loss: 1.339 - Test acc: 0.000\n",
      "Ep. 152 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.331 - Test acc: 0.000\n",
      "Ep. 153 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.329 - Test acc: 0.004\n",
      "Ep. 154 - Loss: 0.582 - Acc: 0.741 - Test loss: 1.328 - Test acc: 0.004\n",
      "Ep. 155 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.328 - Test acc: 0.004\n",
      "Ep. 156 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.330 - Test acc: 0.004\n",
      "Ep. 157 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.328 - Test acc: 0.004\n",
      "Ep. 158 - Loss: 0.585 - Acc: 0.739 - Test loss: 1.347 - Test acc: 0.000\n",
      "Ep. 159 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.331 - Test acc: 0.000\n",
      "Ep. 160 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.327 - Test acc: 0.004\n",
      "Ep. 161 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.327 - Test acc: 0.004\n",
      "Ep. 162 - Loss: 0.580 - Acc: 0.741 - Test loss: 1.327 - Test acc: 0.004\n",
      "Ep. 163 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.332 - Test acc: 0.000\n",
      "Ep. 164 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.330 - Test acc: 0.000\n",
      "Ep. 165 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.328 - Test acc: 0.004\n",
      "Ep. 166 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.337 - Test acc: 0.000\n",
      "Ep. 167 - Loss: 0.580 - Acc: 0.741 - Test loss: 1.329 - Test acc: 0.000\n",
      "Ep. 168 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.330 - Test acc: 0.000\n",
      "Ep. 169 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.329 - Test acc: 0.000\n",
      "Ep. 170 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.326 - Test acc: 0.004\n",
      "Ep. 171 - Loss: 0.586 - Acc: 0.739 - Test loss: 1.336 - Test acc: 0.000\n",
      "Ep. 172 - Loss: 0.580 - Acc: 0.741 - Test loss: 1.326 - Test acc: 0.004\n",
      "Ep. 173 - Loss: 0.588 - Acc: 0.739 - Test loss: 1.357 - Test acc: 0.000\n",
      "Ep. 174 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.350 - Test acc: 0.000\n",
      "Ep. 175 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.345 - Test acc: 0.000\n",
      "Ep. 176 - Loss: 0.581 - Acc: 0.740 - Test loss: 1.338 - Test acc: 0.000\n",
      "Ep. 177 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.333 - Test acc: 0.000\n",
      "Ep. 178 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.330 - Test acc: 0.000\n",
      "Ep. 179 - Loss: 0.582 - Acc: 0.741 - Test loss: 1.329 - Test acc: 0.000\n",
      "Ep. 180 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.329 - Test acc: 0.000\n",
      "Ep. 181 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.329 - Test acc: 0.000\n",
      "Ep. 182 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.329 - Test acc: 0.000\n",
      "Ep. 183 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.329 - Test acc: 0.000\n",
      "Ep. 184 - Loss: 0.580 - Acc: 0.742 - Test loss: 1.328 - Test acc: 0.004\n",
      "Ep. 185 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.330 - Test acc: 0.004\n",
      "Ep. 186 - Loss: 0.580 - Acc: 0.741 - Test loss: 1.330 - Test acc: 0.004\n",
      "Ep. 187 - Loss: 0.590 - Acc: 0.739 - Test loss: 1.356 - Test acc: 0.000\n",
      "Ep. 188 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.344 - Test acc: 0.000\n",
      "Ep. 189 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.340 - Test acc: 0.000\n",
      "Ep. 190 - Loss: 0.581 - Acc: 0.740 - Test loss: 1.336 - Test acc: 0.000\n",
      "Ep. 191 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.333 - Test acc: 0.000\n",
      "Ep. 192 - Loss: 0.581 - Acc: 0.741 - Test loss: 1.332 - Test acc: 0.000\n",
      "Ep. 193 - Loss: 0.584 - Acc: 0.739 - Test loss: 1.339 - Test acc: 0.000\n",
      "Ep. 194 - Loss: 0.582 - Acc: 0.740 - Test loss: 1.336 - Test acc: 0.000\n",
      "Ep. 195 - Loss: 0.583 - Acc: 0.740 - Test loss: 1.333 - Test acc: 0.000\n",
      "Ep. 196 - Loss: 0.580 - Acc: 0.742 - Test loss: 1.332 - Test acc: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Initialize the epoch and step counters to -1\n",
    "# Create an empty list for storing training results\n",
    "epoch = step = -1\n",
    "results = []\n",
    "\n",
    "# Iterate through the batches in the loader_train data loader\n",
    "for batch in loader_train:\n",
    "    # Increment the step counter\n",
    "    step += 1\n",
    "\n",
    "    # Execute the train_step function with the current batch\n",
    "    # Obtain the loss and accuracy\n",
    "    loss, acc = train_step(*batch)\n",
    "\n",
    "    # Append the loss and accuracy to the results list\n",
    "    results.append((loss, acc))\n",
    "\n",
    "    # Check if the current step is equal to the number of steps per epoch (loader_train.steps_per_epoch)\n",
    "    if step == loader_train.steps_per_epoch:\n",
    "        # Reset the step counter to 0\n",
    "        # Increment the epoch counter\n",
    "        step = 0\n",
    "        epoch += 1\n",
    "\n",
    "        # Evaluate the model on the test set using the evaluate function (which should be defined beforehand)\n",
    "        # Store the test results in results_te\n",
    "        results_te = evaluate(loader_test)\n",
    "\n",
    "        # Print the epoch number, mean training loss and accuracy, and test loss and accuracy\n",
    "        print(\n",
    "            \"Ep. {} - Loss: {:.3f} - Acc: {:.3f} - Test loss: {:.3f} - Test acc: {:.3f}\".format(\n",
    "                epoch, *np.mean(results, 0), *results_te\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Reset the results list to start collecting results for the next epoch\n",
    "        results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn [90], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m#print(len(batch[1]))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#A_in = GCNConv.preprocess(inputs[1])\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m#A_in = sp_matrix_to_sp_tensor(A_in)  # Convert to SparseTensor\u001b[39;00m\n\u001b[1;32m      6\u001b[0m A_in \u001b[39m=\u001b[39m inputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m loss, acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtrain_on_batch([inputs[\u001b[39m0\u001b[39m], A_in, inputs[\u001b[39m2\u001b[39;49m]], target)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss:\u001b[39m\u001b[39m\"\u001b[39m, loss, \u001b[39m\"\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m\"\u001b[39m, acc)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "for batch in loader_train:\n",
    "    inputs, target = batch\n",
    "    #print(len(batch[1]))\n",
    "    #A_in = GCNConv.preprocess(inputs[1])\n",
    "    #A_in = sp_matrix_to_sp_tensor(A_in)  # Convert to SparseTensor\n",
    "    A_in = inputs[1]\n",
    "    loss, acc = model.train_on_batch([inputs[0], A_in, inputs[2]], target)\n",
    "    print(\"Loss:\", loss, \"Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1113"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.n_graphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
